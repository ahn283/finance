{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 지도학습(Supervised) 알고리즘 : 회귀분석"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='img/Advanced_Algorithms_SupervisedRegression.png'>\n",
    "\n",
    "#### 0) 실제 데이터분석 접근 방법 : 편향과 분산 모두 최소화하기 위해 반복적으로 업데이트\n",
    "\n",
    "<img src='img/Bias_Variance4.png' width=400>\n",
    "\n",
    "\"Train 데이터의 Bias가 적절(낮게)한지 확인 후, Test 데이터에 적용하여 Variance가 적절(낮게)하도록 반복적 업데이트\"\n",
    "\n",
    "- Train의 Bias가 높다면,  빅데이터(Row & Column) 또는 알고리즘 복잡하게 또는 최적화를 통해 해결\n",
    "\n",
    "- Test의 Variance가 높다면, 빅데이터(Row) & 스몰데이터(Column) 또는 알고리즘 덜 복잡하게 또는 최적화를 통해 해결\n",
    "\n",
    "<img src='img/Bias_Variance_Reduce.png'>\n",
    "\n",
    "- 딥러닝(인공지능 알고리즘): 딥러닝은 엄청나게 복잡한 모델이며 Bias-variance Trade-off를 피할 수 없음\n",
    "\n",
    "- 스몰데이터의 딥러닝은 과대적합되어 High Variance가 우려되기에, 딥러닝으로 성능을 내기 위해선 빅데이터가 반드시 필요!\n",
    "\n",
    "- 빅데이터를 통해 Train과 Test의 패턴 차이 감소되어 Bias & Variance를 모두 감소시키기 유리\n",
    "\n",
    "#### 1) 정규화 방법론\n",
    "\n",
    "\"모델링은 목적 달성을 위해 특정 비용함수를 최소화 하는 최적화 알고리즘을 사용하여 분석 알고리즘을 최적화하여 문제해결하는 것\"\n",
    "\n",
    "| | **Regression** | **Classification** |\n",
    "|:-:|:-|:-|\n",
    "| **분석목적** | 수치 예측 | 라벨 예측 |\n",
    "| **분석단계** |  |  |\n",
    "| <span style=\"color:blue\">전처리</span> | 동일 | 동일 |\n",
    "| <span style=\"color:red\">Base 알고리즘</span> | Linear Regression | Logistic Regression |\n",
    "| <span style=\"color:blue\">특징</span> | 선형 | 선형 |\n",
    "| <span style=\"color:red\">비용함수</span> | $(Y - \\hat{Y})^2$ | $-\\hat{Y}log(Pr(\\hat{Y}))$ $-$ $(1-\\hat{Y})log(1-Pr(\\hat{Y}))$ |\n",
    "| <span style=\"color:red\">검증지표</span> | MSE<br>     MAE<br>     RMSE<br>     MAPE<br>     R^2<br>     F검정<br>     t검정<br>     Log-Likelihood<br>     AIC<br>     BIC | Log-Likelihood<br>     Confusion Matrix<br>     Accuracy<br>     Precision<br>     Recall<br>     F1-score<br>     Classification Report<br>     ROC<br>     AUC |\n",
    "| <span style=\"color:red\">잔차진단</span> | 정규분포<br>     자기상관<br>     등분산성 | - |\n",
    "| **Advanced 알고리즘** | - Linear regression<br>     - Polynomial regression<br>     - Stepwise regression<br>     - Ridge/Lasso/ElasticNet regression<br>     - Bayesian Linear regression<br>     - Quantile regression<br>     - Decision Tree regression<br>     - Random Forest regression<br>     - Support Vector regression | - Logistic Regression<br>     - Ordinal Regression<br>     - Cox Regression<br>     - Naïve Bayes<br>     - Stochastic Gradient Descent<br>     - K-Nearest Neighbours<br>     - Decision Tree<br>     - Random Forest<br>     - Support Vector Machine |\n",
    "\n",
    "- Linear Regression: 수치예측을 위해 MSE 비용함수를 최소화 하는 최소제곱법을 사용하여 Linear Regression을 최적화하여 가중치를 추정(문제해결)\n",
    "\n",
    "- Logistic Regression: 분류예측을 위해 Cross Entropy 비용함수를 최소화 하는 수치해석 방법론을 사용하여 Logistic Regression을 최적화하여 가중치를 추정(문제해결)\n",
    "\n",
    "- 결국 모델링은 다양한 가중치/계수/파라미터 후보들 중에서 비용함수에 따라 특정 가중치/계수/파라미터가 선택되는 과정\n",
    "\n",
    "- 정규화(Regularized)\n",
    "\n",
    "    \"비용함수에 제약을 적용하여 추정해야 하는 가중치/계수/파라미터의 범위/자유도를 제한하여 급격한 수치의 가중치/계수/파라미터가 추정되지 않도록 하여 과대적합 발생을 줄이는 방법론\"\n",
    "\n",
    "    <img src='img/Regularization_Intuition.png'>\n",
    "\n",
    "    - 접근: 과적합 가능성이 있을 시 비용함수에 제약/패널티를 부과\n",
    "\n",
    "    - 결과: 추정 가중치의 갯수를 줄이거나 크기를 조정하여 모델 단순화 및 비극단적 수치 추청\n",
    "\n",
    "    - 성능\n",
    "\n",
    "        - 모델이 단순화되기 때문에, Variance 감소 상대적으로 Bias 증가 (by Bias-variance Trade-off)\n",
    "\n",
    "        - Linear Regression에선 $X^TX$의 역행렬 미존재 이슈가 있지만 정규화 알고리즘은 역행렬 항상 존재\n",
    "\n",
    "    - 동의어: Regularized Method, Penalized Method, Contrained Least Squares\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규화 방법론 (Regulazied Method)\n",
    "\n",
    "#### 0) Linear Regression : MSE를 비용함수로 사용\n",
    "\n",
    "$$\\hat{w}=\\underset{w}\\argmin\\left[\\sum_{j=1}^i\\left(y_j-\\sum_{i=0}^kw_ix_{ij}\\right)^2\\right]$$\n",
    "\n",
    "#### 1) Ridge Regression\n",
    "\n",
    "- 정규화조건/패널티/제약조건: 모든 가중치($w$)들의 제곱합을 비용함수에 추가\n",
    "\n",
    "$$\\hat{w}=\\underset{w}\\argmin\\left[\\sum_{j=1}^t\\left(y_i-\\sum_{i=0}^k w_ix_ij\\right)+\\lambda\\sum_{i=0}^k w_i^2\\right] \\\\ \\text{whsere } \\lambda \\text{ is hyper parameter (given by human)}$$\n",
    "\n",
    "- 하이퍼파라미터($\\lambda$): 기존 MSE에 추가될 제약 조건의 비중\n",
    "\n",
    "    - $\\lambda=0$ : 일반적인 Linear Regression\n",
    "\n",
    "    - $\\lambda$가 커지면, 정규화(패널티) 정도가 커지기 때문에 가중치($w_1$) 작아짐\n",
    "\n",
    "    - $\\lambda$가 작아지면, 정규화(패널티) 정도가 작아 지기 때문에 가중치 ($w_i$) 자유도가 높아져 커짐\n",
    "\n",
    "<img src='img/Regularization_Ridge.png' width=500>\n",
    "\n",
    "#### 2) Lasso (Least Absolute Shrinkage and Selection OPerator) Regression\n",
    "\n",
    "- 정규화조건/패널티/제약조건: 모든 가중치($w$)들의 절대값 합을 비용함수에 추가\n",
    "\n",
    "$$\\hat{w}=\\underset{w}\\argmin\\left[\\sum_{j=1}^t\\left(y_i-\\sum_{i=0}^k w_ix_ij\\right)+\\lambda\\sum_{i=0}^k |w_i|\\right] \\\\ \\text{whsere } \\lambda \\text{ is hyper parameter (given by human)}$$\n",
    "\n",
    "<img src='img/Regularization_Lasso.png' width=500>\n",
    "\n",
    "#### 3) Elastic Net : Ridge + Lasso\n",
    "\n",
    "- 정규화조건/패널티/제약조건: 추정 가중치의  절대값 합과 제곱합을 동시에 최소\n",
    "\n",
    "\n",
    "$$\\hat{w}=\\underset{w}\\argmin\\left[\\sum_{j=1}^t\\left(y_i-\\sum_{i=0}^k w_ix_ij\\right)+\\lambda_1\\sum_{i=0}^k |w_i| + \\lambda_2\\sum_{i=0}^k w_i^2 \\right] \\\\ \\text{whsere } \\lambda \\text{ is hyper parameter (given by human)}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터에 따른 결과 비교\n",
    "\n",
    "#### 1) 파라미터 vs 하이퍼파라미터\n",
    "\n",
    "- 파라미터(Parameter): 데이터의 의해 결정되는 모델의 성질을 의미하는 가중치/계수/매개변수\n",
    "\n",
    "- 하이퍼파라미터(Hyperparameter): 모델링 할 때 사용자/분석가가 직접 결정하는 값\n",
    "\n",
    "    - 정해진 최적 값은 없음\n",
    "\n",
    "    - 통계추론 알고리즘 외에 머신러닝/딥러닝으로 갈수록 하이퍼파라미터의 갯수는 늘어남\n",
    "\n",
    "    - 일반적으로 자동화란, 머신러닝/딥러닝은 사람에 따라 결과가 달라질 수 있기 때문에 하이퍼파라미터를 고성능 컴퓨터를 사용해서 모든 경우의 수를 실행하며 최적화하는 과정\n",
    "\n",
    "#### 2) 하이퍼파라미터 최적화에 따른 학습/예측 성능: 예측을 잘 하려면 별도 추정 필요\n",
    "\n",
    "    - Train: 하이퍼파라미터가 작을수록 성능이 좋아짐(과최적화)\n",
    "\n",
    "    - Test: 하이퍼파라미터가 특정한 범위에 있을때 좋아짐(추정필요)\n",
    "\n",
    "#### 3) 하이퍼파라미터 별 정규화 알고리즘 비교\n",
    "\n",
    "<b>(0) Linear Regression</b>\n",
    "\n",
    "<img src='img/Regression_Result_Standard.png'>\n",
    "\n",
    "<b>(1) Ridge Regression</b>\n",
    "\n",
    "- 알고리즘이 모든 변수들을 포함하려 하기 때문에 가중치의 크기가 작아지고 모형의 복잡도 줄어듬\n",
    "\n",
    "- 변수의 수가 많은 경우 효과가 적으나 과적합 방지에 효과적\n",
    "\n",
    "- 다중공선성 존재할 경우, 변수 간 상관관계에 따라 가중치로 다중공선성이 분산되기에 효과 높음\n",
    "\n",
    "\n",
    "<center><img src='img/Regression_Result_Ridge1.png' width='400'><img src='img/Regression_Result_Ridge2.png' width='400'></center>\n",
    "\n",
    "<b>(2) LASSO Regression</b>\n",
    "\n",
    "- 알고리즘이 최소한의 변수를 포함하려 하기 때문에 나머지 가중치는 0이됨 (Feature Selection 기능)\n",
    "\n",
    "- 변수선택 기능이 있기에 일반적으로 많이 사용되지만 특정변수에 대한 가중치 증가 단점\n",
    "\n",
    "- 다중공선성이 존재할 경우, 특정 변수만을 선택하는 방식이라 Ridge에 비해 효과 높을 수 있음\n",
    "\n",
    "<center><img src='img/Regression_Result_Lasso1.png' width='400'><img src='img/Regression_Result_Lasso2.png' width='400'></center>\n",
    "\n",
    "<b>(3) Elastic Net Regression</b>\n",
    "\n",
    "- 큰 데이터셋에서 Ridge와 LASSO의 효과를 모두 반영 (적은 데이터셋은 효과 낮음)\n",
    "\n",
    "<img src='img/Regression_Result_EN.png' width=400>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용방법 및 알고리즘 결과비교\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
