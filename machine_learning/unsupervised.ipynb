{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비지도학습(Unsupervised) 알고리즘: 군집분석"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Advanced_Algorithms_Unsupervised](./img/Advanced_Algorithms_Unsupervised.png)\n",
    "\n",
    "#### 0) 실제 데이터분석 접근 방법: 편향과 분산 모두 최소화하기 위해 반복적으로 업데이트\n",
    "\n",
    "<img src='./img/Bias_Variance4.png' width=400>\n",
    "\n",
    "**\"Train 데이터의 Bias가 적절(낮게)한지 확인 후, Test 데이터에 적용하여 Variance가 적절(낮게)하도록 반복적 업데이트\"**\n",
    "\n",
    "- Train의 Bias가 높다면,  빅데이터(Row & Column) 또는 알고리즘 복잡하게 또는 최적화를 통해 해결\n",
    "\n",
    "- Test의 Variance가 높다면, 빅데이터(Row) & 스몰데이터(Column) 또는 알고리즘 덜 복잡하게 또는 최적화를 통해 해결\n",
    "\n",
    "<img src='./img/Bias_Variance_Reduce.png' width=500>\n",
    "\n",
    "- 딥러닝(인공지능 알고리즘): 딥러닝은 엄청나게 복잡한 모델이며 Bias-variance Trade-off를 피할 수 없음\n",
    "\n",
    "- 스몰데이터의 딥러닝은 과대적합되어 High Variance가 우려되기에, 딥러닝으로 성능을 내기 위해선 빅데이터가 반드시 필요!\n",
    "\n",
    "- 빅데이터를 통해 Train과 Test의 패턴 차이 감소되어 Bias & Variance를 모두 감소시키기 유리\n",
    "\n",
    "| Clustering Algorithms | Association Rule Learning Algorithms | Dimensionality Reduction Algorithms | Ensemble Algorithms | Deep Learning Algorithms |\n",
    "|------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| <img src='./img/Clustering-Algorithms.png' width='150'> | <img src='./img/Assoication-Rule-Learning-Algorithms.png' width='150'> | <img src='./img/Dimensional-Reduction-Algorithms.png' width='150'> | <img src='./img/Ensemble-Algorithms.png' width='150'> | <img src='./img/Deep-Learning-Algorithms.png' width='150'> |\n",
    "| k-Means | Apriori algorithm | Principal Component Analysis (PCA) | Boosting | Deep Boltzmann Machine (DBM) |\n",
    "| k-Medians | Eclat algorithm | Principal Component Regression (PCR) | Bootstrapped Aggregation (Bagging) | Deep Belief Networks (DBN) |\n",
    "| Expectation Maximisation (EM) | - | Partial Least Squares Regression (PLSR) | AdaBoost | Convolutional Neural Network (CNN) |\n",
    "| Hierarchical Clustering | - | Sammon Mapping | Stacked Generalization (blending) | Stacked Auto-Encoders |\n",
    "| - | - | Multidimensional Scaling (MDS) | Gradient Boosting Machines (GBM) | - |\n",
    "| - | - | Projection Pursuit | Gradient Boosted Regression Trees (GBRT) | - |\n",
    "| - | - | Linear Discriminant Analysis (LDA) | Random Forest | - |\n",
    "| - | - | Mixture Discriminant Analysis (MDA) | - | - |\n",
    "| - | - | Quadratic Discriminant Analysis (QDA) | - | - |\n",
    "| - | - | Flexible Discriminant Analysis (FDA) | - | - |\n",
    "\n",
    "**\"비지도학습(Unsupervised Learning)은 정답 레이블이 없기 때문에, 주로 데이터를 새롭게 표현하여 원래 데이터보다 쉽게 해석하거나 특성들을 추가적으로 파악하는데 주로 사용\"**\n",
    "\n",
    "- 집분석: 비지도학습 알고리즘 중 군집화를 위해 사용되는 가장 기본(Baseline) 알고리즘\n",
    "\n",
    "    **(비수학적) \"일상 속 문제들은 어떤 유형들이 있는지 파악하는 문제\"**\n",
    "\n",
    "    - 고객들의 정보를 통해 성인인지 미성년자인지 정답을 찾는 문제가 분류문제\n",
    "\n",
    "    - 고객들의 정보를 통해 어떤 쇼핑 취향들이 있는지 성인 또는 미성년자 레이블을 할당하며 추론하는 문제가 군집문제\n",
    "\n",
    "        - 데이터가 2차원일 경우 시각화를 통해 눈으로도 패턴, 군집, 관계를 어림짐작 가능\n",
    "\n",
    "        - 데이터가 3차원 이상일 경우 시각화로 패턴, 군집, 관계 파악 어려움\n",
    "\n",
    "    **(수학적) \"특정 출력(종속변수)/입력(독립변수)의 구분이나 관계 추론도 없고 학습을 위한 목표값도 없이, 주어진 데이터를 유사한 그룹으로 군집화(Clustering) 하는 알고리즘\"**\n",
    "\n",
    "    - **분류문제**: 데이터 변수(Feature, Variable)들을 사용하여 특정 분류값을 예측\n",
    "\n",
    "    - **군집문제**: 데이터 변수(Feature, Variable)들을 사용하여 여러개의 레이블을 할당하면서 특정 군집값(Cluster)을 예측\n",
    "\n",
    "- **Target Algorithm**\n",
    "\n",
    "    **(1) Partitional Clustering vs Hierarchical Clustering**\n",
    "\n",
    "    - **Partitional**: 전체 데이터를 Hard Clustering 기준으로 한번에 군집형성하는 방식\n",
    "\n",
    "    - **Hierarchical**: 각각의 데이터에서 유사성 척도(Similarity Measure)에 의해 가까운 데이터들을 Tree 형태의 계층적 군집으로 차근차근 묶어나가는 방식이며 Tree에서 어느 수준을 기준으로 하느냐에 따라 군집이 달라짐\n",
    "\n",
    "    <img src='./img/Partitional_Hierarchical.png' width=500>\n",
    "\n",
    "    **(2) Hierarchical Clustering**\n",
    "\n",
    "    - **Agglomerative(Bottom-up)**: 개별 데이터에서 유사한 데이터끼리 묶어가는 방식\n",
    "\n",
    "    - **Divisive(Top-down)**: 모든 데이터를 하나의 군집이라 가정 후 세부 군집으로 분리하는 방식\n",
    "\n",
    "    <img src='./img/AggloDivHierarClustering.png' width=500>\n",
    "\n",
    "\n",
    "| **군집특성 분류** \t| **접근방법** \t| **측정기준** \t| **알고리즘** \t|\n",
    "|:---:\t|:---:\t|:---:\t|:---:\t|\n",
    "| **Hard Clustering** \t| **Partitional Clustering** \t| **Distance-based** \t| `K-Means` \t|\n",
    "|  \t|  \t|  \t| `K-Median` \t|\n",
    "|  \t|  \t|  \t| `K-Mode` \t|\n",
    "|  \t|  \t|  \t| `K-Medoid` \t|\n",
    "|  \t|  \t|  \t| Fuzzy Clustering \t|\n",
    "|  \t|  \t|  \t| PAM(Partitioning Around Medoids) \t|\n",
    "|  \t|  \t|  \t| CLARA(Clustering LARge Applications) \t|\n",
    "|  \t|  \t|  \t| CLARANS(Clustering Large Applications based on RANdomized Search) \t|\n",
    "| **Soft Clustering** \t| **`Hierarchical Clustering`** \t| **Agglomerative<br>     (Bottom-up)** \t| Single Linkage(Graph-based) \t|\n",
    "|  \t|  \t|  \t| Complete Linkage(Graph-based) \t|\n",
    "|  \t|  \t|  \t| Average Linkage(Graph-based) \t|\n",
    "|  \t|  \t|  \t| Centroid Linkage(Distance-based) \t|\n",
    "|  \t|  \t|  \t| Ward Linkage(Distance-based) \t|\n",
    "|  \t|  \t|  \t| AGNES(AGglomerative NESting) \t|\n",
    "|  \t|  \t| **Divisive<br>     (Top-down)** \t| DIANA(DIvisive ANAlysis) \t|\n",
    "|  \t|  \t|  \t| BIRCH(Balanced Iterative Reducint and Clustering Using Hierarchies) \t|\n",
    "|  \t|  \t|  \t| CURE(Clustering Using Representatives) \t|\n",
    "|  \t|  \t|  \t| Chameleon \t|\n",
    "|  \t| **`Density-based Clustering`** \t|  \t| DBSCAN(Density Based   Spatial Clustering of Applications with Noise) \t|\n",
    "|  \t|  \t|  \t| OPTICS(Ordering Points To   Identify the Clustering Structure) \t|\n",
    "|  \t|  \t|  \t| DENCLUE(DENsity-based   CLUstEring) \t|\n",
    "|  \t|  \t|  \t| Density-peaks \t|\n",
    "|  \t|  \t|  \t| Robust-DB(Density Based) \t|\n",
    "|  \t| **Grid-based Clustering** \t|  \t| STING(Statistical   Information Grid) \t|\n",
    "|  \t|  \t|  \t| WaveCluster \t|\n",
    "|  \t|  \t|  \t| CLIQUE(CLustering In QUEst) \t|\n",
    "|  \t| **Model-based Clustering** \t| **Distribution-based** \t| Gaussian Mixture Algorithm \t|\n",
    "|  \t|  \t|  \t| Expectation Maximization   Algorithm \t|\n",
    "|  \t|  \t|  \t| AutoClass(Mixture of Naïve   Bayes) \t|\n",
    "|  \t|  \t|  \t| Cobweb \t|\n",
    "|  \t|  \t| **Network-based** \t| Kohonen Clustering \t|\n",
    "|  \t|  \t|  \t| SOM(Self-Organizing Map) \t|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 계층적 군집화(Hierarchical Clustering, HC)\n",
    "\n",
    "#### 1) **방향** : 유사도가 높은 또는 거리가 가까운 데이터 그룹을 계층적으로 묶으면서 군집 갯수 줄이는 방법\n",
    "\n",
    "- **(1) 소형견 vs 소 vs 중형견** : (푸들, 요크셔테리어), (물소, 젖소, 황소), (셰퍼드, 골든리트리버)\n",
    "\n",
    "- **(2) 개 vs 소** : (푸들, 요크셔테리어, 셰퍼드, 골든리트리버), (물소, 젖소, 황소)\n",
    "\n",
    "- **(3) 동물** : (푸들, 요크셔테리어, 셰퍼드, 골든리트리버, 물소, 젖소, 황소)\n",
    "\n",
    "<img src='./img/Hierarchical_ExamplePlot.png' width=600>\n",
    "\n",
    "- **결과 표현 방식** : Nested Clusters vs Dendrogram\n",
    "\n",
    "    - 데이터가 2차원인 경우 Nested Clusters를, 일반적으로는 Dendrogram 사용\n",
    "\n",
    "<img src='./img/Hierarchical_ResultExample.png' width=500>\n",
    "\n",
    "- **추정 과정**\n",
    "\n",
    "    - 데이터들의 유사성(Distance, Dissimilarity)을 추정 후 결합과정(Agglomeration)을 거쳐 Dendrogram 출력\n",
    "\n",
    "    - 처음에 모든 군집은 하나의 데이터를 가지기에 데이터 갯수만큼 군집 존재\n",
    "\n",
    "    - 최종적으론 군집이 합처져 군집화되면서 하나의 군집만 존재\n",
    "\n",
    "    <img src='./img/Hierarchical_Process.png' width=500>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 알고리즘 함수세팅: 유사성 추정 방식과 결합 과정에 따라 여러가지 방식 존재\n",
    "\n",
    "**(1) 유사성 추정 방식(Distance Matrix)**: 두 데이터 간의 차이를 어떻게 표현할 것인가 \n",
    "\n",
    "- 데이터 특성에 따라\n",
    "\n",
    "| **대분류** \t| **소분류** \t| **의미/예시** \t|\n",
    "|:---:\t|:---:\t|:---:\t|\n",
    "| **질적변수(Qualitative Variable)** \t| **-** \t| 내부 값이 특정 범주(Category)로 분류된 변수(색상,성별,종교) \t|\n",
    "|  \t| **명목형 변수(Nominal Variable)** \t| 값이 순위가 존재하지 않는 경우(혈액형) \t|\n",
    "|  \t| **순위형 변수(Ordinal Variable)** \t| 값이 순위가 존재하는 경우(성적) \t|\n",
    "| **양적변수(Quantitative Variable)** \t| **-** \t| 내부 값이 다양한 숫자 분포로 구성된 변수(키,몸무게,소득) \t|\n",
    "|  \t| **이산형 변수(Discrete Variable)** \t| 값이 셀수 있는 경우(정수) \t|\n",
    "|  \t| **연속형 변수(Continuous Variable)** \t| 값이 셀수 없는 경우(실수) \t|\n",
    "\n",
    "- 변수 종류에 따른 측정 방식\n",
    "\n",
    "| **변수 종류** \t| **측정** \t| **설명** \t|\n",
    "|:---:\t|:---:\t|:---\t|\n",
    "| **Continuous Variable** \t| **Manhattan Distance(Minkowski at Rank=1)** \t| 최단 루트 측정(변수들의 단위가 다르거나 상관성이 있으면 크게 변함) \t|\n",
    "|  \t| **Euclidean Distance(Minkowski at Rank=2)** \t| 최단 거리 측정(변수들의 단위가 다르거나 상관성이 있으면 크게 변함) \t|\n",
    "|  \t| **Standardized Distance** \t| 변수의 분산을 고려하여 표준화 측정 \t|\n",
    "|  \t| **Mahalanobis Distance** \t| 변수의 표준화 및 변수들의 상관관계 측정 \t|\n",
    "|  \t| **Weighted Euclidean Distance** \t| Euclidean & Standardized의 일반화 측정 \t|\n",
    "| **Continuous/Discrete Variable** \t| **Pearson's Correlation Coefficient** \t| 상관관계 측정 \t|\n",
    "| **Discrete(Binary)/Nominal Variable** \t| **Simple Matching Coefficient** \t| 수식 참고 \t|\n",
    "|  \t| **Jaccard's Coefficient** \t| 수식 참고 \t|\n",
    "|  \t| **Russell and Rao Coefficient** \t| 수식 참고 \t|\n",
    "| **Nominal Variable** \t| **Cosine Distance** \t| 문자 벡터들의 각도 측정 \t|\n",
    "|  \t| **Levenshtein Metric** \t| 문자 벡터들에서 다른 단어로 변경시 필요한 편집수 측정 \t|\n",
    "|  \t| **Tanimoto Coefficient(Expanded Jaccard's Coefficient)** \t| 문자 벡터 적용 Jaccard's Coefficient \t|\n",
    "| **Ordinal Variable** \t| **Rank Correlation Coefficient** \t| 순위기반 상관관계 측정 \t|\n",
    "| **Continuous/Discrete/Nominal/Ordinal** \t| **Hamming Distance** \t| 같은 길이의 데이터에 같은 위치에 있는 값들의 비교 측정 \t|\n",
    "\n",
    "- 실제 데이터 값들마다 유사성을 추정하여 행렬(Matrix)로 표현\n",
    "\n",
    "- 특정 값의 쌍에서 추정된 유사성 거리는 행렬에서 2군데에 대칭적으로 표현\n",
    "\n",
    "- 값 자체의 유사성은 계산하지 않고 0으로 표현\n",
    "\n",
    "<img src='./img/Hierarchical_DistanceMatrix.png'>\n",
    "\n",
    "**(2) 결합 방식(Agglomeration)**: 두 데이터 간의 차이를 어떻게 표현할 것인가\n",
    "\n",
    "- Agglomerative는 각 데이터로부터 군집을 만들며 키워가는 방식\n",
    "\n",
    "- Divisive는 반대로 군집을 점차 나누면서 세부 군집으로 줄여가는 방식\n",
    "\n",
    "<img src='./img/AggloDivHierarClustering.png' width=500>\n",
    "<br/>\n",
    "\n",
    "| **군집특성 분류** \t| **접근방법** \t| **측정기준** \t| **알고리즘** \t|\n",
    "|:---:\t|:---:\t|:---:\t|:---:\t|\n",
    "| **Soft Clustering** \t| **`Hierarchical Clustering`** \t| **Agglomerative<br>     (Bottom-up)** \t| Single Linkage(Graph-based) \t|\n",
    "|  \t|  \t|  \t| Complete Linkage(Graph-based) \t|\n",
    "|  \t|  \t|  \t| Average Linkage(Graph-based) \t|\n",
    "|  \t|  \t|  \t| Centroid Linkage(Distance-based) \t|\n",
    "|  \t|  \t|  \t| Ward Linkage(Distance-based) \t|\n",
    "|  \t|  \t|  \t| AGNES(AGglomerative NESting) \t|\n",
    "|  \t|  \t| **Divisive<br>     (Top-down)** \t| DIANA(DIvisive ANAlysis) \t|\n",
    "|  \t|  \t|  \t| BIRCH(Balanced Iterative Reducint and Clustering Using Hierarchies) \t|\n",
    "|  \t|  \t|  \t| CURE(Clustering Using Representatives) \t|\n",
    "|  \t|  \t|  \t| Chameleon \t|\n",
    "\n",
    "<img src='./img/Hierarchical_DirectionType.png' width=500>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 비용함수: 군집과 군집간의 거리를 계산하며 Linkage라고 함\n",
    "\n",
    "| **Linkage 방향** \t| **Linkage 종류** \t| **설명** \t| **특징** \t|\n",
    "|:---:\t|:---:\t|:---\t|:---\t|\n",
    "| **비계층적 방법** \t| - \t| 모든 군집화 알고리즘에 사용가능 <br>계산량 많은   단점 \t|  \t|\n",
    "|  \t| `Centroid` \t| 서로 다른 군집의 `모든 데이터의 평균 간 거리` \t| 일부 Noise / Outlier에 덜 민감하나 <br>다소 성능이 떨어짐 \t|\n",
    "|  \t| `Single` \t| 서로 다른 군집의 `모든 데이터 간 거리 중 최소값` \t| 일부 Noise / Outlier에 민감하게 반응 \t|\n",
    "|  \t| `Complete` \t| 서로 다른 군집의 `모든 데이터 간 거리 중 최대값` \t| 일부 Noise / Outlier에 민감하게 반응하며 <br>큰 클러스터 생성에 약함  \t|\n",
    "|  \t| `Average` \t| 서로 다른 군집의 `모든 데이터 간 거리의 평균` \t| 일부 Noise / Outlier에 덜 민감하나 <br>편향성 존재 \t|\n",
    "| **계층적 방법** \t| - \t| 계층적 군집화 알고리즘에만 사용가능 <br>계산량 적어 효율적 \t|  \t|\n",
    "|  \t| `Median` \t| Centroid의 변형으로 모든 데이터 아닌 <br>`기존 군집 중심점 평균` 사용 \t|  \t|\n",
    "|  \t| `Weighted` \t| Centroid의 변형으로 모든 군집들 내부와 <br>`외부 데이터와의 거리 평균` 사용 \t|  \t|\n",
    "|  \t| `Ward` \t| Weighted의 변형으로 군집화 증가시 <br>`내부 분산을 가장 작게 증가시키는 군집` \t| 일반적으로 많이 사용하나 <br>편향성 존재 \t|\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src='./img/Hierarchical_Linkage.jpg' width=600>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 추정과정 예시: Agglomerative Hierarchical Clustering\n",
    "\n",
    "**(0) 데이터 기반 Distance Matrix**\n",
    "\n",
    "- 5개의 변수와 각 데이터간 거리는 왼쪽과 같음\n",
    "\n",
    "<img src='./img/Hierarchical_Estimation1.png' width=600>\n",
    "\n",
    "**(1) 군집화**\n",
    "\n",
    "- Single Linkage기준 A & B, D & E 거리가 가장 짧으니 군집화\n",
    "\n",
    "- Dendrogram의 높이는 군집간 거리\n",
    "\n",
    "<img src='./img/Hierarchical_Estimation2.png' width=600>\n",
    "\n",
    "**(2) Distance Matrix 업데이트**\n",
    "\n",
    "- 군집화 된 변수와 그렇지 않은 변수들과의 거리 기반 Distance Matrix 업데이트\n",
    "\n",
    "<img src='./img/Hierarchical_Estimation3.png' width=600>\n",
    "\n",
    "**(3) 군집화**\n",
    "\n",
    "- Single Linkage기준 AB & C 거리가 가장 짧으니 군집화\n",
    "\n",
    "<img src='./img/Hierarchical_Estimation4.png' width=600>\n",
    "\n",
    "**(4) Distance Matrix 업데이트 및 반복**\n",
    "\n",
    "<img src='./img/Hierarchical_Estimation5.png' width=600>\n",
    "\n",
    "**(5) 최종 군집 완성**\n",
    "\n",
    "- 모든 데이터가 하나의 군집으로 합쳐짐\n",
    "\n",
    "<img src='./img/Hierarchical_Estimation6.png' width=400>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) 이슈 및 방향: 비용함수에 따라 군집화 결과 다름\n",
    "\n",
    "<img src='./img/Hierarchical_Issue1.png' width=500>\n",
    "<br/>\n",
    "<img src='./img/Hierarchical_Issue2.png' width=500>\n",
    "\n",
    "- Hierarchical Clustering에서는 K-means와 달리 군집의 갯수를 사전에 설정하지 않음\n",
    "\n",
    "- 최종 Dendrogram에 가상의 선을 그어 군집의 갯수 결정\n",
    "\n",
    "<img src='./img/Hierarchical_ClusterNumber.png' width=400>\n",
    "\n",
    "- 각 군집의 갯수에 따른 Metric을 통해 최종 갯수 결정\n",
    "\n",
    "    - Dunn Index\n",
    "    \n",
    "    - Silhouette Index\n",
    "    \n",
    "    - ARI(Adjusted Rand Index)\n",
    "    \n",
    "    - NMI(Normalized Mutual Information)\n",
    "    \n",
    "    - AMI(Adjusted Mutual Information)\n",
    "\n",
    "**(1) Dunn 지표(Dunn Index)**: 군집 내 데이터 간 거리 최대값 대비 군집간 거리 최소값의 비율\n",
    "$$\n",
    "DI(C)=\\cfrac { \\min _{ i\\neq j }{ \\{ { d }_{ c }({ C }_{ i },{ C }_{ j })\\}  }  }{ \\max _{ 1\\le l\\le k }{ \\{ \\triangle { C }_{ l }\\}  }  }\n",
    "$$\n",
    "\n",
    "<img src='./img/Clustering_DunnIndex.png' width=600>\n",
    "\n",
    "**(2) 실루엣 지표(Silhouette Index):** 군집 내 데이터 간 거리 평균과 가장 가까운 군집 내 데이터 간 거리 평균의 차이\n",
    "\n",
    "$$\n",
    "S(i)=\\frac { b(i)-a(i) }{ \\max { \\{ a(i),b(i)\\}  }  }\n",
    "$$\n",
    "\n",
    "**군집 내 응집도(Cohesion)**\n",
    "\n",
    "- $a(i)$ : $i$번째 군집 내 속한 데이터들과의 거리 평균\n",
    "\n",
    "**군집 간 분리도(Separation)**\n",
    "\n",
    "- $b(i)$ : $i$번째 군집과 가장 가까운 군집에 속한 데이터들과의 거리 평균\n",
    "\n",
    "<img src='./img/Clustering_Silhouette.png' width=600>\n",
    "\n",
    "- 보통 실루엣 지표가 0.5보다 크면 군집 결과가 타당한 것으로 평가\n",
    "\n",
    "<img src='./img/Clustering_Silhouette_BestWorst.png' width=600>\n",
    "\n",
    "- 오히려 평가보다 군집 갯수를 결정하는데 많이 사용\n",
    "\n",
    "- 밀집된 클러스터에선 성능이 좋으나 모양이 복잡할 때는 평가 성능 좋지 않음\n",
    "\n",
    "**(3) Others**: 클러스터 레이블 정답을 아는 경우 군집성능 평가\n",
    "\n",
    "- **ARI(Adjusted Rand Index)**: 얼마나 많은 클러스터들이 정답과 유사한지 측정\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metric import adjusted_rand_score\n",
    "    ```\n",
    "\n",
    "- **NMI(Normalized Mutual Information)**: 상관관계 한계를 대체하기 위해 실제와 예측 클러스터 생성을 위한 정보량 분포의 유사성/의존도 측정\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import normalized_mutual_info_score\n",
    "    ```\n",
    "\n",
    "- **AMI(Adjusted Mutual Information)**: 정보량과 상관없이 클러스터 수가 많을 때 NMI가 높아지는 한계를 보완\n",
    "\n",
    "    ```python\n",
    "    from sklearn.metrics import adjusted_mutual_info_score\n",
    "    ```\n",
    "\n",
    "**(4) 현실 지표**\n",
    "\n",
    "- 데이터에 잡음/변화를 주었을 때의 결과 변동성 고려\n",
    "\n",
    "- 알고리즘 매개변수에 변화를 주었을 때의 결과 변동성 고려\n",
    "\n",
    "- 여러가지 변화에도 결과가 일정하면(Robust) 신뢰할만한 모델링"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 사용방법\n",
    "\n",
    "```python\n",
    "from matplotlib import pyplot as plt \n",
    "from scipy.cluster.hierarchy \n",
    "import dendrogram, linkage from sklearn.cluster \n",
    "import AgglomerativeClustering\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "```python\n",
    "# Linkage 계산 및 Dendrogram 시각화\n",
    "X_tr_link = linkage(X_train, method='ward') \n",
    "dendrogram(X_tr_link, orientation='top', distance_sort='ascending', show_leaf_counts=True) \n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Cluster 추정**\n",
    "\n",
    "```python\n",
    "model_aggclust = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward') \n",
    "Y_trpred = model_aggclust.fit_predict(X_train) \n",
    "Y_tepred = model_aggclust.fit_predict(X_test)\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 밀도기반 군집화(Density-Based Clustering, DBC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 방향\n",
    "\n",
    "| 접근 | 가정 | 특징 | \n",
    "|:---: |:--- |:--- | \n",
    "| Distance-based | 동일 군집에 속하는 데이터들은 어떠한 중심을 기준으로 분포될 것 | 원형태 군집 | \n",
    "| Density-based | 동일 군집에 속하는 데이터들은 서로 근접하게 분포될 것 | 불특정형태 군집 |\n",
    "\n",
    "**\"빅데이터에서 군집화는 3가지 조건을 충족할 수록 좋은 알고리즘\"**\n",
    "\n",
    "- 대용량 데이터라도 파라미터 설정을 위한 최소한의 도메인 지식이 수반되어야 함\n",
    "\n",
    "- 데이터의 형태가 구형, 타원형, 선형 등 다양한 형태이더라도 군집화 성능에 효과가 있어야\n",
    "\n",
    "- 스몰데이터 뿐만 아니라 빅데이터에서도 효율적 군집화 가능해야\n",
    "\n",
    "<img src='./img/Clustering_Comparison.jpg'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 알고리즘 함수세팅: 단순하지만 고성능 및 고효율 밀도 기반 군집화 알고리즘 DBSCAN\n",
    "\n",
    "<img src='./img/KMeas_DBSCAN.png' width=600>\n",
    "\n",
    "- K-means 또는 Hierarchical의 경우 거리를 이용하여 군집화를 하는 반면, 밀도가 높은 부분을 군집화 하는 방법\n",
    "\n",
    "- K-means와 달리 클러스터의 갯수를 미리 지정할 필요 없음\n",
    "\n",
    "- 밀도에 따라 군집화를 확장하기 때문에 다양한 기하학적 구조의 군집화에도 효과\n",
    "\n",
    "- K-means 또는 Hierarchical 대비 다소 느리지만 비교적 큰 데이터셋에도 쉽게 적용 가능\n",
    "\n",
    "- 특정 데이터가 군집에 속하는 경우 해당 클러스터 내 다른 많은 요소들과 가까워야 한다는 아이디어\n",
    "\n",
    "- 컴퓨팅 알고리즘 기반 제안된 방법론이라 특별한 수식 미존재\n",
    "\n",
    "- 용어 정리\n",
    "\n",
    "    - **R(Radius of Neighborhood)**: Radius는 특정 값을 기준으로 반경(밀도)을 지정\n",
    "\n",
    "    - **M(Min Number of Neighbors)**: 핵심요소 지정을 위해, 핵심 주변의 요소 갯수 지정, 최소 3이상 및 기본적으로 변수의 수 정도\n",
    "\n",
    "- 데이터 각각을 핵심점, 경계점, 그리고 이상치로 구분\n",
    "\n",
    "    - **핵심점(Core Point)**: 특정 데이터가 R 반경 이내에 M개의 데이터가 있는 경우 Core Point\n",
    "\n",
    "    - **군집(Cluster)**: Core Point + 주변 M개의 데이터\n",
    "\n",
    "    - **경계점(Border Point)**: 군집 내에서 Core Point가 아닌 데이터를 Border Point\n",
    "\n",
    "    - **이상치(Outlier Point)**: 군집에 속하지 않는 데이터를 Outlier Point\n",
    "\n",
    "<img src='./img/DBSCAN_Words.png' width=400>\n",
    "\n",
    "**(1) 각 데이터 별로 핵심점들 탐색**\n",
    "\n",
    "<img src='./img/DBSCAN_Process1.png' width=300> \n",
    "<img src='./img/DBSCAN_Process2.png' width=300>\n",
    "\n",
    "**(2) 핵심점들 거리가 R보다 작을 경우 같은 클러스터로 군집화**\n",
    "\n",
    "<img src='./img/DBSCAN_Process3.png' width=300>\n",
    "\n",
    "**(3) 핵심 안에 속하지 못한 이상치 탐색**\n",
    "\n",
    "<img src='./img/DBSCAN_Process4.png' width=300>\n",
    "\n",
    "- 추정과정 및 정리\n",
    "\n",
    "<img src='./img/DBSCAN_Simulation.gif' with=400>\n",
    "\n",
    "| 장점 | 단점 | \n",
    "|:---: |:---: | \n",
    "| DBSCAN은 이상치에 민감하지 않고 Robust함 | 부분적으로 비슷한 밀도를 가진 데이터의 경우 효과 떨어짐 | \n",
    "| 이상치를 별도 추정 가능 | 터이터 처리 순서가 매번 달라 결과가 달라짐 | \n",
    "| 저밀도 군집에서도 고밀도 군집 분리 유용 | 고차원의 데이터로 갈수록 Radius 지정이 어려움 | \n",
    "| 다양한 형태의 군집도 추출 가능 | 데이터가 증가하면 계산 시간이 훨씬 증가 | \n",
    "| 군집 수를 미리 지정할 필요 없음 | 적합한 파라미터 설정 어려움 | \n",
    "| 비교적 큰 데이터셋에도 쉽게 적용 | |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 이슈 및 방향: Radius에 따라 군집화 결과 다름\n",
    "\n",
    "<img src='./img/DBSCAN_Issue.png' width=600>\n",
    "\n",
    "- 다양한 밀도를 가지는 데이터에서는 성능이 떨어지며, 고차원에서 적절한 Radius 추정이 어려움\n",
    "\n",
    "- 적절한 Radius 추정을 위해 KNN의 Distance Graph를 활용하여 변곡점으로 적절한 Raduis 선정\n",
    "\n",
    "- M Hyperparameter 입력 필요가 없는 Hierarchical DBSCAN(HDBSCAN) 등장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 사용방법\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# 학습 및 예측\n",
    "model_dbscan = DBSCAN(eps=0.5, min_samples=2)\n",
    "Y_tepred = model_dbscan.fit_predict(X_test)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비지도학습(Unsupervised) 알고리즘: 차원변환\n",
    "\n",
    "<img src='./img/Advanced_Algorithms_Unsupervised.png'>\n",
    "\n",
    "| Clustering Algorithms | Association Rule Learning Algorithms | Dimensionality Reduction Algorithms | Ensemble Algorithms | Deep Learning Algorithms |\n",
    "|------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| <img src='./img/Clustering-Algorithms.png' width='150'> | <img src='./img/Assoication-Rule-Learning-Algorithms.png' width='150'> | <img src='./img/Dimensional-Reduction-Algorithms.png' width='150'> | <img src='./img/Ensemble-Algorithms.png' width='150'> | <img src='./img/Deep-Learning-Algorithms.png' width='150'> |\n",
    "| k-Means | Apriori algorithm | Principal Component Analysis (PCA) | Boosting | Deep Boltzmann Machine (DBM) |\n",
    "| k-Medians | Eclat algorithm | Principal Component Regression (PCR) | Bootstrapped Aggregation (Bagging) | Deep Belief Networks (DBN) |\n",
    "| Expectation Maximisation (EM) | - | Partial Least Squares Regression (PLSR) | AdaBoost | Convolutional Neural Network (CNN) |\n",
    "| Hierarchical Clustering | - | Sammon Mapping | Stacked Generalization (blending) | Stacked Auto-Encoders |\n",
    "| - | - | Multidimensional Scaling (MDS) | Gradient Boosting Machines (GBM) | - |\n",
    "| - | - | Projection Pursuit | Gradient Boosted Regression Trees (GBRT) | - |\n",
    "| - | - | Linear Discriminant Analysis (LDA) | Random Forest | - |\n",
    "| - | - | Mixture Discriminant Analysis (MDA) | - | - |\n",
    "| - | - | Quadratic Discriminant Analysis (QDA) | - | - |\n",
    "| - | - | Flexible Discriminant Analysis (FDA) | - | - |\n",
    "\n",
    "**\"비지도학습(Unsupervised Learning)은 정답 레이블이 없기 때문에, 주로 데이터를 새롭게 표현하여 원래 데이터보다 쉽게 해석하거나 특성들을 추가적으로 파악하는데 주로 사용\"**\n",
    "\n",
    "- **차원변환**: 비지도학습 알고리즘 중 다차원 특성파악을 위해 사용되는 가장 기본(Baseline) 알고리즘\n",
    "\n",
    "    (비수학적) \"일상 속 문제의 다양한 풀이법들의 우선순위를 파악하는 문제\"\n",
    "\n",
    "    - 유사한 고객 정보를 가진 사람들의 공통된 쇼핑 취향을 파악하면서 그룹(레이블)으로 추론하는 문제가 군집문제\n",
    "\n",
    "    - 다양한 고객 정보들에서 신규 쇼핑 취향과 같은 추가적인 특성을 파악하기 위해 고객 정보들을 차원변환하여 문제를 다각도로 살펴보는 것\n",
    "\n",
    "    (수학적) \"특정 출력(종속변수)/입력(독립변수)의 구분이나 관계 추론도 없고 학습을 위한 목표값도 없이, 주어진 데이터의 추가적인 특성확인을 위해 다른 차원으로 변환(Reduction) 하는 알고리즘\"\n",
    "\n",
    "    - 군집문제: 데이터에서 유사한 값들을 가진 군집(Cluster)을 예측하며 레이블을 할당\n",
    "\n",
    "    - 차원변환: 데이터를 다각도로 살펴보기 위해 다른(차원) 관점으로 추가적인 변수를 예측하며 특성 확인\n",
    "\n",
    "<img src='./img/Unsupervised_Clustering_Reduction.png'>\n",
    "\n",
    "**How?**\n",
    "\n",
    "**(1) 변수 선택(Feature/Variable Selection)**: 특정 변수가 다른 변수들로 생성될 수 있는 경우, 특정 변수의 종속성이 강하다고 하고 간단히 제거를 통해 중요 변수들만 구성하는 차원 축소\n",
    "\n",
    "- 장점: 남은 변수들을 통해 중요도와 해석이 용이\n",
    "\n",
    "- 단점: 변수들 간의 종속성/상관성을 명확하게 고려하기 어려움\n",
    "\n",
    "- Ridge, Lasso, VIF(Variance Inflation Factor) 등\n",
    "\n",
    "\n",
    "**(2) 변수 추출(Feature/Variable Extraction)**: 변수들 간의 상관관계를 고려하여 새로운 중요 변수를 생성하는 차원축소\n",
    "\n",
    "- 장점: 변수들 간의 상관성을 고려하기 용이하고 변수들의 갯수를 많이 줄일 수 있음\n",
    "\n",
    "- 단점: 새롭게 추출된 변수들의 의미나 해석이 어려움\n",
    "\n",
    "- PCA(Principal Component Analysis), FA(Factor Analysis) 등        \n",
    "\n",
    "- **종류**: 차원변환문제 해결 알고리즘은 다양하고, 데이터 특성/구조/목적에 맞는 적절한 선택 필요\n",
    "\n",
    "    - **(Linear) Projection**: 선형기준으로 데이터를 근사하여 직관적으로 차원을 축소\n",
    "\n",
    "    - **(Non-linear) Manifold Learning**: 직관적으로 파악이 어려운 데이터의 비선형적 관계를 반영하여 차원을 축소\n",
    "\n",
    "| **접근방법** \t| **알고리즘** \t|\n",
    "|:---:\t|:---\t|\n",
    "| **(Linear) Projection** \t| Eigen Value Decomposition \t|\n",
    "|  \t| Singular Value Decomposition \t|\n",
    "|  \t| Truncated SVD \t|\n",
    "|  \t| Principal Component Analysis \t|\n",
    "|  \t| Factor Analysis \t|\n",
    "|  \t| Linear Discriminant Analysis \t|\n",
    "|  \t| Quadratic Discriminant Analysis \t|\n",
    "| **(Non-linear) Manifold Learning** \t| Kernel Principal Component Analysis \t|\n",
    "|  \t| Non-Negative Matrix Factorization \t|\n",
    "|  \t| Locally Linear Embedding (LLE) \t|\n",
    "|  \t| Isomap \t|\n",
    "|  \t| Multi Dimensional Scaling \t|\n",
    "|  \t| Spectral Embedding \t|\n",
    "|  \t| t-distributed Stochastic Neighbor Embedding (t-SNE) \t|\n",
    "|  \t| Autoencoders \t|\n",
    "|  \t| Self Organizaing Map (SOP) \t|\n",
    "\n",
    "\n",
    "- **Target Algorithm**\n",
    "\n",
    "    - Eigen Value Decomposition\n",
    "\n",
    "    - Singular Value Decomposition\n",
    "\n",
    "    - Truncated SVD\n",
    "\n",
    "    - Principal Component Analysis\n",
    "\n",
    "    - Principal Component Regression\n",
    "\n",
    "    - Factor Analysis\n",
    "\n",
    "    - Linear Discriminant Analysis\n",
    "\n",
    "    - Quadratic Discriminant Analysis\n",
    "\n",
    "    - Mixture Discriminant Analysis\n",
    "\n",
    "    - Non-Negative Matrix Factorization\n",
    "\n",
    "    - Locally Linear Embedding\n",
    "\n",
    "    - t-distributed Stochastic Neighbor Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요인 분석(Factor Analysis, FA)\n",
    "\n",
    "#### 0) 배경: 주성분분석?, 요인분석? 어떤 분석에 활용?\n",
    "\n",
    "- (1) 변수들 내부에 존재하는 구조를 파악하여 정보를 요약하거나 중요도가 낮은 변수를 제거하려는 경우\n",
    "\n",
    "- (2) 변수들에 내재하는 상관관계, 연관성을 이용해 소수의 주성분 또는 요인으로 차원을 축소함으로써 데이터의 이해도 및 관리효율 높임\n",
    "\n",
    "    - 사람은 1/2차원까지는 직관적 이해가 가능하지만 3차원 이상은 어려움\n",
    "\n",
    "    - 만약 변수가 10개(10차원) 있는데 2개의 차원으로 요약 표현해도 변수가 가진 변동의 80~90%를 설명할 수 있다면 굳이 10차원의 데이터가 필요한가?\n",
    "\n",
    "- (3) 상관성이 너무 높은 변수들을 축소된 소수의 주성분 또는 요인으로 추출하여 예측/분류 모델링을 위한 다른 알고리즘의 독립변수들로 사용하여 성능을 높임\n",
    "\n",
    "    - 데이터의 모델링 시 입력 변수들의 상관관계가 높은 다중공선성이 존재할 경우, 모델링 성능도 떨어지고 특히 해석에 문제 발생\n",
    "\n",
    "    - 특히 다중공선성이 있는 데이터로 결정트리 생성 시 하나의 변수가 구분되어 버리면 그 변수와 상관성이 있는 변수들은 중요하지 않은 변수로 간주되거나 모델링에 포함되지 않음\n",
    "\n",
    "    - 요인분석 후 새로운 잠재변수를 추가하여 분석 수행시 다중공선성 문제 해결 가능\n",
    "\n",
    "<img src='./img/FactorAnalysis_Application.png' width=600>\n",
    "\n",
    "- (4) 레이블이 없는 군집분석에도 연산속도 개선에 기여하며 군집 레이블링도 가능\n",
    "\n",
    "    - 요인들의 군집분석을 통해 세부군집 레이블링 가능\n",
    "\n",
    "- (5) 기계장비에서 발생하는 다수의 데이터를 주성분/요인 분석을 통해 시계열의 분포나 추세 변화에 반영하여 기계의 고장징후 등을 사전에 파악하는데 활용\n",
    "\n",
    "- (6) 데이터 전처리 변환 방법으로도 사용되며, 주성분/요인 분석 자체를 해석에도 활용"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 요인분석 방향: 데이터 변수들의 잠재변수를 찾아 변수의 수를 줄이는 것\n",
    "\n",
    "- 통계학자 Spearman이 학생들의 시험성적 상관관계를 보다가 연관성 있는 변수를 묶는 내재 속성 추출 고민에서 유래\n",
    "\n",
    "- 여러개의 변수들로 구성된 데이터에서 상관관계를 기초로 변수들을 설명할 수 있는 새로운 공통 잠재 요인 발견 또는 데이터 구조 확인이 목적인 탐색적 데이터 분석 방법론\n",
    "\n",
    "- 잠재요인과 무관하게 FA에서는 고유요인(e)이 존재하며 각 변수들 자체와 관련있는 고유분산이기 때문에 상관행렬을 그대로 사용하지 않고 수정된 상관행렬(Adjusted Correlation Matrix) 사용\n",
    "\n",
    "- PCA와 깊은 관계가 있지만 목적이 다르고 일반적으로 PCA $\\ne$ FA\n",
    "\n",
    "<img src='./img/PCA_FA_Comparison2D.jpg' width=600>\n",
    "\n",
    "- **요인(Factor)**: 변수들의 연관성을 설명하는 잠재 변수\n",
    "\n",
    "    - 요인의 갯수는 변수의 수만큼 가능\n",
    "\n",
    "    - 모든 요인은 변수들의 공통 분산을 설명\n",
    "\n",
    "    - 분산의 양이 가장 적은 요인은 제거\n",
    "\n",
    "    - 잠재변수, 관측되지 않은 변수, 가상 변수 라고도 함\n",
    "\n",
    "    <img src='./img/FactorAnalysis_Factor.png' width=500>\n",
    "\n",
    "    <img src='./img/FactorAnalysis_FactorWeight.png'>\n",
    "\n",
    "- **요인 추출 및 레이블링**\n",
    "\n",
    "<img src='./img/FactorAnalysis_FactorLoading.png' width=500>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 알고리즘 함수세팅\n",
    "\n",
    "**(0) 데이터 수집**\n",
    "\n",
    "- 각 변수의 값들은 정규분포를 따르고 순위나 비율로 표현되는 동일 분산 범위의 값\n",
    "\n",
    "- 일반적으로 변수의 수 대비 10배 이상의 샘플을 권장\n",
    "\n",
    "**(1) 요인분석 가능성 파악(Factorability)**\n",
    "\n",
    "- **Bartlett Test**: 변수들의 상관관계를 추정하여 단위행렬보다 상관성이 높은지 여부 검정\n",
    "\n",
    "    - 검정을 위해 카이제곱분포를 이용한 통계량을 추정하며 유의할 경우 요인분석 실행\n",
    "\n",
    "- **Kalser-Meyer-Olkln(KMO) Test**: 변수들의 편상관관계가 얼마나 작은지 여부 검정\n",
    "\n",
    "    - **편상관관계**: 두 변수의 상관관계 추정시, 두 변수에 영향을 미치는 공통/잠재 변수가 존재하는 경우 잠재 변수가 미치는 효과를 제외한 순수한 상관관계\n",
    "        \n",
    "    - 편상관관계가 작을수록 잠재 변수의 영향(존재)이 크다\n",
    "\n",
    "    $$\\text{KMO 통계량} =\\frac{\\text{상관계수 제곱}}{\\text{상관계수 제곱}+\\text{편상관계수}}$$\n",
    "\n",
    "    - 편상관관계가 작을수록 KMO 통계량이 크고 공통된 변수를 추출하는 요인분석 적합\n",
    "\n",
    "    - 모든 변수간 분산 비율을 추정하며 KMO 추정을 위해 최소 3개 이상의 변수 필요\n",
    "\n",
    "    - 통계량이 0~1 사이로 움직이며, 0.5 이상이면 요인분석 하기에 적절하다고 판단하는 편\n",
    "\n",
    "**(2) 요인 갯수 선택**: 학문적 정답은 없으며, 경험적 법칙 존재\n",
    "\n",
    "- **상관관계 추정 및 고유벡터 추출**\n",
    "\n",
    "    - (1) 분산의 누적기여율이 최소 0.8 이상\n",
    "\n",
    "    - (2) 표준화 데이터 사용시 고유값이 최소 1이상인 요인 추출\n",
    "\n",
    "    - (3) Scree Plot에서 꺽이는 Elbow 구간이 있다면, Elbow 앞까지 추출\n",
    "\n",
    "    - (4) 다른 알고리즘 활용시 요인의 갯수와 비용함수의 관계에서 선택\n",
    "\n",
    "**(3) 요인 추출(Factor Loading)**: 각 요인이 변수에 미치는 효과로 변수와 요인의 상관행렬\n",
    "\n",
    "<img src='./img/FactorAnalysis_MainProcess.png'>\n",
    "\n",
    "- **상관관계 추정 및 고유벡터 추출**\n",
    "\n",
    "- **요인회전(Factor Rotation)**: 변수들을 요인으로 묶어주기 편리하게 또는 해석하기 쉽도록 축을 회전하는 방법론\n",
    "\n",
    "    - **직교회전법**: 요인으로 묶을 때 독립성을 유지한 상태에서 요인구조가 가장 뚜렷할 때까지 요인을 회전\n",
    "\n",
    "        - 회전축이 직각을 유지하기에 요인들의 상관계수가 0이 되기 때문에 요인들이 상호 독립적인 경우 사용 가능\n",
    "\n",
    "        - 추가적 분석에 활용하려면 다중공선성을 방지하기 위한 방법으로도 유용\n",
    "\n",
    "        - Varimax, Quartimax, Equimax, Transvarimax 등이 있으며 일반적으로 분산을 최대화하는 Varimax를 많이 사용\n",
    "\n",
    "    - **비직교회전법**: 요인으로 묶을 때 독립성을 유지한다 보기 어려운 상태에서 요인구조가 가장 뚜렷할 때까지 요인을 회전\n",
    "\n",
    "        - 일부 변수가 여러 요인들에 높은 요인적재량을 갖으면 요인들이 독립적이라 보기 어려움\n",
    "\n",
    "        - 요인 회전시 서로 직각을 유지하지 않기 때문에 영향력이 높은 요인은 더 높아지도록 그렇지 않은 요인은 더 낮아지도록 추정\n",
    "\n",
    "        - Oblimin, Covarimin, Quartimin 등이 있음\n",
    "\n",
    "        - 사회과학에서는 독립적 변수를 인정하지 않아 주로 사용하는 회전법\n",
    "\n",
    "- 변수들과 요인들의 상관관계 행렬 추출\n",
    "\n",
    "- 통계적 유의성 대신 실제 의미적 유의성 관심\n",
    "\n",
    "- Factor Loading이 0.5 이상인 값들을 중심으로 유의성 판단\n",
    "\n",
    "- 변수가 여러개의 요인 중 하나의 요인에만 Loading이 높은 경우 변수 유지\n",
    "\n",
    "- 변수가 여러개의 요인 중 2개 이상의 요인에 Loading이 높은 경우 변수 삭제\n",
    "\n",
    "**(4) 요인 해석 및 레이블링**: 학문적 정답은 없으며, 경험적 법칙 존재\n",
    "\n",
    "<img src='./img/FactorAnalysis_Decision.png'>\n",
    "\n",
    "- Factor Loading이 0.5 이상인 값들을 중심으로 요인의 명칭 레이블링\n",
    "\n",
    "- 연구자의 추가적인 분석/조사나 주관적으로 레이블링하며 상식적으로 의미가 일치하는지 중요\n",
    "\n",
    "**(5) 요인 레이블링 검증**\n",
    "\n",
    "- 레이블링된 요인 명칭으로 변수 묶음을 대체해도 될지 검증\n",
    "\n",
    "- **Communality**: 추출된 요인들에 의해 설명되는 변수의 분산 정도로 0.5 이상인 변수들에 대해 레이블링 유의하다 판단\n",
    "\n",
    "- **크론바흐 알파(Cronbach Alpha)**: 기준 수치가 1에 가까울수록 요인 대체 승인\n",
    "\n",
    "**(6) 요인 활용 추가적 분석 수행**\n",
    "\n",
    "- 요인 점수(Factor Score) 추출 및 활용 가능\n",
    "\n",
    "- 심리검사 및 설문조사에서 대부분 사용하는 기초분석\n",
    "\n",
    "- 소비자 만족도, 경험, 관심도 등의 특성을 확인하기 위한 기초분석\n",
    "\n",
    "- 시장조사, 광고, 금융, 운용, 리서치, 브랜딩 등 이해도 향상을 위한 기초분석\n",
    "\n",
    "- 예측, 분류 등의 문제에 성능 높은 변수로 반영하기 위한 전처리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 정리\n",
    "\n",
    "- **PCA & FA 공통점**: 데이터의 차원/변수의 갯수를 줄이는 것이며, 원래 데이터에서 새로운 변수들을 생성\n",
    "\n",
    "- **PCA & FA 차이점**\n",
    "\n",
    "| PCA | FA | \n",
    "|:---: |:---: | \n",
    "| 데이터 분포상 최대분산 설명 | 데이터의 공분산 설명 | \n",
    "| 주성분에 수직 거리 합의 제곱이 최소화 되도록 추정 | 변수를 설명할 수 있는 새로운 공통변수 추정 | \n",
    "| 데이터 최대보존 시각적 변환 기술 | 데이터의 가정을 검증하는 모델링적 추정 기술 | \n",
    "| 오차항 불필요 | 오차항 필요 | \n",
    "| 주성분은 변수들의 (선형)조합 | 변수들은 요인들의 (선형)조합 | \n",
    "| 주성분들이 완벽하게 독립 | 요인들은 독립일 필요 없음 | \n",
    "| 주성분들은 레이블링 및 해석 불가 (분류력이 높은 임의 변수) | 요인들은 레이블링 및 해석 가능 (의미 중심 그룹화) | \n",
    "| 주성분들의 우선순위 존재 | 요인들은 대등한 관계 | | 차원 축소 방법론 | 잠재 변수 방법론 | \n",
    "| 목표함수 기반 예측/분류력 분석 | 목표함수 없이 변수 그룹핑 | \n",
    "| Correlation 행렬 기반 대각성분 분석 | Adjusted Correlation 행렬 기반 대각성분 분석 |\n",
    "\n",
    "- 요인은 보다 자연스러운 데이터 해석을 가능하게 함\n",
    "\n",
    "- 요인을 식별하고 레이블링 할 때는 도메인 지식 반영 필요\n",
    "\n",
    "- 동일한 데이터 요소에 대해 둘 이상의 해석이 가능하기 때문에 요인 분석의 결과는 논란의 여지가 있을 수 있음\n",
    "\n",
    "- 기본적으로 상관관계가 높은 변수들끼리 그룹핑하는 것으로 상관관계 낮으면 부적합\n",
    "\n",
    "- 변수들이 상관관계가 너무 높아 완벽한 다중공선성 상태면 추정이 어려움\n",
    "\n",
    "- 샘플들의 갯수 > 변수/요인의 갯수 이어야 추정 가능\n",
    "\n",
    "- 4~5개의 변수에서 요인추출을 하는 것은 의미있는 결과 도출 어려울 수 있음\n",
    "\n",
    "- 많은 변수들 중에서 적은 일부 요인들이 많은 변수를 대표할 수 있는 구조 추천\n",
    "\n",
    "- 빅데이터를 탐색하고 상호 연관성을 추정\n",
    "\n",
    "- 연구자가 시장 상황을 압축하고 소비자 취향, 선호도 및 문화적 영향 간의 숨겨진 관계를 찾는 데 도움될 뿐 아니라 관련 심리학, 금융 및 운영 등에 널리 사용됨"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 사용방법\n",
    "```python\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# 요인분석 가능성 파악\n",
    "statistics_bar, p_value = calculate_bartlett_sphericity(X_train)\n",
    "kmo_each, kmo_all = calculate_kmo(X_train)\n",
    "# 모델링\n",
    "model_fa = FactorAnalyzer(n_factors=5, rotation='varimax')\n",
    "model_fa.fit(X_train)\n",
    "# 요인과 변수 상관관계 확인\n",
    "model_fa.loadings_\n",
    "# 분산 설명력 확인\n",
    "model_fa.get_factor_variance()\n",
    "# 잠재요인 추출\n",
    "X_train_fa = model_fa.transform(X_train)\n",
    "X_test_fa = model_fa.transform(X_test)\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형판별 분석(Linear Discriminant Analysis, LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
