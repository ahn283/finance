{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터분석 단계 (Data Analysis Cycle)\n",
    "\n",
    "<img src='./img/DataAnalysis_Cycle0.png'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='./img/DataAnalysis_Cycle1.png'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='./img/DataAnalysis_Cycle2.png'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='./img/DataAnalysis_Cycle3.png'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='./img/DataAnalysis_Cycle4.png'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='./img/DataAnalysis_Cycle5.png'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='./img/DataAnalysis_Cycle6.png'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='./img/DataAnalysis_Cycle7.png'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='./img/DataAnalysis_Cycle8.png'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='./img/DataAnalysis_Cycle9.png'>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시계열분석과 기계학습의 차이 (Comparison)\n",
    "\n",
    "## 확률적 프로세스 및 시계열 데이터\n",
    "\n",
    "<img src='./img/TS_MS_Comparison.png'>\n",
    "\n",
    "- **확률적 프로세스(Stochastic Process)** : 시간에 따라 확률적 수치의 변화를 가지는 함수\n",
    "\n",
    "    - **변수**\n",
    "\n",
    "    $$X={X_1, X_2, ..., X_n}$$\n",
    "\n",
    "    - **확률적 프로세스** : 각 변수 내 확률적 수치의 변화\n",
    "\n",
    "    $$Y={..., Y_{-2}, Y_{-1}, Y_0, Y_1, Y_2, ...}, X_1={..., X_{1, -2}, X_{1, -1}, X_{1, 0}, X_{1, 1}, X_{1, 2}, ...}, X_2={..., X_{2, -2}, X_{2, -1}, X_{2, 0}, X_{2, 1}, X_{2, 2}, ...}, ...$$\n",
    "\n",
    "    - 종속변수($Y_t$) 또는 독립변수($X_{1, t}$)가 시간단위($t$)를 포함\n",
    "\n",
    "    - 모델링읜 출력(Output)은 변수 $Y$의 특정 시간 $t$에서의 예측값 ($\\hat{Y}_t$)\n",
    "\n",
    "- **시계열 데이터(Time Series Data)** : 일정한 시간 간격에 따라 순차적(Sequentially)으로 기록된 확률적 프로세스 또는 시간변화 데이터\n",
    "\n",
    "    - 시간의 흐름에 따라 불규칙적 변동을 분석하기 위해 필수적 데이터\n",
    "\n",
    "    - 과거가 미래에 어떤 영향을 주는 지 분석을 통해 예측 가능\n",
    "\n",
    "    - 최근 기계학습과 딥러닝을 사용하여 복잡한 데이터 예측\n",
    "\n",
    "    - 시계열예측과 기계학습/딥러닝 간 전처리와 알고리즘 방식 차이 때문에 별도로 (1) 시계열 데이터분석 단계 이해, (2) 시계열 데이터의 전처리, (3) 시계열 알고리즘 이해 필수!\n",
    "\n",
    "<img src='./img/TS_Example1.png'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='./img/TS_Example2.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시계열 데이터의 활용\n",
    "\n",
    "- 관측된 시계열 데이터를 분석하여 미래를 예측하는 문제가 바로 시계열 예측 문제\n",
    "\n",
    "- 흔하게 접하는 문제로 주로 경제 지표를 예측하거나 시간에 따른 정당지지율을 예측하거나, 어떤 상품의 수요를 예측하는 문제에 이르기까지 다양하게 활용되고 있으며 현실에 가까울수록 항상 고려해야 하는 문제\n",
    "\n",
    "<img src='./img/TS_Applications.png'>\n",
    "\n",
    "- 예측된 결과를 바탕으로 여러 정책이나 비즈니스 전략을 결정하는 과정에 활용되기에, 실제 비즈니스 영역에서는 시계열 예측 문제가 매우 중요\n",
    "\n",
    "<img src='./img/DataUseCase.png'>\n",
    "\n",
    "- McKinsey Global Institute에 따르면, 시계열 데이터가 텍스트나 이미지 데이터 보다 더 큰 잠재적 가치를 가지고 있다고 보고있음\n",
    "\n",
    "<img src='./img/DataPotential.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터분석 단계의 변화\n",
    "\n",
    "**\"시계열예측과 기계학습/딥러닝 간 전처리와 알고리즘 방식 차이 때문에, 별도로 (1) 시계열 데이터분석 단계 이해, (2) 시계열 데이터의 전처리, (3) 시계열 알고리즘 이해 필수!\"**\n",
    "\n",
    "<img src='./img/DataAnalysis_CycleTS0.png'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='./img/DataAnalysis_Cycle1.png'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='./img/DataAnalysis_CycleTS1.png'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='./img/DataAnalysis_CycleTS2.png'>\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src='./img/DataAnalysis_CycleTS5.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [시계열 알고리즘 종류](https://paperswithcode.com/area/time-series)\n",
    "\n",
    "- 통계추론, 기계학습 및 딥러닝의 흐름에 시간패턴을 반영하려 진화\n",
    "\n",
    "- 지도학습(예측 분류), 비지도학습 문제에 모두 활용되는 필수 알고리즘\n",
    "\n",
    "- 미래 예측을 포함한 추천 서비스와 같은 비즈니스에 활용중\n",
    "\n",
    "<img src='./img/TS_Evolution.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 통계추론(Statistical Inference) 알고리즘: 통계분포에 기반한 설명력 중시 알고리즘\n",
    "\n",
    "<img src='./img/TS_Algorithm_Concept.png'>\n",
    "\n",
    "**(1) 단변량 선형기반**: Y가 1개 & Y와 X의 관계를 선형 가정\n",
    "\n",
    "- Linear Regression\n",
    "\n",
    "- ARIMA(AutoRegressive Integrated Moving Average)\n",
    "\n",
    "- ARIMAX\n",
    "\n",
    "- SARIMAX\n",
    "\n",
    "**(2) 다변량 선형기반**: Y가 2개 이상 & Y와 X의 관계를 선형 가정\n",
    "\n",
    "- Bayesian-based Models\n",
    "\n",
    "- Vector Autoregression(VAR)\n",
    "\n",
    "- Vector Error Correction Model(VECM)\n",
    "\n",
    "**(3) 비선형기반**: Y와 X의 관계를 비선형 가정\n",
    "\n",
    "- Exponential Smoothing\n",
    "\n",
    "- ETS(Error/Trend/Seasonal)\n",
    "\n",
    "- Kalman Filter\n",
    "\n",
    "- State Space Model\n",
    "\n",
    "- Change Point Detection(CPD)\n",
    "\n",
    "- Autoregressive conditional heteroskedasticity(ARCH)\n",
    "\n",
    "- Generalized Autoregressive Conditional Heteroskedasticity(GARCH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 기계학습/딥러닝 알고리즘: 컴퓨팅 기반 인공지능 알고리즘으로 정확성 높은 비선형 관계 추론\n",
    "\n",
    "- Prophet\n",
    "\n",
    "- Neural Prophet\n",
    "\n",
    "- RNN(Recurrent Neural Network)\n",
    "\n",
    "- LSTM(Long Short-Term Memory)\n",
    "\n",
    "- GRU(Gated Recurrent Unit)\n",
    "\n",
    "- Neural Networks Autoregression(NNAR)\n",
    "\n",
    "- Attention\n",
    "\n",
    "- Self-attention\n",
    "\n",
    "- Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Platforms: 글로벌 기업들이 독자적으로 개발한 시계열 분석 플랫폼 확대중\n",
    "\n",
    "- [Amazon Forecast](https://aws.amazon.com/ko/forecast/)\n",
    "\n",
    "- [Automated ML Time-series Forecasting at Microsoft Azure](https://azure.microsoft.com/en-us/blog/build-more-accurate-forecasts-with-new-capabilities-in-automated-machine-learning/)\n",
    "\n",
    "- [Time Series Forecasting with Google Cloud AI Platform](https://codelabs.developers.google.com/codelabs/time-series-forecasting-with-cloud-ai-platform#0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전처리 방향 (Preprcessing)\n",
    "\n",
    "**\"시계열예측과 기계학습/딥러닝 간 전처리와 알고리즘 방식 차이 때문에, 별도로 (1) 시계열 데이터분석 단계 이해, (2) 시계열 데이터의 전처리, (3) 시계열 알고리즘 이해 필수!\"**\n",
    "\n",
    "- **목표**\n",
    "\n",
    "    - 대량으로 수집된 데이터는 그대로 활용 어려움\n",
    "\n",
    "    - 잘못 수집/처리 된 데이터는 엉뚱한 결과를 발생\n",
    "\n",
    "    - 알고리즘이 학습이 가능한 형태로 데이터를 정리\n",
    "\n",
    "<img src='./img/DataAnalysis_Time.jpg' width=500>\n",
    "\n",
    "- **일반적인 전처리 필요항목**\n",
    "\n",
    "    - 데이터 결합\n",
    "\n",
    "    - 결측값 처리\n",
    "\n",
    "    - 이상치 처리\n",
    "\n",
    "    - 자료형 변환\n",
    "\n",
    "    - 데이터 분리\n",
    "\n",
    "    - 데이터 변환\n",
    "\n",
    "    - 스케일 조정\n",
    "\n",
    "$\\Rightarrow$ **\"알고리즘의 범위와 종류가 다양하여, 각 알고리즘의 입력에 맞게 변환하는 것이 최선\"**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 시계열 변수추출 (Feature Engineering)\n",
    "\n",
    "- 데이터 과학자들은 보통 수동/자동 변수 처리 및 변환(Feature Engineering)에 익숙하지만, 새로운 변수를 생성하는 것은 분석에서 가장 중요하고 시간이 많이 걸리는 작업 중 하나이며, 머신러닝과 딥러닝의 발전으로 점차 자동화 중\n",
    "\n",
    "<img src='./img/DL_AutoFE.png'>\n",
    "\n",
    "- **변수 생성시 주의할 점!**\n",
    "\n",
    "    - 미래의 실제 종속변수 예측값이 어떤 독립/종속변수의 Feature Engineering에 의해 효과가 있을지 단정할 수 없음\n",
    "\n",
    "    - 독립변수의 예측값을 Feature Engineering를 통해 생성될 수 있지만 이는 종속변수의 예측에 오류증가를 야기할 수 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 구성요소 및 시간빈도\n",
    "\n",
    "#### 1) 시계열 데이터는 여러 구성 성분\n",
    "\n",
    "- **시계열 = 체계적 성분 + 불규칙 성분**\n",
    "\n",
    "<img src='./img/TimeSeries_Decomposition.png' width=500>\n",
    "\n",
    "| 시계열 방향 | 시계열 구성요소 | 의미 | \n",
    "|:---: |:--- |:--- | \n",
    "| 장기적 움직임 | 추세(Trend) | 장기적으로 시간흐름에 따른 데이터의 상승/하강 경향 | \n",
    "| 단기적 움직임 | 계절성(Seasonality) | 특정기간 간격으로 반복적으로 나타나는 패턴 <br> 주기(Cycle) | 계절변동으로 설명되지 않는 패턴(기후변화, 정책변화, 사회관습 등) | \n",
    "| 불규칙 움직임 | 에러(Error) | 사전(Prior)적으로 예측할 수 없는 특수 패턴(지진, 전쟁, 파업 등) |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 여러 구성성분들이 단기적 및 장기적으로 복합적으로 융합되어 작동\n",
    "\n",
    "<img src='./img/TS_Example3.png' width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3)  빈도(Frequency): 사람이 인지하는 데이터의 빈도를 컴퓨터는 명확히 인지하지 못하기 때문에 별도 설정필요\n",
    "\n",
    "- **계절성 패턴(Seasonality)**이 나타나기 전까지의 데이터 갯수로 분석가가 반영\n",
    "\n",
    "    - 예시 1\n",
    "\n",
    "    | Data | Seasonality | Frequency |\n",
    "    |:---:|:---:|:---:| \n",
    "    | Annual | Annual | 1 | \n",
    "    | Quarterly | Annual | 4 | \n",
    "    | Monthly | Annual | 12 | \n",
    "    | Weekly | Annual | 52 |\n",
    "\n",
    "    - 예시 2\n",
    "\n",
    "    | Data | Seasonality | Frequency |\n",
    "    |:---:|:---:|:---:| \n",
    "    | Daily(데이터가 Day 단위로 수집) | Weekly | 7 | \n",
    "    | | Annual | 365 |\n",
    "\n",
    "    - 예시 3\n",
    "\n",
    "    | Data | Seasonality | Frequency |\n",
    "    |:---:|:---:|:---:| \n",
    "    | Minutely(데이터가 Minute 단위로 수집) | Hourly | 60 | \n",
    "    | | Daily | 24 x 60 |\n",
    "    | | Weekly | 24 x 60 x 7 | \n",
    "    | | Annual | 24 x 60 x 365 |\n",
    "    \n",
    "\n",
    "- **빈도 설정을 위한 Python 함수 옵션**\n",
    "\n",
    "    | **Alias** | **Description** |\n",
    "    |:---:|:---:|\n",
    "    | **B** | Business day |\n",
    "    | **D** | Calendar day |\n",
    "    | **W** | Weekly |\n",
    "    | **M** | Month end |\n",
    "    | **Q** | Quaterly end |\n",
    "    | **A** | Year end |\n",
    "    | **BA** | Business year end |\n",
    "    | **AS** | Year start |\n",
    "    | **H** | Hourly frequency |\n",
    "    | **T, min** | Minutely frequency |\n",
    "    | **S** | Secondly frequency |\n",
    "    | **L, ms** | Millisecond frequency |\n",
    "    | **U, us** | Microsecond frequency |\n",
    "    | **N, ns** | Nanosecond frequency |\n",
    "\n",
    "- 빈도 설정 후 비어있는 시간 결측치가 발견되거나 빈도 변경으로 발생하는 **결측치를 채우기 위한 Python 함수 옵션**\n",
    "\n",
    "    - 시계열에는 노이즈 데이터 또는 관찰되지 못한 기간이 종종 존재\n",
    "    \n",
    "    - 측정하고 데이터를 기록하는 과정에서의 오류나 예측치 못한 상황으로 인해 발생\n",
    "    \n",
    "    - 예를들어 상품의 품절로 인하여 장기간 판매량이 없는 경우 데이터는 없을 수 있고 이를 적절하게 전처리 하는 것이 실제 문제해결 성능에 매우 중요\n",
    "\n",
    "    | **Method** | **Description** |\n",
    "    |:---:|:---:|\n",
    "    | **bfill** | Backeard fill |\n",
    "    | **count** | Count of values |\n",
    "    | **ffill** | Forward fill |\n",
    "    | **first** | First valid data value |\n",
    "    | **last** | Last valid data value |\n",
    "    | **max** | Maximum data value |\n",
    "    | **mean** | Mean of values in time range |\n",
    "    | **median** | Median of values in time range |\n",
    "    | **min** | Minimum data value |\n",
    "    | **nunique** | Number of unique values |\n",
    "    | **ohlc** | Opening value, highest value, lowest value, closing value |\n",
    "    | **pad** | Same as forward fill |\n",
    "    | **std** | Standard deviation of values |\n",
    "    | **sum** | Sum of values |\n",
    "    | **var** | Variance of values |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추세/계절성/주기/지연값 등\n",
    "\n",
    "#### 1) 추세(Trend, $T_t$)\n",
    "\n",
    "- 시계열이 시간에 따라 증가, 감소 또는 일정 수준을 유지하는 경우\n",
    "\n",
    "**(시각적 이해)**\n",
    "\n",
    "<img src='./img/Trend_Increasing.png' width=500>\n",
    "\n",
    "<img src='./img/Trend_Decreasing.png' width=500>\n",
    "\n",
    "<img src='./img/Trend_None.png' width=500>\n",
    "\n",
    "**(수학적 이해)**\n",
    "\n",
    "- 확률과정의 기댓값 함수를 알아내는 것\n",
    "\n",
    "- 확률과정($Y_t$)이 추정이 가능한 추세함수($f(t)$)와 정상확률과정($Y_t^s$)의 합\n",
    "\n",
    "$$Y_t=f(t)+Y_t^s$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 계절성(Seasonality, $S_t$)\n",
    "\n",
    "- 일정한 빈도로 주기적으로 반복되는 패턴($m$), 특정한 시간값(달/요일)에 따라 기대값이 달라지는 것\n",
    "\n",
    "- **예시** : 주기적 패턴이 12개월마다 반복($m=12$)\n",
    "\n",
    "<img src='./img/Seasonal.png' width=500>\n",
    "\n",
    "- **대표적 계절성 변수 생성 2가지**\n",
    "\n",
    "    - 수치값 그대로 반영 $\\rightarrow$ `LabelEncoding`\n",
    "\n",
    "        <img src='./img/Label_Encoding.png' width=300>\n",
    "\n",
    "    - 계절성 주기 내 시점마다 별도 변수 생성 $\\rightarrow$ `OneHotEncoding`\n",
    "\n",
    "        <img src='./img/Dummy_Engineering.png' width=500>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 주기(Cycle, $C_t$)\n",
    "\n",
    "- 일정하지 않은 빈도로 발생하는 패턴(계절성과 다름)\n",
    "\n",
    "- 예시: 빈도가 1인 경우에도 발생 가능($m=1$)\n",
    "\n",
    "<img src='./img/Cycle.png' width=400>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 시계열 분해(추세/계절성/잔차(Residual, $e_t$))\n",
    "\n",
    "<img src='./img/Decomposed_into_its_trend_seasonal_and_irregular.jpeg' width=600>\n",
    "\n",
    "**(1) 가법모형(Additive Model)**: 추세 + 계절성 + 주기 + 오차\n",
    "\n",
    "- 구성요소간 독립성을 가정하고 시계열이 구성요소의 합 가정\n",
    "\n",
    "- 주로 계절성분의 진폭/분산이 시간이 흘러도 일정한 경우 사용\n",
    "\n",
    "**(2) 승법모형(Multiplicative Model)**: 추세 * 계절성 * 주기 * 오차\n",
    "\n",
    "- 구성요소간 비독립성을 가정하고 시계열이 구성요소의 곱 가정\n",
    "\n",
    "- 주로 계절성분의 진폭/분산이 시간에 따라 일정하지 않은 경우 사용"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) 지연값(Lagged values, $Lag(X_1)$) \n",
    "\n",
    "- 변수의 지연된 값들로 별도 독립변수를 생성하는 것으로, ARIMA/VAR/NNAR(Neural Network Autoregression) 등의 알고리즘도 내부적으로 활용\n",
    "\n",
    "<img src='./img/Lag-explanation.png' width=400>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) 시간변수 \n",
    "\n",
    "- 시간정보가 담고 있는 년/월/일/요일 등 자체를 별도 독립적인 변수로 생성"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 요약\n",
    "\n",
    "- 시계열 구성요소는 각 변수의 시간패턴을 파악하는데 중요\n",
    "\n",
    "- FE를 통해 생성된 변수의 입력(Input) 형태나 빈도로 알고리즘 선택시 고려해야\n",
    "\n",
    "- 생성된 변수의 패턴이 기존 모델에서 반영하지 않던 패턴이라면 예측 성능을 높임\n",
    "\n",
    "- 예측성능 향상 뿐 아니라 결과를 해석하고 해당 속성을 분석하며 가능한 원인 식별에 도움"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 분리 (Data Split)\n",
    "\n",
    "- **배경**\n",
    "\n",
    "    **(1) 독립변수와 종속변수 구분**\n",
    "\n",
    "    | 대분류 | 의미/예시 |\n",
    "    |:---: |:---: | \n",
    "    | 독립변수(Independent Variable) | 다른 변수에 영향을 미치는 변수 (X) | \n",
    "    | 종속변수(Dependent Variable) | 다른 변수에 의해 영향을 받는 변수 (Y) |\n",
    "\n",
    "    **(2) 과거/현재와 미래 기간 구분**\n",
    "\n",
    "    - 과거/현재의 상황을 분석하고, 미래를 예측 할 수 있는 환경 구축\n",
    "\n",
    "    - **Training Period**: 과거/현재의 상황을 분석\n",
    "\n",
    "    <img src='./img/DataSplit_Concept1.png'>\n",
    "\n",
    "    - **Testing Period**: 미래를 예측 할 수 있는 환경\n",
    "\n",
    "    <img src='./img/DataSplit_Concept2.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 간단한 방법(Holdout Validation)\n",
    "\n",
    "- **훈련셋(Training set)**: 일반적으로 전체 데이터의 70% 사용\n",
    "\n",
    "- **테스트셋(Testing set)**: 일반적으로 전체 데이터의 30% 사용"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 일반적 방법(Simple Validation)\n",
    "\n",
    "- **훈련셋(Training set)**: 일반적으로 전체 데이터의 60%를 사용\n",
    "\n",
    "- **검증셋(Validation set)**:\n",
    "\n",
    "    - 개발셋이라고도 하며, 일반적으로 전체 데이터의 20%를 사용함\n",
    "\n",
    "    - 훈련된 여러가지 모델들의 성능을 테스트 하는데 사용되며 모델 선택의 기준이 됨\n",
    "\n",
    "- **테스트셋(Testing set)**: 일반적으로 전체 데이터의 20%를 사용하며 최종 모델의 정확성을 확인하는 목적에 사용됨\n",
    "\n",
    "<img src='./img/DataSplit_Simple.png' width=500>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) K교차검사 (K-fold cross validation)\n",
    "\n",
    "(1) 훈련셋을 복원없이 K개로 분리한 후, K-1는 하위훈련셋으로 나머지 1개는 검증셋으로 사용함\n",
    "\n",
    "(2) 검증셋과 하위훈련셋을 번갈아가면서 K번 반복하여 각 모델별로 K개의 성능 추정치를 계산\n",
    "\n",
    "(3) K개의 성능 추정치 평균을 최종 모델 성능 기준으로 사용\n",
    "\n",
    "<img src='./img/DataSplit_Kfold.png' width=500>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) K-fold vs. Random-subsamples vs. Leave-out vs. Leave-p-out\n",
    "\n",
    "- **K-fold**\n",
    "\n",
    "<img src='./img/DataSplit_ver1.png' width=500>\n",
    "\n",
    "- **Random-subsamples**\n",
    "\n",
    "<img src='./img/DataSplit_ver3.png' width=500>\n",
    "\n",
    "- **Leave-p-out**\n",
    "\n",
    "<img src='./img/DataSplit_ver4.png' width=500>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시계열 데이터 분리\n",
    "\n",
    "#### 1) 시계열 데이터인 경우 랜덤성(set.seed) 없이 시간축 유지가 핵심!\n",
    "\n",
    "- **훈련셋 < 검증셋 < 테스트셋**\n",
    "\n",
    "    - **훈련셋(Training set)**: 가장 오래된 데이터\n",
    "\n",
    "    - **검증셋(Validation set)**: 그 다음 최근 데이터\n",
    "\n",
    "    - **테스트셋(Testing set)**: 가장 최신의 데이터\n",
    "\n",
    "    <img src='./img/DataSplit_TimeSeries.png' width=500>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 최선의 성능을 가지는 Robust 모델링을 위해 시계열 교차검증 필수!\n",
    "\n",
    "(1) **모든 미래 시점을 한번에 예측하면 정확성 낮아짐**: 미래 시점마다 Train/Test 재분류 필요\n",
    "\n",
    "- **Sliding Window**: 고정된 사이즈의 Train & Test Window를 움직이며 데이터를 재분리\n",
    "\n",
    "- **Expanding Window**: 가변 사이즈 Train Window & 고정된 사이즈 Test Window를 움직이며 데이터를 재분리\n",
    "\n",
    "<img src='./img/DataSplit_TSWindow.png'>\n",
    "\n",
    "(2) 단기/중기/장기 모든 미래시점에 동일 성능을 가지는 Robust 알고리즘은 없기 때문에 시점마다 별도 모델링 필요\n",
    "\n",
    "- **1스텝 교차검사(One-step Ahead Cross-validation)**\n",
    "\n",
    "<img src='./img/DataSplit_TimeSeries_ver1.png' width=500>\n",
    "\n",
    "- **2스텝 교차검사(Two-step Ahead Cross-validation)**\n",
    "\n",
    "<img src='./img/DataSplit_TimeSeries_ver2.png' width=500>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 변환 (Realitu & Feature Selection)\n",
    "\n",
    "### 시간현실 반영 (Reality)\n",
    "\n",
    "**미래의 시간패턴을 미리 반영하는건 비현실적, 이는 과적합(Overfitting) 유발**\n",
    "\n",
    "- 데이터 전처리시, 데이터 분리 후 패턴을 추출하여 해결\n",
    "\n",
    "<img src='./img/DataAnalysis_CycleTS0.png'>\n",
    "\n",
    "<img src='./img/DataAnalysis_Cycle1.png'>\n",
    "\n",
    "<img src='./img/DataAnalysis_CycleTS1.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 변수들의 독립성 향상(Condition Number)\n",
    "\n",
    "#### 1) 배경: 예측 성능을 향상시키려면 독립변수들의 독립성이 높아야 함\n",
    "\n",
    "- 이상적: Train 예측성능 $\\uparrow$ + Test 예측성능 $\\uparrow$\n",
    "\n",
    "- 현실적: Train 예측성능 <<< Test 예측성능 $\\Leftrightarrow$ 낮은 조건수(Condition Number)\n",
    "\n",
    "#### 2) 낮은 조건수란?\n",
    "\n",
    "**(비수학적 이해)**\n",
    "\n",
    "- 독립변수들($X$)의 상호의존도(관련성)가 분석결과($Y$)에 주는 영향을 줄이고, 독립변수 각각의 순수한 영향만을 반영\n",
    "\n",
    "**(수학적 이해)**\n",
    "\n",
    "- 독립변수($X$)가 움직일 수 있는 변동성(공분산)을 줄여 분석결과(Y)의 변동을 줄여 신뢰성 향상\n",
    "\n",
    "- **변동성(공분산)?**: 알고리즘이 예측 영향력(계수/가중치)추정 과정에서 발생하는 역행렬의 변화에 오차가 미치는 영향\n",
    "\n",
    "$$\\text{Condition Number(조건수)}=\\frac{\\lambda_{max}}{\\lambda_{min}}$$\n",
    "\n",
    "$$\\lambda_{max}=\\max[\\text{eigenvalue}\\{Cov(X^TX)\\}]$$\n",
    "\n",
    "$$\\lambda_{max}=\\min[\\text{eigenvalue}\\{Cov(X^TX)\\}]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 조건수가 작을 때\n",
    "# X 데이터\n",
    "import numpy as np\n",
    "A = np.eye(4)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y 데이터\n",
    "Y = np.ones(4)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 계수 추정\n",
    "np.linalg.solve(A, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0001, 0.    , 0.    , 0.    ],\n",
       "       [0.    , 1.0001, 0.    , 0.    ],\n",
       "       [0.    , 0.    , 1.0001, 0.    ],\n",
       "       [0.    , 0.    , 0.    , 1.0001]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X 데이터 오차반영\n",
    "A_new = A + 0.0001 * np.eye(4)\n",
    "A_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99990001, 0.99990001, 0.99990001, 0.99990001])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 계수 추정\n",
    "np.linalg.solve(A_new, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 조건수 확인\n",
    "np.linalg.cond(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.5       , 0.33333333, 0.25      ],\n",
       "       [0.5       , 0.33333333, 0.25      , 0.2       ],\n",
       "       [0.33333333, 0.25      , 0.2       , 0.16666667],\n",
       "       [0.25      , 0.2       , 0.16666667, 0.14285714]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 조건수가 클 때\n",
    "# X 데이터\n",
    "from scipy.linalg import hilbert\n",
    "A = hilbert(4)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y 데이터\n",
    "Y = np.ones(4)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -4.,   60., -180.,  140.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 계수 추정\n",
    "np.linalg.solve(A, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0001    , 0.5       , 0.33333333, 0.25      ],\n",
       "       [0.5       , 0.33343333, 0.25      , 0.2       ],\n",
       "       [0.33333333, 0.25      , 0.2001    , 0.16666667],\n",
       "       [0.25      , 0.2       , 0.16666667, 0.14295714]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터 오차 반영\n",
    "A_new = A + 0.0001 * np.eye(4)\n",
    "A_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -0.58897672,  21.1225671 , -85.75912499,  78.45650825])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 계수 추정\n",
    "np.linalg.solve(A_new, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15513.738738929662"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 조건수 확인\n",
    "np.linalg.cond(A)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 조건수를 낮추는 방법: 예측 안정성(Robust/Goodfit)을 높이는 방법\n",
    "\n",
    "(1) 변수들의 단위차이로 숫자의 스케일들이 크게 다른 경우, 스케일링(Scaling)으로 해결 가능\n",
    "\n",
    "(2) 독립변수들 간에 상관관계(상호의존성)가 높은 다중공선성 존재할 경우, Variance Inflation Factor(VIF)나 Principal Component Analysis(PCA)를 통한 변수선별로 해결 가능\n",
    "\n",
    "(3) 독립변수들 간 상호의존성이 높은 변수들에 패널티를 부여하는 정규화(Resularization)로 해결 가능"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중공선성(Multicollinearity)\n",
    "\n",
    "#### 1) 다중공선성(Multicollinearity)이 발생하는 경우\n",
    "\n",
    "- 특정 독립변수가 다른 독립변수의 조합으로 표현될 수 있는 경우\n",
    "\n",
    "- 독립변수들이 서로 독립이 아니라 상호상관관계가 강한 경우\n",
    "\n",
    "- 독립변수의 공분산 행렬(Covariance Matrix) 벡터공간(Vector Space)의 차원과 독립변수의 수가 달라 변수들이 모두 독립이 아닌 경우(Full Rank가 아니다)\n",
    "\n",
    "$\\Rightarrow$ 다중공선성이 있으면 독립변수 공분산 행렬의 조건수(Condition Number) 증가\n",
    "\n",
    "$\\Rightarrow$ 조건수(Condition Number)가 증가하면 과적합(Overfitting) 발생가능성 증가"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 다중공선성을 줄이는 방법\n",
    "\n",
    "(1) Variance Inflation Factor(VIF) 변수선택\n",
    "\n",
    "- VIF는 독립변수를 다른 독립변수들로 선형회귀한 성능들을 비교하여 상호 의존성이 높은 변수를 제거하거나 의존성이 낮은 변수를 선택\n",
    "\n",
    "$$VIF_i=Var(\\hat{\\beta}_i)=\\frac{\\sigma^2}{(n-1)Var(X_i)}\\cdot\\frac{1}{1-R_i^2}$$\n",
    "\n",
    "$$(R_i^2\\text{ : 독립변수 $X_i$를 다른 독립변수들로 선형회귀한 성능})$$\n",
    "\n",
    "\n",
    "(2) Principal Component Analysis(PCA) 변수선택\n",
    "\n",
    "- PCA는 다차원에서 상호 관련성이 높은 독립변수들을 관련성이 없는 서로 독립인 소차원으로 변경하는 알고리즘으로, 이를 통해 상호 의존성을 제거\n",
    "\n",
    "<img src='./img/PCA_2D_Ex.png'>\n",
    "\n",
    "<img src='./img/PCA_Visualization.png'>\n",
    "\n",
    "<img src='./img/PCA_MultiEx.png'>\n",
    "\n",
    "<img src='./img/PCA_MultiExPC1PC2.png' width=600>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [스케일 조정(Scaling)](https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-download-auto-examples-preprocessing-plot-all-scaling-py)\n",
    "\n",
    "- **목적**: 변수들의 크기를 일정하게 맞추어 크기 때문에 영향이 높은 현상을 회피\n",
    "\n",
    "    - **수학적**: 독립 변수의 공분산 행렬 조건수(Condition Number)를 감소시켜 최적화 안정성 및 수렴 속도 향상\n",
    "\n",
    "    - **컴퓨터적**: PC 메모리를 고려하여 오버플로우(Overflow)나 언더플로우(Underflow)를 줄여줌    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Standard Scaler\n",
    "\n",
    "$$\\frac{X_{it}-E(X_i)}{SD(X_i)}$$\n",
    "\n",
    "- 기본 스케일로 평균을 제외하고 표준편차를 나누어 변환\n",
    "\n",
    "- 각 변수(Feature)가 정규분포를 따른다는 가정이기에 정규분포가 아닐 시 최선이 아닐 수 있음\n",
    "\n",
    "```python\n",
    "sklearn.preprocessing.StandardScaler().fit()\n",
    "sklearn.preprocessing.StandardScaler().transform()\n",
    "sklearn.preprocessing.StandardScaler().fit_transform()\n",
    "```\n",
    "\n",
    "<img src='./img/Scaling_StandardScaler.png' width=500>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Min-Max Scaler\n",
    "\n",
    "$$\\frac{X_{it}-\\min(X_i)}{\\max(X_i)-\\min(X_i)}$$\n",
    "\n",
    "- 가장 많이 활용되는 방식으로 최소~최대 값이 0~1 또는 -1~1 사이의 값으로 변환\n",
    "\n",
    "- 각 변수(Feature)가 정규분포가 아니거나 표준편차가 매우 작을 때 효과적\n",
    "\n",
    "```python\n",
    "sklearn.preprocessing.MinMaxScaler().fit()\n",
    "sklearn.preprocessing.MinMaxScaler().transform()\n",
    "sklearn.preprocessing.MinMaxScaler().fit_transform()\n",
    "```\n",
    "\n",
    "<img src='./img/Scaling_MinMaxScaler.png' width=500>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Robust Scaler\n",
    "\n",
    "$$\\frac{X_{it}-Q_1(X_i)}{Q_3(X_i)-Q_1(X_i)}$$\n",
    "\n",
    "- 최소-최대 스케일러와 유사하지만 최소/최대 대신에 IQR(Interquartile Range) 중 25%값/75%값을 사용하여 변환\n",
    "\n",
    "- 이상치(Outlier)에 영향을 최소화하였기에 이상치가 있는 데이터에 효과적이고 적은 데이터에도 효과적인 편\n",
    "\n",
    "```python\n",
    "sklearn.preprocessing.RobustScaler().fit()\n",
    "sklearn.preprocessing.RobustScaler().transform()\n",
    "sklearn.preprocessing.RobustScaler().fit_transform()\n",
    "```\n",
    "\n",
    "<img src='./img/Scaling_RobustScaler.png' width=500>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Normalizer\n",
    "\n",
    "$$\\frac{X_{it}}{\\sqrt{X_i^2+X_j^2+...+X_k^2}}$$\n",
    "\n",
    "- 각 변수(Feature)를 전체 n개 모든 변수들의 크기들로 나누어서 변환(by Cartesian Coordinates)\n",
    "\n",
    "- 각 변수들의 값은 원점으로부터 반지름 1만큼 떨어진 범위 내로 변환\n",
    "\n",
    "```python\n",
    "sklearn.preprocessing.Normalizer().fit()\n",
    "sklearn.preprocessing.Normalizer().transform()\n",
    "sklearn.preprocessing.Normalizer().fit_transform()\n",
    "```\n",
    "\n",
    "<img src='./img/Scaling_Normalizer.png' width=500>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델링 방향 (Modeling)\n",
    "\n",
    "시계열예측과 기계학습/딥러닝 간 전처리와 알고리즘 방식 차이 때문에, 별도로 (1) 시계열 데이터분석 단계 이해, (2) 시계열 데이터의 전처리, (3) 시계열 알고리즘 이해 필수!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 시계열 알고리즘의 특징과 종류\n",
    "\n",
    "(1) 학습된 데이터 범위 내 패턴 뿐 아니라 외부 순차적 시점들의 패턴으로도 확장 할 수 있어야 시계열 알고리즘\n",
    "\n",
    "- 대다수의 기계학습 알고리즘은 데이터 범위 내 패턴만을 추출\n",
    "\n",
    "- 시계열 데이터나 FE를 통해 생성된 변수들은 미래시점의 패턴으로도 확장시킬 수 있음\n",
    "\n",
    "(2) 시계열 알고리즘은 특정시점(점추정)이 아닌 특정시점의 값의범위(구간추정)를 추정하는 알고리즘으로 설명력을 보유\n",
    "\n",
    "- 대부분의 기계학습 알고리즘은 통계분포에 기반하지 않기 때문에 점추정 알고리즘\n",
    "\n",
    "- 값의범위 정확성을 담보하진 않지만, 점추정 보다 다양한 해석을 가능하게 함\n",
    "\n",
    "- 정확성 vs. 설명력 반비례 관계 존재\n",
    "\n",
    "\n",
    "\"통계추론, 기계학습 및 딥러닝의 흐름에 시간패턴을 반영하려 진화\"\n",
    "\n",
    "\"지도학습(예측 분류), 비지도학습 문제에 모두 활용되는 필수 알고리즘\"\n",
    "\n",
    "\"미래 예측을 포함한 추천 서비스와 같은 비즈니스에 활용중\"\n",
    "\n",
    "<img src='./img/TS_Evolution.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 통계추론(Statistical Inference) 알고리즘: 통계분포에 기반한 설명력 중시 알고리즘\n",
    "\n",
    "<img src='./img/TS_Algorithm_Concept.png' width=600>\n",
    "\n",
    "**(1) 단변량 선형기반**: Y가 1개 & Y와 X의 관계를 선형 가정\n",
    "\n",
    "- Linear Regression\n",
    "\n",
    "- ARIMA(AutoRegressive Integrated Moving Average)\n",
    "\n",
    "- ARIMAX\n",
    "\n",
    "- SARIMAX\n",
    "\n",
    "**(2) 다변량 선형기반**: Y가 2개 이상 & Y와 X의 관계를 선형 가정\n",
    "\n",
    "- Bayesian-based Models\n",
    "\n",
    "- Vector Autoregression(VAR)\n",
    "\n",
    "- Vector Error Correction Model(VECM)\n",
    "\n",
    "**(3) 비선형기반**: Y와 X의 관계를 비선형 가정\n",
    "\n",
    "- Exponential Smoothing\n",
    "\n",
    "- ETS(Error/Trend/Seasonal)\n",
    "\n",
    "- Kalman Filter\n",
    "\n",
    "- State Space Model\n",
    "\n",
    "- Change Point Detection(CPD)\n",
    "\n",
    "- Autoregressive conditional heteroskedasticity(ARCH)\n",
    "\n",
    "- Generalized Autoregressive Conditional Heteroskedasticity(GARCH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 기계학습/딥러닝 알고리즘: 컴퓨팅 기반 인공지능 알고리즘으로 정확성 높은 비선형 관계 추론\n",
    "\n",
    "- Prophet\n",
    "\n",
    "- Neural Prophet\n",
    "\n",
    "- RNN(Recurrent Neural Network)\n",
    "\n",
    "- LSTM(Long Short-Term Memory)\n",
    "\n",
    "- GRU(Gated Recurrent Unit)\n",
    "\n",
    "- Neural Networks Autoregression(NNAR)\n",
    "\n",
    "- Attention\n",
    "\n",
    "- Self-attention\n",
    "\n",
    "- Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Platforms: 글로벌 기업들이 독자적으로 개발한 시계열 분석 플랫폼 확대중\n",
    "\n",
    "- [Amazon Forecast](https://aws.amazon.com/ko/forecast/)\n",
    "\n",
    "- [Automated ML Time-series Forecasting at Microsoft Azure](https://azure.microsoft.com/en-us/blog/build-more-accurate-forecasts-with-new-capabilities-in-automated-machine-learning/)\n",
    "\n",
    "- [Time Series Forecasting with Google Cloud AI Platform](https://codelabs.developers.google.com/codelabs/time-series-forecasting-with-cloud-ai-platform#0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [정상성(Stationarity Process)](https://en.wikipedia.org/wiki/Stationary_process)\n",
    "\n",
    "<img src='./img/DataAnalysis_CycleTS2.png'>\n",
    "\n",
    "#### 0) 배경: 시계열 데이터를 활용한 분석에서는 새로운 가정 필요\n",
    "\n",
    "| **데이터 방향** | **현실 가정** |\n",
    "| :------: | :-------: |\n",
    "| 시간 무관 일반적 데이터 | 모든 데이터 샘플들이 동일 분포에서 독립적으로 생성 <br> $\\rightarrow$ Independent and Identically Distributed (i.i.d.) |\n",
    "| 시간 관련 시계열 데이터 | i.i.d.가정 할 수 없고 현실문제를 단순화 한 새로운 가정 필요 <br> $\\rightarrow$ 시간이 흘러도 통계적/확률적 성질이 변하지 않는다는 정상성(Stationary) |\n",
    "\n",
    "- 새로운 가정 하에서, 정상성/비정상성 자체를 라벨로 활용하여 분류 지도학습 및 군집 비지도학습에도 활용"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 정상성의 의미 : 시간이 흘러도 통계적 특성(Statistical Properties)이 변하지 않고 일정\n",
    "\n",
    "- **통계적 특성(Statistical Properties)**: 주로 평균(Mean)과 분산(Variance)/공분산(Covariance)를 의미하지만, 가장 보수적으로는 이를 포함한 모든 분포적 특성을 포함\n",
    "\n",
    "- [Homoscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity) : 분산이 일정한(유한한, 발산하지않는) 경우를 의미\n",
    "\n",
    "- [Heteroscedasticity](https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity) : 분산이 발산하는 경우를 의미\n",
    "\n",
    "<img src='./img/Stationary(on).png' width=250>\n",
    "\n",
    "<img src='./img/Stationary(non).png' width=600>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 정상성의 종류\n",
    "\n",
    "- 변수 :\n",
    "\n",
    "$$X=\\{X_1, X_2, ..., X_n\\}$$\n",
    "\n",
    "- 확률과정 :\n",
    "\n",
    "$$Y={..., Y_{-2}, Y_{-1}, Y_0, Y_1, Y_2, ...}\\\\ X_1={..., X_{1, -2}, X_{1, -1}, X_{1, 0}, X_{1,1}, X_{1, 2}, ...}\\\\ X_2={..., X_{2, -2}, X_{2, -1}, X_{2,0}, X_{2, 1}, X_{2,2}, ...}$$\n",
    "\n",
    "**(1) 약정상(Weak Stationarity, Wide-sense Stationary Process)**\n",
    "\n",
    "(비수학적 이해)\n",
    "\n",
    "1. $..., X_{i,-1}, X_{1,0}, X_{i,1}...$ 변수의 각 시계열 샘플들은 동일 분포이며,\n",
    "\n",
    "2. $..., (X_{i,-3}, X_{i,-1}), (X_{i,-2}, X_{i,0}), (X_{i,-1}),X_{i,1})...$ 임의간격 샘플 2개씩의 묶음도 동일 분포\n",
    "\n",
    "(수학적 이해)\n",
    "\n",
    "1. $E(X_{it})=\\mu$, for all time $t$ (The first moment estimation)\n",
    "\n",
    "2. $Var(X_{it})=E(X_{it}^2)-E(X_{it})^2<\\infty$, for all time $t$ (The second moment estimation)\n",
    "\n",
    "3. $Cov(X_{is}, X_{ik})=Cov(X_{1(s+h)},X_{i(k+h)})=f(h)$, for all time $s,k,h$ (the cross moment estimation) $\\Rightarrow$ Covariance just depends on $h$.\n",
    "\n",
    "**(2) 강정상(Strong Stationarity, Strictly Stationary Process)**\n",
    "\n",
    "\"확률과정의 모든 분포 모멘트(Moment)가 시간 차이에만 의존하는 것(절대시간 미의존)\"\n",
    "\n",
    "(비수학적 이해)\n",
    "\n",
    "1. $..., X_{i,-1}, X_{i,0}, X_{i,1},... 변수의 각 시계열 샘플들은 동일 분포이며,\n",
    "\n",
    "2. $..., (X_{i,-3}, X_{i, -1}), (X_{i,-2}, X_{i,0}), (X_{i,-1}, X_{i,1}),...$ 임의간격 샘플 2개씩의 묶음도 동일 분포\n",
    "\n",
    "3. $..., (X_{i,-5}, X_{i,-3}, X_{i, -1}), (X_{i,-2}, X_{i,0}, X_{i, 2}), (X_{i,-1}, X_{i,1}, X_{i, 3}),...$ 임의간격 샘플 3개씩의 묶음도 동일 분포\n",
    "\n",
    "4. $(X_{i,-\\infty},..., X_{i,-1}, X_{i,1}, X_{i,3}, ..., X_{i, \\infty})$ 모든 시간 변화에도 동일 분포\n",
    "\n",
    "**(3) 강정상과 약정상의 관계: 강정상** $\\Rightarrow$ 약정상, but 약정상 $\\nRightarrow$ 강정상\n",
    "\n",
    "강정상 ${x_{it}}_{t=-\\infty}^{t=+\\infty}$의 특정 샘플 ${x_{it}}_{t=i_1}^{t=i_2}$의 평균과 분산까지만 일정한 약정상 일 수 있다.\n",
    "\n",
    "약정상 ${x_{}it}_{t=-\\infty}^{t=+\\infty}$이 3차 이상의 통계적 특성인 모멘텀(Skewness, Kurtosis)에서도 일정하면 강정상이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 백색잡음(White Noise, $WN$)\n",
    "\n",
    "- 강정상의 대표예시로 시계열분석 기본알고리즘 중 가장 중요함\n",
    "\n",
    "<img src='./img/White_Noise.png' width=500>\n",
    "\n",
    "(1) 잔차들은 정규분포이고, (unbiased) 평균 0과 일정한 분산을 가져야 함\n",
    "\n",
    "$${\\epsilon_t : t = ..., -2, -1, 0, 1, 2,...}~N(0,\\sigma_{\\epsilon_t}^2)$$\n",
    "$$\\text{where } \\epsilon_t \\sim i,i,d \\text{(independent and identically distributed)}$$\n",
    "$$\\epsilon_t=Y_i-\\bar{Y}_i$$\n",
    "$$E(\\epsilon_t)=0$$\n",
    "$$Var(\\epsilon_t)=\\sigma_{\\epsilon_t}^2$$\n",
    "$$Cov(\\epsilon_s,\\epsilon_k)=0 \\text{ for different times! }(s \\ne k)$$\n",
    "\n",
    "(2) 잔차들이 시간의 흐름에 따라 상관성이 없어야 함 : 자기상관함수(Autocorrelation function, ACF) =0 확인\n",
    "\n",
    "- 공분산(Covariance) : $Cov(Y_s, Y_k)=E[(Y_s-E(Y_s))(Y_k-E(Y_k))]=\\gamma_{s,k}$\n",
    "\n",
    "- 자기상관함수(Autocorrelation Function)\n",
    "\n",
    "$$Corr(Y_s, Y_k)=\\frac{Cov(Y_s, Y_k)}{\\sqrt{Var(Y_s)Var(Y_k)}}=\\frac{\\gamma_{s,k}}{\\sqrt{\\gamma_s\\gamma_k}}$$\n",
    "\n",
    "- 편자기상관함수(Partial Autocorrelation Function): $s$와 $k$사이의 상관성을 제거한 자기상관함수\n",
    "$$Corr[(Y_s-\\hat{Y},Y_{s-t}-\\hat{Y_{s-t}})] \\text{ for } \\$1$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 비정상 확률과정 (Non-stationary Process)\n",
    "\n",
    "- 추세가 있어서 평균인 일차 모멘트($E(y_t)$)가 0이 아니며 시간에 따라 변함\n",
    "\n",
    "- 추세가 없지만($E(y_t)=0$) 분산인 이차 모멘트($Var(y_t)$)가 시간에 따라 변함\n",
    "\n",
    "    - 랜덤 워크(Random Walk): 비정상(Non-stationary) 대표예시로 차분시 백색잡음으로 변경\n",
    "\n",
    "    $$Y_{it} = Y_{it-1}+\\epsilon_t$$\n",
    "    $$Y_{it}-Y_{it-1}=\\epsilon_t$$\n",
    "    $$\\epsilon_t\\sim N(0, \\sigma_{\\epsilon_t}^2)$$\n",
    "\n",
    "<img src='./img/Random_Walk.png' width=400>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) 정리\n",
    "\n",
    "| **예시** | **데이터** |\n",
    "|:-----:|:-----:|\n",
    "| 비정상 | 백색잡음(White Noise) |\n",
    "| 비정상성 | 랜덤워크(Random Walk |\n",
    "\n",
    "- 활용주요목적\n",
    "\n",
    "    1) 모델링: 시계열 모형은 데이터가 Stationary라 가정 $\\rightattow$ Stationary여야 예측 성능 높다\n",
    "\n",
    "    2) 잔차진단: 백색잡음 또한 Stationary $\\rightarrow$ 잔차도 Stationary여야 예측 성능 높다\n",
    "\n",
    "- 활용단어예시\n",
    "\n",
    "    - Stationary Process: 정상성인 시계열 데이터셋(프로세스)\n",
    "\n",
    "    - Stationary Model: 정상성인 시계열데이터를 설명하는 모델\n",
    "\n",
    "    - Trend Stationary: 트랜드를 제거하면 정상성인 시계열데이터\n",
    "\n",
    "    - Seasonal Stationary: 계절성을 제거하면 정상성인 시계열데이터\n",
    "\n",
    "    - Difference Stationary: 차분하면 정상성인 시계열데이터"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [정상성 테스트(Stationarity Test)](https://en.wikipedia.org/wiki/Unit_root_test)\n",
    "\n",
    "#### 1) 기초통계 테스트(by Summary Statistics)\n",
    "\n",
    "- 특정 시간(ex. 계절)에 따라 통계량이 일정한지 파악하여 정상성 확인\n",
    "\n",
    "#### 2) 시각화 테스트(by Visual)\n",
    "\n",
    "- 추세와 계절성이 없는지 파악하여 정상성 확인\n",
    "\n",
    "<img src='./img/Stationary_Visual.png' width=500>\n",
    "\n",
    "- **자기상관함수(Autocorrelation Function)**: 시차변화에 따른 자기상관을 의미\n",
    "\n",
    "    $$Corr(Y_{s},Y_k)=\\frac{Cov(Y_s, Y_k)}{\\sqrt{Var(Y_s)Var(Y_k)}}=\\frac{\\gamma_{s,k}}{\\sqrt{\\gamma_s\\gamma_k}}$$\n",
    "\n",
    "    - 만약 데이터가 정상시계열이면, ACF 그래프가 모든 시차에서 0에 수렴하고,\n",
    "\n",
    "    - 비정상시계열이면, ACF 그래프가 천천히 감소하거나 큰 양의 값\n",
    "\n",
    "<img src='./img/Stationary_ACFComparison.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 통계량 테스트(by Statistics Test)\n",
    "\n",
    "- 정상성 테스트 통계량으로 정상성 확인\n",
    "\n",
    "- [\"In statistics, a unit root test tests whether a time series variable is non-stationary and possesses a unit root.\"](https://en.wikipedia.org/wiki/Unit_root_test)\n",
    "\n",
    "- [Augmented Dickey-Fuller(ADF) test](https://en.wikipedia.org/wiki/Augmented_Dickey–Fuller_test) : 주로 추세제거에 효과\n",
    "\n",
    "**(1) 가설확인**\n",
    "\n",
    "| 종류 | 해석 |\n",
    "|:---:|:---:|\n",
    "|**대중주장(귀무가설, Null Hypothesis)** | 데이터는 단위근(unit root) 있다. / 비정상 상태 / 시간의존 구조 |\n",
    "|**나의주장(대립가설, Alternative Hypothesis)** | 데이터는 단위근 없다 / 정상 상태 / 시간의존 구조 아니다 |\n",
    "\n",
    "**(2) 유의수준 설정 및 유의확률 확인**\n",
    "\n",
    "- 유의수준: 5% (0.05) 분석가가 알아서 결정\n",
    "\n",
    "- 유의확률(p-value): 컴퓨터가 알아서 추정\n",
    "\n",
    "**(3) 의사결정**\n",
    "\n",
    "|**기준**|**의사결정**|**해석**|\n",
    "|:---:|:---:|:---:|\n",
    "|**p-value>=유의수준(ex. 0.05)**| 대중주장 참 | 내가 수집/분석한 데이터는 단위근이 있다 / 비정상 상태 / 시간의존 구도 |\n",
    "|**p-value<유의수준(ex. 0.05)** | 나의주장 참 | 내가 수집/분석한 데이터는 단위근이 없다 / 정상 상태 / 시간 의존 구도 아니다 |\n",
    "\n",
    "- [ADF-GLS test](https://en.wikipedia.org/wiki/ADF-GLS_test)\n",
    "\n",
    "    - 가설확인 : ADF와 동일\n",
    "\n",
    "- [Phillips-Perron(PP) test](https://en.wikipedia.org/wiki/Phillips–Perron_test)\n",
    "\n",
    "    - 가설확인 : ADF와 동일\n",
    "\n",
    "- [Kwiatkowski Phillips Schmidt Shin(KPSS) test](https://en.wikipedia.org/wiki/Phillips–Perron_test)\n",
    "\n",
    "    - 주로 계절성제거에 효과\n",
    "    \n",
    "    - 가설확인 : ADF와 반대"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정상성 이해실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 호출\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from random import seed, random\n",
    "import matplotlib.pyplot as plt\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# from module_timeseries import stationarity_adf_test, stationarity_kpss_test\n",
    "\n",
    "# 랜덤워크 데이터 생성 및 통계량 test (rho=0)\n",
    "plt.figure(figsize=(10, 4))\n",
    "seed(1)\n",
    "rho = 0\n",
    "random_walk = [-1 if random() < 0.5 else 1]\n",
    "for i in range(1, 1000):\n",
    "    white_noise = -1 if random() < 0.5 else 1\n",
    "    value = rho * random_walk[i-1] + white_noise\n",
    "    random_walk.append(value)\n",
    "plt.plot(random_walk)\n",
    "# plt.title('Rho : {}\\n ADF p-value : {}'.format(rho, np.ravel(stationarity_adf_test(random_walk, []))[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
