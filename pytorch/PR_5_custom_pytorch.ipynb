{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Layers and Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fully connected `nn.Linear` layer functional definition\n",
    "import torch\n",
    "\n",
    "def linear(input, weight, bias=None):\n",
    "    \n",
    "    if input.dim() == 2 and bias is not None:\n",
    "        \n",
    "        # fused op is marginally faster\n",
    "        ret = torch.addmm(bias, input, weight.t())\n",
    "    \n",
    "    else:\n",
    "        output = input.matmul(weight.t())\n",
    "        if bias is not None:\n",
    "            output += bias\n",
    "            \n",
    "        ret = output\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we derive the `nn.Linear` class from `nn.Module`\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Linear(nn.Module):\n",
    "    \n",
    "    # initialize input and output sizes, weights, and biases\n",
    "    def __init__(self, in_features, out_features, bias):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = Parameter(\n",
    "            torch.Tensor(out_features, in_features)\n",
    "        )\n",
    "        if bias:\n",
    "            self.bias = Parameter(\n",
    "                torch.Tensor(out_features)\n",
    "            )\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "            \n",
    "    def reset_paramet(self):\n",
    "        torch.nn.init.kaiming_uniform_(self.weight, a=torch.math.sqrt(5))\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / torch.math.sqrt(fan_in)\n",
    "            torch.nn.init.uniform_(self.bias, -bound, bound)\n",
    "            \n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        # define the forward pass\n",
    "        # use the functional definition of linear()\n",
    "        return F.linear(input, self.weight, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original : tensor([1.]) new : tensor([1.])\n",
      "original : tensor([2.]) new : tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor는 Tensor 객체를 받으며 메모리 주소값을 복사해 온다.\n",
    "original_data = torch.Tensor([1])\n",
    "new_data = torch.Tensor(original_data)\n",
    "print(f\"original : {original_data} new : {new_data}\")\n",
    "\n",
    "# original data를 수정\n",
    "original_data[0] = 2\n",
    "print(f\"original : {original_data} new : {new_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original : [1] new : tensor([1.])\n",
      "original : [2] new : tensor([1.])\n"
     ]
    }
   ],
   "source": [
    "# torch.Tensor()는 list나 numpy를 받으면 값을 복사해온다.\n",
    "original_data = [1]\n",
    "new_data = torch.Tensor(original_data)\n",
    "print(f\"original : {original_data} new : {new_data}\")\n",
    "\n",
    "# original data 수정\n",
    "original_data[0] = 2\n",
    "print(f\"original : {original_data} new : {new_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original : tensor([1]) new : tensor([1])\n",
      "original : tensor([2]) new : tensor([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/_c9pchvx7m9274g3530s1jcw0000gn/T/ipykernel_17316/330758021.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_data = torch.tensor(original_data)\n"
     ]
    }
   ],
   "source": [
    "# torch.torch의 경우 값을 복사해 Tensor 생성\n",
    "original_data = torch.tensor([1])\n",
    "new_data = torch.tensor(original_data)\n",
    "print(f\"original : {original_data} new : {new_data}\")\n",
    "\n",
    "# data 수정\n",
    "original_data[0] = 2\n",
    "print(f\"original : {original_data} new : {new_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will create a functional version of our complex linear layer\n",
    "def complex_linear(in_r, in_i, w_r, w_i, b_i, b_r):\n",
    "    out_r = (in_r.matmul(w_r.t())\n",
    "             - in_i.matmul(w_i.t()) + b_r)\n",
    "    out_i = (in_r.matmul(w_i.t())\n",
    "             + in_i.matmul(w_r.t()) + b_i)\n",
    "    \n",
    "    return out_r, out_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create out class verion of ComplexLinear based on nn.Module\n",
    "class ComplexLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weights_r = Parameter(torch.randn(out_features, in_features))\n",
    "        self.weights_i = Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias_r = Parameter(torch.randn(out_features))\n",
    "        self.bias_i = Parameter(torch.randn(out_features))\n",
    "        \n",
    "    def forward(self, in_r, in_i):\n",
    "        return F.complex_linear(in_r, in_i, self.weights_r, self.weights_i, self.bias_r, self.bias_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can create also use PyTorch's existing `nn.Linear` layer\n",
    "\n",
    "class ComplexLinearSimple(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(Linear, self).__init__()\n",
    "        self.fc_r = Linear(in_features, out_features)\n",
    "        self.fc_i = Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, in_r, in_i):\n",
    "        return (self.fc_r(in_r) - self.fc_i(in_i), \n",
    "                self.fc_r(in_i) + self.fc_i(in_r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_relu(input, thresh=0.0):\n",
    "    return torch.where(\n",
    "        input > thresh,\n",
    "        input,\n",
    "        torch.zeros_like(input)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyReLU(nn.Module):\n",
    "    def __init__(self, thresh = 0.0):\n",
    "        super(MyReLU, self).__init__()\n",
    "        self.thresh = thresh\n",
    "        \n",
    "    def foreward(self, input):\n",
    "        return my_relu(input, self.thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the functional version\n",
    "# a common way to import the functional package\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(D_in, H)\n",
    "        self.fc2 = nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # the functional version of ReLU is used tere\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the class version\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Activation Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom activation example - complex ReLU\n",
    "\n",
    "# functional version\n",
    "def complex_relu(in_r, in_i):\n",
    "    return (F.relu(in_r), F.relu(in_i))\n",
    "\n",
    "# class version\n",
    "class ComplexReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexReLU, self).__init__()\n",
    "        \n",
    "    def forward(self, in_r, in_i):\n",
    "        return complex_relu(in_r, in_i)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `tocchvision.model.alexnet()`\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=10000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11,\n",
    "                      stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool1d((6, 6))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "model_urls = {\n",
    "    'alexnet': 'https://pytorch.tips/alexnet-download',\n",
    "}\n",
    "\n",
    "def alexnet(pretrained=True, progress=True, **kwargs):\n",
    "    model = AlexNet(**kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls['alexnet'], progress=progress)\n",
    "    \n",
    "        model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our own custom function.\n",
    "\n",
    "def complex_mse_loss(input_r, input_i, target_r, target_i):\n",
    "    return (((input_r-target_r)**2).mean(),\n",
    "            ((input_i-target_i)**2).mean())\n",
    "\n",
    "class ComplexMSELoss(nn.Module):\n",
    "    def __init__(self, real_only=False):\n",
    "        super(ComplexMSELoss, self).__init__()\n",
    "        self.real_only = real_only\n",
    "        \n",
    "def forward(self, input_r, input_i, target_r, target_i):\n",
    "    if (self.real_only):\n",
    "        return F.mse_loss(input_r, target_r)\n",
    "    else:\n",
    "        return complex_mse_loss(input_r, input_i, target_r, target_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # Training\n",
    "    for data in train_dataloader:\n",
    "        input, targets = data\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        train_loss = criterion(output, targets)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # validation\n",
    "    with torch.no_grad():\n",
    "        for input, targets in val_dataloader:\n",
    "            output = model(input)\n",
    "            val_loss = criterion(output, targets)\n",
    "            \n",
    "    # testing\n",
    "    with torch.no_grad():\n",
    "        for input, targets in test_dataloader:\n",
    "            output = model(input)\n",
    "            test_loss = criterion(output, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some additional capabilities to our loops\n",
    "# this exmaple will demonstrate some simple tasks like printing information, reconfiguring a model, and adjusting a hyperparameter in the middle of training.\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    # pringing epoch, training, and validation loss\n",
    "    total_train_loss = 0.0\n",
    "    total_val_loss = 0.0\n",
    "    \n",
    "    # reconfiguring a model (best practice) -> fine tuning parameters updates after training on half of the epochs\n",
    "    if (epoch == epoch // 2):\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "        \n",
    "    # training\n",
    "    model.train()\n",
    "    for data in train_dataloader:\n",
    "        input, targets = data\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        train_loss = criterion(output, targets)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += train_loss\n",
    "        \n",
    "    # validation\n",
    "    # modifying a hyperparameter during trainig\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input, targets in val_dataloader:\n",
    "            output = model(input)\n",
    "            val_loss = criterion(output, targets)\n",
    "            total_val_loss += val_loss\n",
    "    print(\"\"\"Epoch: {}\n",
    "          Train Loss: {}\n",
    "          Val Loss: {}\"\"\".format(\n",
    "              epoch, total_train_loss, total_val_loss\n",
    "          ))\n",
    "    \n",
    "    # testing\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for input, targets in test_dataloader:\n",
    "            output = model(input)\n",
    "            test_loss = criterion(output, targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
