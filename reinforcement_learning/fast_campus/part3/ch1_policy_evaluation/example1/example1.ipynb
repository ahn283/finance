{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        state space : 4x4 grid info using numpy\n",
    "        value of the agent location : 1\n",
    "        value of the goal location : -1\n",
    "        \n",
    "        action_space : {0, 1, 2, 3}\n",
    "        0: up\n",
    "        1: right\n",
    "        2: down\n",
    "        3: left\n",
    "        '''\n",
    "        self.agent_pos = {'y': 0, 'x': 0}\n",
    "        self.goal_pos = {'y': 3, 'x': 3}\n",
    "        self.y_min, self.x_min, self.y_max, self.x_max = 0, 0, 3, 3\n",
    "        \n",
    "        # set up state\n",
    "        self.state = np.zeros([4, 4])\n",
    "        self.state[self.goal_pos['y'], self.goal_pos['x']] = -1\n",
    "        self.state[self.agent_pos['y'], self.agent_pos['x']] = 1\n",
    "        \n",
    "        # make a state space list\n",
    "        self.state_space = list()\n",
    "        for y in range(4):\n",
    "            for x in range(4):\n",
    "                state = np.zeros([4, 4])\n",
    "                state[self.goal_pos['y'], self.goal_pos['x']] = -1\n",
    "                state[y, x] = 1\n",
    "                self.state_space.append(state)\n",
    "        \n",
    "        # make a action space list\n",
    "        self.action_space = [0, 1, 2, 3]        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.agent_pos = {'y':0, 'x': 0}\n",
    "        self.state = np.zeros([4, 4])\n",
    "        self.state[self.goal_pos['y'], self.goal_pos['x']] = -1\n",
    "        self.state[self.agent_pos['y'], self.agent_pos['x']] = 1\n",
    "        \n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # update environment variables\n",
    "        if action == 0:\n",
    "            # 'y' should be decreased by 1 or stay the same when it is at the top row\n",
    "            self.agent_pos['y'] = max(self.agent_pos['y'] - 1, self.y_min)\n",
    "            \n",
    "        elif action == 1:\n",
    "            # 'x' should be increased by 1 or stay the same when it is at the most right column\n",
    "            self.agent_pos['x'] = min(self.agent_pos['x'] + 1, self.x_max)\n",
    "            \n",
    "        elif action == 2:\n",
    "            # 'y' should be increased by 1 or stay the same when it is at the bottom row\n",
    "            self.agent_pos['y'] = min(self.agent_pos['y'] + 1, self.y_max)\n",
    "        \n",
    "        elif action == 3:\n",
    "            # 'x' should be decreased by 1 or stay the same when it is at the most left column\n",
    "            self.agent_pos['x'] = max(self.agent_pos['x'] - 1, self.x_min)\n",
    "            \n",
    "        else:\n",
    "            assert False, \"Invalid action value was fed to step.\"\n",
    "            \n",
    "        # make a next state after transition\n",
    "        prev_state = self.state\n",
    "        self.state = np.zeros([4, 4])\n",
    "        self.state[self.goal_pos['y'], self.goal_pos['x']] = -1\n",
    "        self.state[self.agent_pos['y'], self.agent_pos['x']] = 1\n",
    "        \n",
    "        done = False\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            done = True\n",
    "        \n",
    "        reward = self.reward(prev_state, action, self.state)\n",
    "        \n",
    "        return reward, self.state, done\n",
    "    \n",
    "    def reward(self, s, a, s_next):\n",
    "        reward = 0\n",
    "        y, x = np.where(s == 1)\n",
    "        y_next, x_next = np.where(s_next == 1)\n",
    "        if (\n",
    "            (y_next == self.goal_pos['y'] and x_next == self.goal_pos['x']) \n",
    "            and (y != self.goal_pos['y'] or x != self.goal_pos['x'])\n",
    "        ):      # reached the goal\n",
    "            reward = 10\n",
    "            \n",
    "        return reward\n",
    "    \n",
    "    def transition_probability(self, s, a, s_next):\n",
    "        y, x = np.where(s == 1)     # get agent pos from s\n",
    "        y_next, x_next = np.where(s_next == 1)  # get agent pos from s_next\n",
    "        \n",
    "        # already reached goal\n",
    "        if y == self.goal_pos['y'] and x == self.goal_pos['x']:\n",
    "            y_next_temp, x_next_temp = self.goal_pos['y'], self.goal_pos['x']\n",
    "        \n",
    "        # upward movement\n",
    "        elif a == 0:\n",
    "            y_next_temp, x_next_temp = max(y - 1, self.y_min), x\n",
    "            \n",
    "        # right movement\n",
    "        elif a == 1:\n",
    "            y_next_temp, x_next_temp = y, min(x + 1, self.x_max)\n",
    "        \n",
    "        # downward movement\n",
    "        elif a == 2:\n",
    "            y_next_temp, x_next_temp = min(y + 1, self.y_max), x\n",
    "        \n",
    "        # left movement\n",
    "        elif a == 3:\n",
    "            y_next_temp, x_next_temp = y, max(x - 1, self.x_min)\n",
    "        \n",
    "        else:\n",
    "            assert False, \"Invalid action value wasfed to step.\"\n",
    "            \n",
    "            \n",
    "        is_correct_transition = (\n",
    "            y_next_temp == y_next and \n",
    "            x_next_temp == x_next\n",
    "        )\n",
    "\n",
    "        if is_correct_transition:\n",
    "            return 1.0\n",
    "        else:\n",
    "            return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment verification is done\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import random\n",
    "    \n",
    "    env = Env()\n",
    "    s = env.reset()\n",
    "    transition_list = list()\n",
    "    \n",
    "    for i in range(10000):\n",
    "        a = np.random.randint(len(env.action_space))\n",
    "        r, s_next, done = env.step(a)\n",
    "        transition = (s, a, s_next)\n",
    "        transition_list.append(transition)\n",
    "        \n",
    "        s = s_next\n",
    "        if done:\n",
    "            s = env.reset()\n",
    "            \n",
    "    for transition in transition_list:\n",
    "        s, a, s_next = transition\n",
    "        \n",
    "        # check if possible to transit possible next state\n",
    "        check1 = env.transition_probability(s, a, s_next) == 1.0\n",
    "        \n",
    "        s_next_ = s_next\n",
    "        while (s_next_ == s_next).all():\n",
    "            s_next_ = random.sample(env.state_space, 1)[0]\n",
    "        \n",
    "        # check if not possible to transit impossible next state\n",
    "        check2 = env.transition_probability(s, a, s_next_) == 0.0\n",
    "        \n",
    "        check = ((check1 and check2) == True)\n",
    "        \n",
    "        if not check:\n",
    "            print(\"Something is wrong!!\")\n",
    "            break\n",
    "        \n",
    "    print(\"Environment verification is done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
