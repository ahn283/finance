{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- !pip install -q numpy\n",
    "!pip install -q matplotlib\n",
    "!pip install -q mujoco\n",
    "!command -v ffmpeg >/dev/null || (apt update && apt install -y ffmpeg)\n",
    "!pip install -q mediapy\n",
    "\n",
    "%env MUJOCO_GL=egl -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import mediapy as media\n",
    "import matplotlib.pyplot as plt\n",
    "import mujoco\n",
    "\n",
    "import scipy.signal\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.optim import Adam\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define MuJoCo Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_string=\"\"\"<!-- Cheetah Model\n",
    "\n",
    "    The state space is populated with joints in the order that they are\n",
    "    defined in this file. The actuators also operate on joints.\n",
    "\n",
    "    State-Space (name/joint/parameter):\n",
    "        - rootx     slider      position (m)\n",
    "        - rootz     slider      position (m)\n",
    "        - rooty     hinge       angle (rad)\n",
    "        - bthigh    hinge       angle (rad)\n",
    "        - bshin     hinge       angle (rad)\n",
    "        - bfoot     hinge       angle (rad)\n",
    "        - fthigh    hinge       angle (rad)\n",
    "        - fshin     hinge       angle (rad)\n",
    "        - ffoot     hinge       angle (rad)\n",
    "        - rootx     slider      velocity (m/s)\n",
    "        - rootz     slider      velocity (m/s)\n",
    "        - rooty     hinge       angular velocity (rad/s)\n",
    "        - bthigh    hinge       angular velocity (rad/s)\n",
    "        - bshin     hinge       angular velocity (rad/s)\n",
    "        - bfoot     hinge       angular velocity (rad/s)\n",
    "        - fthigh    hinge       angular velocity (rad/s)\n",
    "        - fshin     hinge       angular velocity (rad/s)\n",
    "        - ffoot     hinge       angular velocity (rad/s)\n",
    "\n",
    "    Actuators (name/actuator/parameter):\n",
    "        - bthigh    hinge       torque (N m)\n",
    "        - bshin     hinge       torque (N m)\n",
    "        - bfoot     hinge       torque (N m)\n",
    "        - fthigh    hinge       torque (N m)\n",
    "        - fshin     hinge       torque (N m)\n",
    "        - ffoot     hinge       torque (N m)\n",
    "\n",
    "-->\n",
    "<mujoco model=\"cheetah\">\n",
    "  <compiler angle=\"radian\" coordinate=\"local\" inertiafromgeom=\"true\" settotalmass=\"14\"/>\n",
    "  <default>\n",
    "    <joint armature=\".1\" damping=\".01\" limited=\"true\" solimplimit=\"0 .8 .03\" solreflimit=\".02 1\" stiffness=\"8\"/>\n",
    "    <geom conaffinity=\"0\" condim=\"3\" contype=\"1\" friction=\".4 .1 .1\" rgba=\"0.8 0.6 .4 1\" solimp=\"0.0 0.8 0.01\" solref=\"0.02 1\"/>\n",
    "    <motor ctrllimited=\"true\" ctrlrange=\"-1 1\"/>\n",
    "  </default>\n",
    "  <size nstack=\"300000\" nuser_geom=\"1\"/>\n",
    "  <option gravity=\"0 0 -9.81\" timestep=\"0.01\"/>\n",
    "  <asset>\n",
    "    <texture builtin=\"gradient\" height=\"100\" rgb1=\"1 1 1\" rgb2=\"0 0 0\" type=\"skybox\" width=\"100\"/>\n",
    "    <texture builtin=\"flat\" height=\"1278\" mark=\"cross\" markrgb=\"1 1 1\" name=\"texgeom\" random=\"0.01\" rgb1=\"0.8 0.6 0.4\" rgb2=\"0.8 0.6 0.4\" type=\"cube\" width=\"127\"/>\n",
    "    <texture builtin=\"checker\" height=\"100\" name=\"texplane\" rgb1=\"0 0 0\" rgb2=\"0.8 0.8 0.8\" type=\"2d\" width=\"100\"/>\n",
    "    <material name=\"MatPlane\" reflectance=\"0.5\" shininess=\"1\" specular=\"1\" texrepeat=\"60 60\" texture=\"texplane\"/>\n",
    "    <material name=\"geom\" texture=\"texgeom\" texuniform=\"true\"/>\n",
    "  </asset>\n",
    "  <worldbody>\n",
    "    <light cutoff=\"100\" diffuse=\"1 1 1\" dir=\"-0 0 -1.3\" directional=\"true\" exponent=\"1\" pos=\"0 0 1.3\" specular=\".1 .1 .1\"/>\n",
    "    <geom conaffinity=\"1\" condim=\"3\" material=\"MatPlane\" name=\"floor\" pos=\"0 0 0\" rgba=\"0.8 0.9 0.8 1\" size=\"40 40 40\" type=\"plane\"/>\n",
    "    <body name=\"torso\" pos=\"0 0 .7\">\n",
    "      <camera name=\"track\" mode=\"trackcom\" pos=\"0 -3 0.3\" xyaxes=\"1 0 0 0 0 1\"/>\n",
    "      <joint armature=\"0\" axis=\"1 0 0\" damping=\"0\" limited=\"false\" name=\"rootx\" pos=\"0 0 0\" stiffness=\"0\" type=\"slide\"/>\n",
    "      <joint armature=\"0\" axis=\"0 0 1\" damping=\"0\" limited=\"false\" name=\"rootz\" pos=\"0 0 0\" stiffness=\"0\" type=\"slide\"/>\n",
    "      <joint armature=\"0\" axis=\"0 1 0\" damping=\"0\" limited=\"false\" name=\"rooty\" pos=\"0 0 0\" stiffness=\"0\" type=\"hinge\"/>\n",
    "      <geom fromto=\"-.5 0 0 .5 0 0\" name=\"torso\" size=\"0.046\" type=\"capsule\"/>\n",
    "      <geom axisangle=\"0 1 0 .87\" name=\"head\" pos=\".6 0 .1\" size=\"0.046 .15\" type=\"capsule\"/>\n",
    "      <!-- <site name='tip'  pos='.15 0 .11'/>-->\n",
    "      <body name=\"bthigh\" pos=\"-.5 0 0\">\n",
    "        <joint axis=\"0 1 0\" damping=\"6\" name=\"bthigh\" pos=\"0 0 0\" range=\"-.52 1.05\" stiffness=\"240\" type=\"hinge\"/>\n",
    "        <geom axisangle=\"0 1 0 -3.8\" name=\"bthigh\" pos=\".1 0 -.13\" size=\"0.046 .145\" type=\"capsule\"/>\n",
    "        <body name=\"bshin\" pos=\".16 0 -.25\">\n",
    "          <joint axis=\"0 1 0\" damping=\"4.5\" name=\"bshin\" pos=\"0 0 0\" range=\"-.785 .785\" stiffness=\"180\" type=\"hinge\"/>\n",
    "          <geom axisangle=\"0 1 0 -2.03\" name=\"bshin\" pos=\"-.14 0 -.07\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .15\" type=\"capsule\"/>\n",
    "          <body name=\"bfoot\" pos=\"-.28 0 -.14\">\n",
    "            <joint axis=\"0 1 0\" damping=\"3\" name=\"bfoot\" pos=\"0 0 0\" range=\"-.4 .785\" stiffness=\"120\" type=\"hinge\"/>\n",
    "            <geom axisangle=\"0 1 0 -.27\" name=\"bfoot\" pos=\".03 0 -.097\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .094\" type=\"capsule\"/>\n",
    "          </body>\n",
    "        </body>\n",
    "      </body>\n",
    "      <body name=\"fthigh\" pos=\".5 0 0\">\n",
    "        <joint axis=\"0 1 0\" damping=\"4.5\" name=\"fthigh\" pos=\"0 0 0\" range=\"-1 .7\" stiffness=\"180\" type=\"hinge\"/>\n",
    "        <geom axisangle=\"0 1 0 .52\" name=\"fthigh\" pos=\"-.07 0 -.12\" size=\"0.046 .133\" type=\"capsule\"/>\n",
    "        <body name=\"fshin\" pos=\"-.14 0 -.24\">\n",
    "          <joint axis=\"0 1 0\" damping=\"3\" name=\"fshin\" pos=\"0 0 0\" range=\"-1.2 .87\" stiffness=\"120\" type=\"hinge\"/>\n",
    "          <geom axisangle=\"0 1 0 -.6\" name=\"fshin\" pos=\".065 0 -.09\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .106\" type=\"capsule\"/>\n",
    "          <body name=\"ffoot\" pos=\".13 0 -.18\">\n",
    "            <joint axis=\"0 1 0\" damping=\"1.5\" name=\"ffoot\" pos=\"0 0 0\" range=\"-.5 .5\" stiffness=\"60\" type=\"hinge\"/>\n",
    "            <geom axisangle=\"0 1 0 -.6\" name=\"ffoot\" pos=\".045 0 -.07\" rgba=\"0.9 0.6 0.6 1\" size=\"0.046 .07\" type=\"capsule\"/>\n",
    "          </body>\n",
    "        </body>\n",
    "      </body>\n",
    "    </body>\n",
    "  </worldbody>\n",
    "  <actuator>\n",
    "    <motor gear=\"120\" joint=\"bthigh\" name=\"bthigh\"/>\n",
    "    <motor gear=\"90\" joint=\"bshin\" name=\"bshin\"/>\n",
    "    <motor gear=\"60\" joint=\"bfoot\" name=\"bfoot\"/>\n",
    "    <motor gear=\"120\" joint=\"fthigh\" name=\"fthigh\"/>\n",
    "    <motor gear=\"60\" joint=\"fshin\" name=\"fshin\"/>\n",
    "    <motor gear=\"30\" joint=\"ffoot\" name=\"ffoot\"/>\n",
    "  </actuator>\n",
    "</mujoco>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HalfCheetahEnv():\n",
    "    def __init__(\n",
    "        self,\n",
    "        frame_skip=5,\n",
    "        forward_reward_weight=1.0,\n",
    "        ctrl_cost_weight=0.1,\n",
    "        reset_noise_scale=0.1\n",
    "    ):\n",
    "        self.frame_skip = frame_skip\n",
    "        self.forward_reward_weight = forward_reward_weight\n",
    "        self.ctrl_cost_weight = ctrl_cost_weight\n",
    "        self.reset_noise_scale = reset_noise_scale\n",
    "        \n",
    "        self.initialize_simulation()\n",
    "        self.init_qpos = self.data.qpos.ravel().copy()\n",
    "        self.init_qvel = self.data.qvel.ravel().copy()\n",
    "        self.dt = self.model.opt.timestep * self.frame_skip\n",
    "        \n",
    "        self.observation_dim = 17\n",
    "        self.action_dim = 6\n",
    "        self.action_limit = 1.\n",
    "        \n",
    "    def initialize_simulation(self):\n",
    "        self.model = mujoco.MjModel.from_xml_string(xml_string)\n",
    "        self.data = mujoco.MjData(self.model)\n",
    "        mujoco.mj_resetData(self.model, self.data)\n",
    "        self.renderer = mujoco.Renderer(self.model)\n",
    "        \n",
    "    def reset_simulation(self):\n",
    "        mujoco.mj_resetData(self.model, self.data)\n",
    "        \n",
    "    def step_mujoco_simulation(self, ctrl, n_frames):\n",
    "        self.data.ctrl[:] = ctrl\n",
    "        mujoco.mj_step(self.model, self.data, nstep=n_frames)\n",
    "        self.renderer.update_scene(self.data, 0)\n",
    "        \n",
    "    def set_state(self, qpos, qvel):\n",
    "        self.data.qpos[:] = np.copy(qpos)\n",
    "        self.data.qvel[:] = np.copy(qvel)\n",
    "        if self.model.na == 0:\n",
    "            self.data.act[:] = None\n",
    "        mujoco.mj_forward(self.model, self.data)\n",
    "        \n",
    "    def sample_action(self):\n",
    "        return (2. * np.random.uniform(size=(self.action_dim, )) - 1) * self.action_limit\n",
    "    \n",
    "    def step(self, action):\n",
    "        x_position_before = self.data.qpos[0]\n",
    "        self.step_mujoco_simulation(action, self.frame_skip)\n",
    "        x_position_after = self.data.qpos[0]\n",
    "        x_velocity = (x_position_after - x_position_before) / self.dt\n",
    "        \n",
    "        # Rewards\n",
    "        ctrl_cost = self.ctrl_cost_weight * np.sum(np.square(action))\n",
    "        forward_reward = self.forward_reward_weight * x_velocity\n",
    "        observation = self.get_obs()\n",
    "        reward = forward_reward - ctrl_cost\n",
    "        terminated = False\n",
    "        info = {\n",
    "            \"x_position\": x_position_after,\n",
    "            \"x_velocity\": x_velocity,\n",
    "            \"reward_run\": forward_reward,\n",
    "            \"reward_ctrl\": -ctrl_cost,\n",
    "        }\n",
    "        \n",
    "        return observation, reward, terminated, info\n",
    "    \n",
    "    def get_obs(self):\n",
    "        position = self.data.qpos.flat.copy()\n",
    "        velocity = self.data.qvel.flat.copy()\n",
    "        position = position[1:]\n",
    "        \n",
    "        observation = np.concatenate((position, velocity)).ravel()\n",
    "        return observation\n",
    "    \n",
    "    def render(self):\n",
    "        return self.renderer.render()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.reset_simulation()\n",
    "        noise_low = -self.reset_noise_scale\n",
    "        noise_high = self.reset_noise_scale\n",
    "        qpos = self.init_qpos + np.random.uniform(\n",
    "            low=noise_low, high=noise_high, size=self.model.nq\n",
    "        )\n",
    "        qvel = (\n",
    "            self.init_qvel\n",
    "            + self.reset_noise_scale * np.random.standard_normal(self.model.nv)\n",
    "        )\n",
    "        self.set_state(qpos, qvel)\n",
    "        observation = self.get_obs()\n",
    "        return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to : NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "else:\n",
    "    print(\"Device set to : cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proximal policy optimization\n",
    "\n",
    "### Buffer : Buffer 클래스는 에피소드를 저장하고 가공하는 역할\n",
    "\n",
    "- 저장해야 할 것들\n",
    "\n",
    "    - state $\\mathcal{S}_t$\n",
    "\n",
    "    - action, $A_t$\n",
    "\n",
    "    - reward, $R_{t+1}$\n",
    "\n",
    "    - value, $V_\\phi(S_t)$\n",
    "\n",
    "    - log policy distribution (old), $\\log(\\pi_\\theta(A_t|S_t))$\n",
    "\n",
    "- Buffer 클래스에서 계산해줘야 하는 것들\n",
    "\n",
    "    - advantage\n",
    "\n",
    "        $\\delta_t=R_{t+1}+\\gamma V_\\phi(S_{t+1})-V_\\phi(S_t)$\n",
    "\n",
    "        $A_t^{(\\lambda)}=\\delta_t+\\gamma\\lambda A_{t+1}^{(\\lambda)}$\n",
    "\n",
    "    - return:\n",
    "\n",
    "        $G_t=R_{t+1} + \\gamma G_{t+1}$\n",
    "\n",
    "- PPO는 on-policy 알고리즘으로 policy의 업데이트 이후 buffer 초기화 작업 필요\n",
    "\n",
    "    - buffer의 크기 = transition 수집 횟수\n",
    "\n",
    "    - 정해진 횟수만큼 transition을 모두 수집하면 이전에 수집된 데이터가 모두 업데이트 됨. 별도의 refresh 작업이 필요하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions\n",
    "def combined_shape(length, shape=None):\n",
    "    if shape is None:\n",
    "        return (length, )\n",
    "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
    "\n",
    "def count_vars(module):\n",
    "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
    "\n",
    "def discount_cumsum(x, discount):\n",
    "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOBuffer:\n",
    "    \n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.idx, self.path_start_idx, self.max_size = 0, 0, size\n",
    "        \n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \n",
    "        self.obs_buf[self.idx] = obs\n",
    "        self.act_buf[self.idx] = act\n",
    "        self.rew_buf[self.idx] = rew\n",
    "        self.val_buf[self.idx] = val\n",
    "        self.logp_buf[self.idx] = logp\n",
    "        self.idx += 1\n",
    "        \n",
    "    def finish_path(self, last_val=0):\n",
    "        \n",
    "        path_slice = slice(self.path_start_idx, self.idx)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.idx\n",
    "        \n",
    "    def get(self):\n",
    "        self.idx, self.path_start_idx = 0, 0\n",
    "        # the next two lines implment the advantage normalization trick\n",
    "        adv_mean, adv_std = np.mean(self.adv_buf), np.std(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        data = dict(obs=self.obs_buf, act=self.act_buf, ret=self.ret_buf, \n",
    "                    adv=self.adv_buf, logp=self.logp_buf)\n",
    "        return {k: torch.as_tensor(v, dtype=torch.float32) for k, v in data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network and Value Network\n",
    "\n",
    "- Policy network : observation이 입력, $\\mu_t$를 출력으로 하는 네트워크 설계\n",
    "\n",
    "    - $\\mu_t=\\pi_\\theta(s)$\n",
    "\n",
    "    - $\\sigma_t=\\exp(\\text{log\\_std}_t)$\n",
    "\n",
    "- Value network : observation이 입력, 1차원의 value를 출력하는 네트워크 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch 네트워크 클래스 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp(sizes, activation, output_activation=nn.Identity):\n",
    "    layers = []\n",
    "    for j in range(len(sizes) - 1):\n",
    "        act = activation if j < len(sizes) - 2 else output_activation\n",
    "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class MLPGaussianActor(nn.Module):\n",
    "    \n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        log_std = -0.5 * np.ones(act_dim, dtype=np.float32)\n",
    "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
    "        self.mu_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
    "        \n",
    "    def _distribution(self, obs):\n",
    "        mu = self.mu_net(obs)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return Normal(mu, std)\n",
    "    \n",
    "    def _log_prob_from_distribution(self, pi, act):\n",
    "        return pi.log_prob(act).sum(axis=-1)    # Last axis sum needed for Torch Normal distribution\n",
    "    \n",
    "    def _get_mode(self, obs):\n",
    "        return self.mu_net(obs)\n",
    "    \n",
    "    def forward(self, obs, act=None):\n",
    "        pi = self._distribution(obs)\n",
    "        logp_a = None\n",
    "        if act is not None:\n",
    "            logp_a = self._log_prob_from_distribution(pi, act)\n",
    "        return pi, logp_a\n",
    "\n",
    "class MLPCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, obs_dim, hidden_sizes, activation):\n",
    "        super().__init__()\n",
    "        self.v_net = mlp([obs_dim] + list(hidden_sizes) + [1], activation)\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        return torch.squeeze(self.v_net(obs), -1)   # critic to ensure v has right shape.\n",
    "    \n",
    "class MLPActorCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, obs_dim, act_dim, \n",
    "                 hidden_sizes=(64, 64), activation=nn.Tanh):\n",
    "        super().__init__()\n",
    "        \n",
    "        # policy builder depends on action space\n",
    "        self.pi = MLPGaussianActor(obs_dim, act_dim, hidden_sizes, activation)\n",
    "        \n",
    "        # build value function\n",
    "        self.v = MLPCritic(obs_dim, hidden_sizes, activation)\n",
    "        \n",
    "    def step(self, obs): \n",
    "        # for getting experience buffer   \n",
    "        with torch.no_grad():\n",
    "            pi = self.pi._distribution(obs)\n",
    "            a = pi.sample()\n",
    "            logp_a = self.pi._log_prob_from_distribution(pi, a)\n",
    "            v = self.v(obs)\n",
    "            \n",
    "        return a.numpy(), v.numpy(), logp_a.numpy()\n",
    "    \n",
    "    def act(self, obs):     \n",
    "        # for evaluation\n",
    "        return self.pi._get_mode(obs).numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment 생성, Buffer 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_per_epoch = 4000\n",
    "gamma = 0.99\n",
    "lam = 0.97\n",
    "\n",
    "env = HalfCheetahEnv()\n",
    "obs_dim = env.observation_dim\n",
    "act_dim = env.action_dim\n",
    "\n",
    "# set up experience buffer\n",
    "buf = PPOBuffer(obs_dim, act_dim, step_per_epoch, gamma, lam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actor critic 네트워크 및 optimizer 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of parameters: \t pi: 5708, \t v: 5377\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_sizes = [64, 64]\n",
    "pi_lr = 3e-4\n",
    "vf_lr = 1e-3\n",
    "\n",
    "# create actor-critic module\n",
    "ac = MLPActorCritic(env.observation_dim, env.action_dim, hidden_sizes)\n",
    "\n",
    "# count variables\n",
    "var_counts = tuple(count_vars(module) for module in [ac.pi, ac.v])\n",
    "print(\"\\nNumber of parameters: \\t pi: %d, \\t v: %d\\n\" %var_counts)\n",
    "\n",
    "pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "vf_optimizer = Adam(ac.v.parameters(), lr=vf_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 에피소드 수집\n",
    "\n",
    "- Environment 초기화, 초기 상태 $s_0$\n",
    "\n",
    "- Policy로부터 다음 정보 획득 : $a_t, v(s_t), \\log(\\pi(a_t|s_t))$\n",
    "\n",
    "    - $a_t=\\mu_t+\\sigma_t\\epsilon_t$\n",
    "\n",
    "    - $v(s_t)$: advantage를 계산하기 위해 필요\n",
    "\n",
    "    - $\\log(\\pi(a_t|s_t))$ : ratio를 계산하기 위해 필요\n",
    "\n",
    "- $s_{t+1}, r_{t+1}, d_t$ 획득\n",
    "\n",
    "- Buffer에 저장\n",
    "\n",
    "- 에피소드 종료 체크, 환경 초기화, $s_0$ 획득"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ep_len = 1000\n",
    "\n",
    "# prepare for ineteraction with environment\n",
    "start_time = time.time()\n",
    "o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "# main loop: collect experience in env and update/log each epoch\n",
    "for t in range(step_per_epoch):\n",
    "    a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "    \n",
    "    next_o, r, d, _ = env.step(a)\n",
    "    ep_ret += r\n",
    "    ep_len += 1\n",
    "    \n",
    "    # save and log\n",
    "    buf.store(o, a, r, v, logp)\n",
    "    \n",
    "    # update obs (ciritical)\n",
    "    o = next_o\n",
    "    \n",
    "    timeout = ep_len == max_ep_len\n",
    "    terminal = d or timeout\n",
    "    epoch_ended = t == step_per_epoch - 1\n",
    "    \n",
    "    if terminal or epoch_ended:\n",
    "        if timeout or epoch_ended:\n",
    "            _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "        else:\n",
    "            v = 0\n",
    "        buf.finish_path(v)\n",
    "        o, ep_ret, ep_len = env.reset(), 0, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buffer에서 data를 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = buf.get()\n",
    "obs, act, adv, logp_old, ret = data['obs'], data['act'], data['adv'], data['logp'], data['ret']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Policy 네트워크 업데이트\n",
    "\n",
    "- 이 때 업데이트된 kl divergence가 특정 threshold 이상이면 업데이트 중지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_kl = 0.02\n",
    "clip_ratio = 0.2\n",
    "train_pi_iters = 80\n",
    "\n",
    "# train policy with multiple steps of gradient descent\n",
    "for i in range(train_pi_iters):\n",
    "    \n",
    "    # policy loss\n",
    "    pi, logp = ac.pi(obs, act)\n",
    "    ratio = torch.exp(logp - logp_old)\n",
    "    clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
    "    loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "    \n",
    "    pi_optimizer.zero_grad()\n",
    "    approx_kl = (logp_old - logp).mean().item()\n",
    "    kl = np.mean(approx_kl)\n",
    "    if kl > 1.5 * target_kl:\n",
    "        print('Early stopping at step %d due to reaching max kl.' %i)\n",
    "        break\n",
    "    loss_pi.backward()\n",
    "    pi_optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value network 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_v_iters = 80\n",
    "\n",
    "# value function learning\n",
    "for i in range(train_v_iters):\n",
    "    vf_optimizer.zero_grad()\n",
    "    loss_v = ((ac.v(obs) - ret)**2).mean()\n",
    "    loss_v.backward()\n",
    "    vf_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO\n",
    "\n",
    "- 앞 서 만든 클래스들과 함수들을 모두 합쳐 PPO 알고리즘 구현\n",
    "\n",
    "- Policy loss : clipped surrogated loss\n",
    "\n",
    "    - $L_{\\text{clip}}(\\theta):=\\sum_{t=1}^B\\min(r_t(\\theta)A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)$\n",
    "\n",
    "    - $r_t(\\theta)=\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_k}(a_t|s_t)}$ : $\\pi_{\\theta_k}(a_t|s_t)$를 미리 buffer에 저장해두기\n",
    "\n",
    "- Value loss\n",
    "\n",
    "    - $L_v(\\phi):=\\sum_{t=1}^B(G_t-V_\\phi(s_t))^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo(env_fn, actor_critic=MLPActorCritic, ac_kwargs=dict(), seed=0,\n",
    "        steps_per_epoch=4000, epochs=50, gamma=0.99, clip_ratio=0.2, pi_lr=3e-4,\n",
    "        vf_lr=1e-3, train_pi_iters=80, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
    "        target_kl=0.01, save_freq=10):\n",
    "    \n",
    "    # random seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    EpRet = []\n",
    "    EpLen = []\n",
    "    VVals = []\n",
    "    TotalEnvInteracts = []\n",
    "    LossPi = []\n",
    "    LossV = []\n",
    "    DeltaLossPi = []\n",
    "    DeltaLossV = []\n",
    "    Entropy = []\n",
    "    KL = []\n",
    "    ClipFrac = []\n",
    "    StopIter = []\n",
    "    Time = []\n",
    "    \n",
    "    # instantiate environment\n",
    "    env = env_fn()\n",
    "    obs_dim = env.observation_dim\n",
    "    act_dim = env.action_dim\n",
    "    \n",
    "    # create actor-critic module\n",
    "    ac = actor_critic(env.observation_dim, env.action_dim, **ac_kwargs)\n",
    "    \n",
    "    # count variables\n",
    "    var_counts = tuple(count_vars(module) for module in [ac.pi, ac.v])\n",
    "    print(\"\\nNumver of parameters: \\t pi: %d \\t v: %d\\n\" %var_counts)\n",
    "    \n",
    "    # set up experience buffer\n",
    "    buf = PPOBuffer(obs_dim, act_dim, steps_per_epoch, gamma, lam)\n",
    "    \n",
    "    # set up function for computing PPO policy loss\n",
    "    def compute_loss_pi(data):\n",
    "        obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
    "        \n",
    "        # policy loss\n",
    "        pi, logp = ac.pi(obs, act)\n",
    "        ratio = torch.exp(logp - logp_old)\n",
    "        clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
    "        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "        \n",
    "        # useful extra info\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        ent = pi.entropy().mean().item()\n",
    "        clipped = ratio.gt(1+clip_ratio) | ratio.lt(1-clip_ratio)\n",
    "        clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()\n",
    "        pi_info = dict(kl=approx_kl, ent=ent, cf=clipfrac)\n",
    "        \n",
    "        return loss_pi, pi_info\n",
    "    \n",
    "    # set up function for computing value loss\n",
    "    def compute_loss_v(data):\n",
    "        obs, ret = data['obs'], data['ret']\n",
    "        return ((ac.v(obs)- ret) ** 2).mean()\n",
    "    \n",
    "    # set up optgimizers for policy and value function\n",
    "    pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    vf_optimizer = Adam(ac.v.parameters(), lr=vf_lr)\n",
    "    \n",
    "    def update():\n",
    "        data = buf.get()\n",
    "        \n",
    "        pi_l_old, pi_info_old = compute_loss_pi(data)\n",
    "        pi_l_old = pi_l_old.item()\n",
    "        v_l_old = compute_loss_v(data).item()\n",
    "        \n",
    "        # train policy with multiple steps of gradient descent\n",
    "        for i in range(train_pi_iters):\n",
    "            pi_optimizer.zero_grad()\n",
    "            loss_pi, pi_info = compute_loss_pi(data)\n",
    "            kl = np.mean(pi_info['kl'])\n",
    "            if kl > 1.5 * target_kl:\n",
    "                print('Early stopping at step %d due to reaching max kl.'%i)\n",
    "                break\n",
    "            loss_pi.backward()\n",
    "            pi_optimizer.step()\n",
    "            \n",
    "        # value function learning\n",
    "        for i in range(train_v_iters):\n",
    "            vf_optimizer.zero_grad()\n",
    "            loss_v = compute_loss_v(data)\n",
    "            loss_v.backward()\n",
    "            vf_optimizer.step()\n",
    "            \n",
    "        # log changes from update\n",
    "        LossPi.append(pi_l_old)\n",
    "        LossV.append(v_l_old)\n",
    "        KL.append(pi_info['kl'])\n",
    "        Entropy.append(pi_info_old['ent'])\n",
    "        ClipFrac.append(pi_info['cf'])\n",
    "        DeltaLossPi.append(loss_pi.item() - pi_l_old)\n",
    "        DeltaLossV.append(loss_v.item() - v_l_old)\n",
    "        \n",
    "    # prepare for interaction with environment\n",
    "    start_time = time.time()\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "    \n",
    "    # main loop: collect experience in env and update/log each epoch\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(step_per_epoch):\n",
    "            a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "            \n",
    "            next_o, r, d, _ = env.step(a)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "            \n",
    "            # save and log\n",
    "            buf.store(o, a, r, v, logp)\n",
    "            VVals.append(v)\n",
    "            \n",
    "            # update obs (critical)\n",
    "            o = next_o\n",
    "            \n",
    "            timeout = (ep_len == max_ep_len)\n",
    "            terminal = d or timeout\n",
    "            epoch_ended = (t == steps_per_epoch - 1)\n",
    "            \n",
    "            if terminal or epoch_ended:\n",
    "                if epoch_ended and not (terminal):\n",
    "                    print('Warning: trajectory cut off by epoch at %d steps.' %ep_len, flush=True)\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                if timeout or epoch_ended:\n",
    "                    _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "                else:\n",
    "                    v = 0\n",
    "                buf.finish_path(v)\n",
    "                if terminal:\n",
    "                    EpRet.append(ep_ret)\n",
    "                    EpLen.append(ep_len)\n",
    "                o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "                    \n",
    "        # perform PPO update\n",
    "        update()\n",
    "        \n",
    "        TotalEnvInteracts.append((epoch+1)*step_per_epoch)\n",
    "        Time.append(time.time() - start_time)\n",
    "        \n",
    "        print(f'[Epoch:{epoch}] EpRet:{np.min(EpRet[-10:]):8.2f} < {np.mean(EpRet[-10:]):8.2f} < {np.max(EpRet[-10:]):8.2f}, EpLen:{np.mean(EpLen[-10:]):8.2f}, VVals:{np.mean(VVals[-10:]):8.2f}, TotalEnvInteracts:{TotalEnvInteracts[-1]:8d}, LossPi:{np.mean(LossPi[-10:]):8.2f}, LossV:{np.mean(LossV[-10:]):8.2f}, Entropy:{np.mean(Entropy[-10:]):8.2f}, KL:{np.mean(KL[-10:]):8.2f}, Time:{Time[-1]:8.2f}')\n",
    "    \n",
    "    return ac, EpRet, EpLen, VVals, TotalEnvInteracts, LossPi, LossV, Entropy, KL, Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo(env_fn, actor_critic=MLPActorCritic, ac_kwargs=dict(), seed=0,\n",
    "        steps_per_epoch=4000, epochs=50, gamma=0.99, clip_ratio=0.2, pi_lr=3e-4,\n",
    "        vf_lr=1e-3, train_pi_iters=80, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
    "        target_kl=0.01, save_freq=10):\n",
    "\n",
    "    # Random seed\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    EpRet = []\n",
    "    EpLen = []\n",
    "    VVals = []\n",
    "    TotalEnvInteracts = []\n",
    "    LossPi = []\n",
    "    LossV = []\n",
    "    DeltaLossPi = []\n",
    "    DeltaLossV = []\n",
    "    Entropy = []\n",
    "    KL = []\n",
    "    ClipFrac = []\n",
    "    StopIter = []\n",
    "    Time = []\n",
    "\n",
    "    # Instantiate environment\n",
    "    env = env_fn()\n",
    "    obs_dim = env.observation_dim\n",
    "    act_dim = env.action_dim\n",
    "\n",
    "    # Create actor-critic module\n",
    "    ac = actor_critic(env.observation_dim, env.action_dim, **ac_kwargs)\n",
    "\n",
    "    # Count variables\n",
    "    var_counts = tuple(count_vars(module) for module in [ac.pi, ac.v])\n",
    "    print('\\nNumber of parameters: \\t pi: %d, \\t v: %d\\n'%var_counts)\n",
    "\n",
    "    # Set up experience buffer\n",
    "    buf = PPOBuffer(obs_dim, act_dim, steps_per_epoch, gamma, lam)\n",
    "\n",
    "    # Set up function for computing PPO policy loss\n",
    "    def compute_loss_pi(data):\n",
    "        obs, act, adv, logp_old = data['obs'], data['act'], data['adv'], data['logp']\n",
    "\n",
    "        # Policy loss\n",
    "        pi, logp = ac.pi(obs, act)\n",
    "        ratio = torch.exp(logp - logp_old)\n",
    "        clip_adv = torch.clamp(ratio, 1-clip_ratio, 1+clip_ratio) * adv\n",
    "        loss_pi = -(torch.min(ratio * adv, clip_adv)).mean()\n",
    "\n",
    "        # Useful extra info\n",
    "        approx_kl = (logp_old - logp).mean().item()\n",
    "        ent = pi.entropy().mean().item()\n",
    "        clipped = ratio.gt(1+clip_ratio) | ratio.lt(1-clip_ratio)\n",
    "        clipfrac = torch.as_tensor(clipped, dtype=torch.float32).mean().item()\n",
    "        pi_info = dict(kl=approx_kl, ent=ent, cf=clipfrac)\n",
    "\n",
    "        return loss_pi, pi_info\n",
    "\n",
    "    # Set up function for computing value loss\n",
    "    def compute_loss_v(data):\n",
    "        obs, ret = data['obs'], data['ret']\n",
    "        return ((ac.v(obs) - ret)**2).mean()\n",
    "\n",
    "    # Set up optimizers for policy and value function\n",
    "    pi_optimizer = Adam(ac.pi.parameters(), lr=pi_lr)\n",
    "    vf_optimizer = Adam(ac.v.parameters(), lr=vf_lr)\n",
    "\n",
    "    def update():\n",
    "        data = buf.get()\n",
    "\n",
    "        pi_l_old, pi_info_old = compute_loss_pi(data)\n",
    "        pi_l_old = pi_l_old.item()\n",
    "        v_l_old = compute_loss_v(data).item()\n",
    "\n",
    "        # Train policy with multiple steps of gradient descent\n",
    "        for i in range(train_pi_iters):\n",
    "            pi_optimizer.zero_grad()\n",
    "            loss_pi, pi_info = compute_loss_pi(data)\n",
    "            kl = np.mean(pi_info['kl'])\n",
    "            if kl > 1.5 * target_kl:\n",
    "                print('Early stopping at step %d due to reaching max kl.'%i)\n",
    "                break\n",
    "            loss_pi.backward()\n",
    "            pi_optimizer.step()\n",
    "\n",
    "        # Value function learning\n",
    "        for i in range(train_v_iters):\n",
    "            vf_optimizer.zero_grad()\n",
    "            loss_v = compute_loss_v(data)\n",
    "            loss_v.backward()\n",
    "            vf_optimizer.step()\n",
    "\n",
    "        # Log changes from update\n",
    "        LossPi.append(pi_l_old)\n",
    "        LossV.append(v_l_old)\n",
    "        KL.append(pi_info['kl'])\n",
    "        Entropy.append(pi_info_old['ent'])\n",
    "        ClipFrac.append(pi_info['cf'])\n",
    "        DeltaLossPi.append(loss_pi.item() - pi_l_old)\n",
    "        DeltaLossV.append(loss_v.item() - v_l_old)\n",
    "\n",
    "    # Prepare for interaction with environment\n",
    "    start_time = time.time()\n",
    "    o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(steps_per_epoch):\n",
    "            a, v, logp = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "\n",
    "            next_o, r, d, _ = env.step(a)\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            # save and log\n",
    "            buf.store(o, a, r, v, logp)\n",
    "            VVals.append(v)\n",
    "\n",
    "            # Update obs (critical!)\n",
    "            o = next_o\n",
    "\n",
    "            timeout = ep_len == max_ep_len\n",
    "            terminal = d or timeout\n",
    "            epoch_ended = t==steps_per_epoch-1\n",
    "\n",
    "            if terminal or epoch_ended:\n",
    "                if epoch_ended and not(terminal):\n",
    "                    print('Warning: trajectory cut off by epoch at %d steps.'%ep_len, flush=True)\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                if timeout or epoch_ended:\n",
    "                    _, v, _ = ac.step(torch.as_tensor(o, dtype=torch.float32))\n",
    "                else:\n",
    "                    v = 0\n",
    "                buf.finish_path(v)\n",
    "                if terminal:\n",
    "                    EpRet.append(ep_ret)\n",
    "                    EpLen.append(ep_len)\n",
    "                o, ep_ret, ep_len = env.reset(), 0, 0\n",
    "\n",
    "        # Perform PPO update!\n",
    "        update()\n",
    "\n",
    "        TotalEnvInteracts.append((epoch+1)*steps_per_epoch)\n",
    "        Time.append(time.time()-start_time)\n",
    "\n",
    "        print(f'[Epoch:{epoch}] EpRet:{np.min(EpRet[-10:]):8.2f} < {np.mean(EpRet[-10:]):8.2f} < {np.max(EpRet[-10:]):8.2f}, EpLen:{np.mean(EpLen[-10:]):8.2f}, VVals:{np.mean(VVals[-10:]):8.2f}, TotalEnvInteracts:{TotalEnvInteracts[-1]:8d}, LossPi:{np.mean(LossPi[-10:]):8.2f}, LossV:{np.mean(LossV[-10:]):8.2f}, Entropy:{np.mean(Entropy[-10:]):8.2f}, KL:{np.mean(KL[-10:]):8.2f}, Time:{Time[-1]:8.2f}')\n",
    "    return ac, EpRet, EpLen, VVals, TotalEnvInteracts, LossPi, LossV, Entropy, KL, Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of parameters: \t pi: 5708, \t v: 5377\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:0] EpRet: -409.43 <  -307.95 <  -241.82, EpLen: 1000.00, VVals:    0.03, TotalEnvInteracts:    5000, LossPi:   -0.00, LossV: 1115.15, Entropy:    0.92, KL:    0.01, Time:    1.80\n",
      "Early stopping at step 46 due to reaching max kl.\n",
      "[Epoch:1] EpRet: -409.43 <  -281.83 <  -193.32, EpLen: 1000.00, VVals:   -7.36, TotalEnvInteracts:   10000, LossPi:    0.00, LossV:  765.52, Entropy:    0.92, KL:    0.01, Time:    3.45\n",
      "Early stopping at step 78 due to reaching max kl.\n",
      "[Epoch:2] EpRet: -374.54 <  -262.57 <  -189.14, EpLen: 1000.00, VVals:  -15.37, TotalEnvInteracts:   15000, LossPi:    0.00, LossV:  605.40, Entropy:    0.91, KL:    0.01, Time:    5.47\n",
      "Early stopping at step 61 due to reaching max kl.\n",
      "[Epoch:3] EpRet: -374.54 <  -262.46 <  -154.48, EpLen: 1000.00, VVals:  -19.75, TotalEnvInteracts:   20000, LossPi:   -0.00, LossV:  510.14, Entropy:    0.91, KL:    0.01, Time:    7.28\n",
      "Early stopping at step 57 due to reaching max kl.\n",
      "[Epoch:4] EpRet: -415.62 <  -250.61 <  -122.42, EpLen: 1000.00, VVals:  -22.18, TotalEnvInteracts:   25000, LossPi:   -0.00, LossV:  460.49, Entropy:    0.91, KL:    0.01, Time:    9.01\n",
      "Early stopping at step 34 due to reaching max kl.\n",
      "[Epoch:5] EpRet: -415.62 <  -245.94 <  -122.42, EpLen: 1000.00, VVals:  -23.73, TotalEnvInteracts:   30000, LossPi:   -0.00, LossV:  413.59, Entropy:    0.91, KL:    0.01, Time:   10.68\n",
      "Early stopping at step 30 due to reaching max kl.\n",
      "[Epoch:6] EpRet: -300.80 <  -238.06 <  -122.75, EpLen: 1000.00, VVals:  -23.36, TotalEnvInteracts:   35000, LossPi:    0.00, LossV:  376.59, Entropy:    0.90, KL:    0.01, Time:   12.30\n",
      "Early stopping at step 6 due to reaching max kl.\n",
      "[Epoch:7] EpRet: -339.37 <  -231.08 <  -122.75, EpLen: 1000.00, VVals:  -22.90, TotalEnvInteracts:   40000, LossPi:    0.00, LossV:  351.46, Entropy:    0.90, KL:    0.01, Time:   13.81\n",
      "Early stopping at step 38 due to reaching max kl.\n",
      "[Epoch:8] EpRet: -339.37 <  -225.29 <   -88.26, EpLen: 1000.00, VVals:  -25.54, TotalEnvInteracts:   45000, LossPi:    0.00, LossV:  328.57, Entropy:    0.90, KL:    0.01, Time:   15.52\n",
      "Early stopping at step 35 due to reaching max kl.\n",
      "[Epoch:9] EpRet: -288.10 <  -183.36 <   -70.36, EpLen: 1000.00, VVals:  -26.76, TotalEnvInteracts:   50000, LossPi:    0.00, LossV:  313.15, Entropy:    0.90, KL:    0.01, Time:   17.21\n",
      "Early stopping at step 33 due to reaching max kl.\n",
      "[Epoch:10] EpRet: -195.60 <  -137.15 <   -16.42, EpLen: 1000.00, VVals:  -14.09, TotalEnvInteracts:   55000, LossPi:    0.00, LossV:  214.61, Entropy:    0.89, KL:    0.02, Time:   18.81\n",
      "Early stopping at step 32 due to reaching max kl.\n",
      "[Epoch:11] EpRet: -279.60 <  -145.45 <   -16.42, EpLen: 1000.00, VVals:  -18.69, TotalEnvInteracts:   60000, LossPi:    0.00, LossV:  185.46, Entropy:    0.89, KL:    0.02, Time:   20.52\n",
      "Early stopping at step 38 due to reaching max kl.\n",
      "[Epoch:12] EpRet: -279.60 <  -140.57 <     2.24, EpLen: 1000.00, VVals:  -25.76, TotalEnvInteracts:   65000, LossPi:    0.00, LossV:  169.30, Entropy:    0.88, KL:    0.02, Time:   22.18\n",
      "Early stopping at step 7 due to reaching max kl.\n",
      "[Epoch:13] EpRet: -251.13 <  -134.50 <     2.24, EpLen: 1000.00, VVals:   -4.92, TotalEnvInteracts:   70000, LossPi:    0.00, LossV:  164.10, Entropy:    0.88, KL:    0.02, Time:   23.67\n",
      "Early stopping at step 53 due to reaching max kl.\n",
      "[Epoch:14] EpRet: -251.13 <  -128.16 <   -32.75, EpLen: 1000.00, VVals:   -8.36, TotalEnvInteracts:   75000, LossPi:   -0.00, LossV:  152.94, Entropy:    0.87, KL:    0.02, Time:   25.42\n",
      "Early stopping at step 60 due to reaching max kl.\n",
      "[Epoch:15] EpRet: -160.36 <   -88.88 <   -25.51, EpLen: 1000.00, VVals:  -13.53, TotalEnvInteracts:   80000, LossPi:   -0.00, LossV:  149.58, Entropy:    0.87, KL:    0.02, Time:   27.17\n",
      "[Epoch:16] EpRet: -174.57 <   -80.72 <     0.50, EpLen: 1000.00, VVals:  -20.35, TotalEnvInteracts:   85000, LossPi:   -0.00, LossV:  152.35, Entropy:    0.86, KL:    0.01, Time:   29.03\n",
      "Early stopping at step 33 due to reaching max kl.\n",
      "[Epoch:17] EpRet: -174.57 <   -57.98 <    48.41, EpLen: 1000.00, VVals:   -0.93, TotalEnvInteracts:   90000, LossPi:   -0.00, LossV:  151.76, Entropy:    0.86, KL:    0.01, Time:   30.69\n",
      "Early stopping at step 54 due to reaching max kl.\n",
      "[Epoch:18] EpRet: -232.85 <   -57.86 <    48.41, EpLen: 1000.00, VVals:    4.24, TotalEnvInteracts:   95000, LossPi:   -0.00, LossV:  164.59, Entropy:    0.85, KL:    0.01, Time:   32.46\n",
      "Early stopping at step 24 due to reaching max kl.\n",
      "[Epoch:19] EpRet: -283.84 <  -105.66 <    23.86, EpLen: 1000.00, VVals:  -20.49, TotalEnvInteracts:  100000, LossPi:   -0.00, LossV:  160.99, Entropy:    0.85, KL:    0.01, Time:   34.14\n",
      "Early stopping at step 6 due to reaching max kl.\n",
      "[Epoch:20] EpRet: -283.84 <   -70.66 <   115.71, EpLen: 1000.00, VVals:  -11.90, TotalEnvInteracts:  105000, LossPi:    0.00, LossV:  163.26, Entropy:    0.84, KL:    0.01, Time:   35.71\n",
      "Early stopping at step 5 due to reaching max kl.\n",
      "[Epoch:21] EpRet: -176.87 <   -28.13 <   115.71, EpLen: 1000.00, VVals:    4.52, TotalEnvInteracts:  110000, LossPi:    0.00, LossV:  164.46, Entropy:    0.84, KL:    0.02, Time:   37.21\n",
      "Early stopping at step 38 due to reaching max kl.\n",
      "[Epoch:22] EpRet: -257.04 <   -50.48 <    13.86, EpLen: 1000.00, VVals:   -2.52, TotalEnvInteracts:  115000, LossPi:    0.00, LossV:  161.56, Entropy:    0.83, KL:    0.02, Time:   38.94\n",
      "Early stopping at step 74 due to reaching max kl.\n",
      "[Epoch:23] EpRet: -257.04 <    -7.14 <   117.77, EpLen: 1000.00, VVals:   -1.94, TotalEnvInteracts:  120000, LossPi:    0.00, LossV:  164.24, Entropy:    0.83, KL:    0.01, Time:   40.90\n",
      "Early stopping at step 34 due to reaching max kl.\n",
      "[Epoch:24] EpRet: -225.62 <    15.74 <   117.77, EpLen: 1000.00, VVals:  -19.83, TotalEnvInteracts:  125000, LossPi:    0.00, LossV:  161.15, Entropy:    0.82, KL:    0.02, Time:   42.69\n",
      "Early stopping at step 68 due to reaching max kl.\n",
      "[Epoch:25] EpRet: -225.62 <    -2.09 <    89.73, EpLen: 1000.00, VVals:    5.25, TotalEnvInteracts:  130000, LossPi:    0.00, LossV:  161.54, Entropy:    0.82, KL:    0.01, Time:   44.59\n",
      "Early stopping at step 5 due to reaching max kl.\n",
      "[Epoch:26] EpRet:   -6.96 <    45.09 <   124.07, EpLen: 1000.00, VVals:   -3.24, TotalEnvInteracts:  135000, LossPi:    0.00, LossV:  161.62, Entropy:    0.81, KL:    0.02, Time:   46.16\n",
      "Early stopping at step 49 due to reaching max kl.\n",
      "[Epoch:27] EpRet: -186.67 <     7.11 <   124.07, EpLen: 1000.00, VVals:   13.15, TotalEnvInteracts:  140000, LossPi:    0.00, LossV:  176.22, Entropy:    0.81, KL:    0.02, Time:   47.98\n",
      "Early stopping at step 42 due to reaching max kl.\n",
      "[Epoch:28] EpRet: -190.01 <   -12.72 <   169.52, EpLen: 1000.00, VVals:  -27.03, TotalEnvInteracts:  145000, LossPi:    0.00, LossV:  171.13, Entropy:    0.81, KL:    0.02, Time:   49.78\n",
      "Early stopping at step 6 due to reaching max kl.\n",
      "[Epoch:29] EpRet: -190.01 <    16.97 <   169.52, EpLen: 1000.00, VVals:    4.17, TotalEnvInteracts:  150000, LossPi:    0.00, LossV:  178.07, Entropy:    0.80, KL:    0.02, Time:   51.33\n",
      "Early stopping at step 5 due to reaching max kl.\n",
      "[Epoch:30] EpRet: -140.22 <    28.71 <   191.95, EpLen: 1000.00, VVals:  -28.84, TotalEnvInteracts:  155000, LossPi:    0.00, LossV:  181.87, Entropy:    0.80, KL:    0.02, Time:   52.80\n",
      "Early stopping at step 41 due to reaching max kl.\n",
      "[Epoch:31] EpRet:  -97.10 <    72.10 <   191.95, EpLen: 1000.00, VVals:   10.45, TotalEnvInteracts:  160000, LossPi:   -0.00, LossV:  187.38, Entropy:    0.79, KL:    0.02, Time:   54.50\n",
      "Early stopping at step 27 due to reaching max kl.\n",
      "[Epoch:32] EpRet: -209.46 <    48.27 <   227.79, EpLen: 1000.00, VVals:  -25.14, TotalEnvInteracts:  165000, LossPi:    0.00, LossV:  190.79, Entropy:    0.79, KL:    0.02, Time:   56.14\n",
      "Early stopping at step 42 due to reaching max kl.\n",
      "[Epoch:33] EpRet: -209.46 <    63.43 <   227.79, EpLen: 1000.00, VVals:   12.88, TotalEnvInteracts:  170000, LossPi:    0.00, LossV:  195.04, Entropy:    0.79, KL:    0.02, Time:   57.92\n",
      "Early stopping at step 29 due to reaching max kl.\n",
      "[Epoch:34] EpRet:  -55.87 <   101.97 <   250.23, EpLen: 1000.00, VVals:    5.63, TotalEnvInteracts:  175000, LossPi:   -0.00, LossV:  204.60, Entropy:    0.78, KL:    0.02, Time:   59.56\n",
      "Early stopping at step 28 due to reaching max kl.\n",
      "[Epoch:35] EpRet:  -77.35 <    86.66 <   250.23, EpLen: 1000.00, VVals:    9.93, TotalEnvInteracts:  180000, LossPi:   -0.00, LossV:  213.05, Entropy:    0.78, KL:    0.02, Time:   61.21\n",
      "Early stopping at step 27 due to reaching max kl.\n",
      "[Epoch:36] EpRet:  -77.35 <   100.37 <   260.33, EpLen: 1000.00, VVals:    5.96, TotalEnvInteracts:  185000, LossPi:   -0.00, LossV:  222.47, Entropy:    0.77, KL:    0.02, Time:   62.84\n",
      "Early stopping at step 37 due to reaching max kl.\n",
      "[Epoch:37] EpRet:  -32.30 <   143.63 <   331.80, EpLen: 1000.00, VVals:   12.74, TotalEnvInteracts:  190000, LossPi:   -0.00, LossV:  230.04, Entropy:    0.77, KL:    0.02, Time:   64.52\n",
      "Early stopping at step 52 due to reaching max kl.\n",
      "[Epoch:38] EpRet: -125.67 <   136.43 <   331.80, EpLen: 1000.00, VVals:   23.15, TotalEnvInteracts:  195000, LossPi:    0.00, LossV:  232.54, Entropy:    0.77, KL:    0.02, Time:   66.43\n",
      "Early stopping at step 5 due to reaching max kl.\n",
      "[Epoch:39] EpRet: -125.67 <   140.19 <   245.59, EpLen: 1000.00, VVals:   14.69, TotalEnvInteracts:  200000, LossPi:    0.00, LossV:  233.65, Entropy:    0.76, KL:    0.02, Time:   67.98\n",
      "Early stopping at step 28 due to reaching max kl.\n",
      "[Epoch:40] EpRet:  -18.20 <   131.35 <   245.59, EpLen: 1000.00, VVals:   15.97, TotalEnvInteracts:  205000, LossPi:   -0.00, LossV:  238.33, Entropy:    0.76, KL:    0.02, Time:   69.74\n",
      "Early stopping at step 26 due to reaching max kl.\n",
      "[Epoch:41] EpRet:  -18.20 <   147.74 <   253.94, EpLen: 1000.00, VVals:   15.97, TotalEnvInteracts:  210000, LossPi:    0.00, LossV:  252.17, Entropy:    0.75, KL:    0.02, Time:   71.35\n",
      "Early stopping at step 27 due to reaching max kl.\n",
      "[Epoch:42] EpRet:  133.51 <   236.60 <   355.63, EpLen: 1000.00, VVals:   18.09, TotalEnvInteracts:  215000, LossPi:   -0.00, LossV:  268.62, Entropy:    0.74, KL:    0.02, Time:   73.00\n",
      "[Epoch:43] EpRet:   77.45 <   241.59 <   384.69, EpLen: 1000.00, VVals:   23.84, TotalEnvInteracts:  220000, LossPi:   -0.00, LossV:  269.64, Entropy:    0.74, KL:    0.02, Time:   75.16\n",
      "Early stopping at step 53 due to reaching max kl.\n",
      "[Epoch:44] EpRet:   77.45 <   243.00 <   384.69, EpLen: 1000.00, VVals:   27.09, TotalEnvInteracts:  225000, LossPi:   -0.00, LossV:  281.88, Entropy:    0.73, KL:    0.02, Time:   77.09\n",
      "Early stopping at step 59 due to reaching max kl.\n",
      "[Epoch:45] EpRet:  141.36 <   274.54 <   392.42, EpLen: 1000.00, VVals:   14.64, TotalEnvInteracts:  230000, LossPi:   -0.00, LossV:  287.16, Entropy:    0.73, KL:    0.02, Time:   79.08\n",
      "Early stopping at step 31 due to reaching max kl.\n",
      "[Epoch:46] EpRet:  141.36 <   268.57 <   392.42, EpLen: 1000.00, VVals:   29.55, TotalEnvInteracts:  235000, LossPi:   -0.00, LossV:  277.04, Entropy:    0.72, KL:    0.01, Time:   80.79\n",
      "Early stopping at step 5 due to reaching max kl.\n",
      "[Epoch:47] EpRet:  114.06 <   240.10 <   324.27, EpLen: 1000.00, VVals:   19.04, TotalEnvInteracts:  240000, LossPi:   -0.00, LossV:  268.85, Entropy:    0.72, KL:    0.02, Time:   82.40\n",
      "Early stopping at step 31 due to reaching max kl.\n",
      "[Epoch:48] EpRet:  -20.36 <   238.98 <   402.51, EpLen: 1000.00, VVals:    2.26, TotalEnvInteracts:  245000, LossPi:    0.00, LossV:  280.16, Entropy:    0.71, KL:    0.02, Time:   84.16\n",
      "Early stopping at step 26 due to reaching max kl.\n",
      "[Epoch:49] EpRet:  -20.36 <   279.43 <   414.94, EpLen: 1000.00, VVals:   21.04, TotalEnvInteracts:  250000, LossPi:    0.00, LossV:  282.01, Entropy:    0.71, KL:    0.02, Time:   85.85\n"
     ]
    }
   ],
   "source": [
    "ac = ppo(lambda : HalfCheetahEnv(), actor_critic=MLPActorCritic,\n",
    "         ac_kwargs=dict(hidden_sizes=[64,64]), gamma=0.99,\n",
    "         seed=0, steps_per_epoch=5000, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "# if (torch.cuda.is_available()):\n",
    "#     device = torch.device('cuda:0')\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n",
    "# else:\n",
    "#     print(\"Device set to : cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"show_videos\" style=\"border-spacing:0px;\"><tr><td style=\"padding:1px;\"><video controls width=\"320\" height=\"240\" style=\"object-fit:cover;\" loop autoplay muted>\n",
       "      <source src=\"data:video/mp4;base64,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\" type=\"video/mp4\"/>\n",
       "      This browser does not support the video tag.\n",
       "      </video></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = HalfCheetahEnv()\n",
    "imgs = []\n",
    "\n",
    "obs = env.reset()\n",
    "for t in range(1000):\n",
    "  with torch.no_grad():\n",
    "    obs = torch.as_tensor(obs, dtype=torch.float32)\n",
    "    action = ac[0].act(obs)\n",
    "  obs, reward, terminated, info = env.step(action)\n",
    "  img = env.render()\n",
    "  imgs.append(img)\n",
    "\n",
    "media.show_video(imgs, fps=1/env.dt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
