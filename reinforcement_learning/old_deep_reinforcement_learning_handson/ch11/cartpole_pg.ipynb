{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "REWARD_STEPS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(old: Optional[float], val: float, alpha: float = 0.95) -> float:\n",
    "    if old is None:\n",
    "        return val\n",
    "    return old * alpha + (1-alpha)*val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "writer = SummaryWriter(comment=\"-cartpole-pg\")\n",
    "\n",
    "net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor,\n",
    "                               apply_softmax=True)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "    env, agent, gamma=GAMMA, steps_count=REWARD_STEPS\n",
    ")\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "step_rewards = []\n",
    "step_idx = 0\n",
    "done_episodes = 0\n",
    "reward_sum = 0.0\n",
    "bs_smoothed = entropy = l_entropy = l_policy = l_total = None\n",
    "\n",
    "batch_states, batch_actions, batch_scales = [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22: reward:  22.00, mean_100:  22.00, episodes: 1\n",
      "34: reward:  12.00, mean_100:  17.00, episodes: 2\n",
      "53: reward:  19.00, mean_100:  17.67, episodes: 3\n",
      "69: reward:  16.00, mean_100:  17.25, episodes: 4\n",
      "84: reward:  15.00, mean_100:  16.80, episodes: 5\n",
      "101: reward:  17.00, mean_100:  16.83, episodes: 6\n",
      "110: reward:   9.00, mean_100:  15.71, episodes: 7\n",
      "121: reward:  11.00, mean_100:  15.12, episodes: 8\n",
      "132: reward:  11.00, mean_100:  14.67, episodes: 9\n",
      "188: reward:  56.00, mean_100:  18.80, episodes: 10\n",
      "239: reward:  51.00, mean_100:  21.73, episodes: 11\n",
      "297: reward:  58.00, mean_100:  24.75, episodes: 12\n",
      "326: reward:  29.00, mean_100:  25.08, episodes: 13\n",
      "356: reward:  30.00, mean_100:  25.43, episodes: 14\n",
      "377: reward:  21.00, mean_100:  25.13, episodes: 15\n",
      "435: reward:  58.00, mean_100:  27.19, episodes: 16\n",
      "462: reward:  27.00, mean_100:  27.18, episodes: 17\n",
      "480: reward:  18.00, mean_100:  26.67, episodes: 18\n",
      "501: reward:  21.00, mean_100:  26.37, episodes: 19\n",
      "538: reward:  37.00, mean_100:  26.90, episodes: 20\n",
      "569: reward:  31.00, mean_100:  27.10, episodes: 21\n",
      "602: reward:  33.00, mean_100:  27.36, episodes: 22\n",
      "630: reward:  28.00, mean_100:  27.39, episodes: 23\n",
      "675: reward:  45.00, mean_100:  28.12, episodes: 24\n",
      "719: reward:  44.00, mean_100:  28.76, episodes: 25\n",
      "733: reward:  14.00, mean_100:  28.19, episodes: 26\n",
      "763: reward:  30.00, mean_100:  28.26, episodes: 27\n",
      "851: reward:  88.00, mean_100:  30.39, episodes: 28\n",
      "904: reward:  53.00, mean_100:  31.17, episodes: 29\n",
      "939: reward:  35.00, mean_100:  31.30, episodes: 30\n",
      "982: reward:  43.00, mean_100:  31.68, episodes: 31\n",
      "1037: reward:  55.00, mean_100:  32.41, episodes: 32\n",
      "1053: reward:  16.00, mean_100:  31.91, episodes: 33\n",
      "1115: reward:  62.00, mean_100:  32.79, episodes: 34\n",
      "1139: reward:  24.00, mean_100:  32.54, episodes: 35\n",
      "1174: reward:  35.00, mean_100:  32.61, episodes: 36\n",
      "1224: reward:  50.00, mean_100:  33.08, episodes: 37\n",
      "1235: reward:  11.00, mean_100:  32.50, episodes: 38\n",
      "1296: reward:  61.00, mean_100:  33.23, episodes: 39\n",
      "1341: reward:  45.00, mean_100:  33.52, episodes: 40\n",
      "1392: reward:  51.00, mean_100:  33.95, episodes: 41\n",
      "1406: reward:  14.00, mean_100:  33.48, episodes: 42\n",
      "1498: reward:  92.00, mean_100:  34.84, episodes: 43\n",
      "1526: reward:  28.00, mean_100:  34.68, episodes: 44\n",
      "1554: reward:  28.00, mean_100:  34.53, episodes: 45\n",
      "1591: reward:  37.00, mean_100:  34.59, episodes: 46\n",
      "1684: reward:  93.00, mean_100:  35.83, episodes: 47\n",
      "1706: reward:  22.00, mean_100:  35.54, episodes: 48\n",
      "1736: reward:  30.00, mean_100:  35.43, episodes: 49\n",
      "1798: reward:  62.00, mean_100:  35.96, episodes: 50\n",
      "1819: reward:  21.00, mean_100:  35.67, episodes: 51\n",
      "1863: reward:  44.00, mean_100:  35.83, episodes: 52\n",
      "1939: reward:  76.00, mean_100:  36.58, episodes: 53\n",
      "1982: reward:  43.00, mean_100:  36.70, episodes: 54\n",
      "2000: reward:  18.00, mean_100:  36.36, episodes: 55\n",
      "2014: reward:  14.00, mean_100:  35.96, episodes: 56\n",
      "2064: reward:  50.00, mean_100:  36.21, episodes: 57\n",
      "2111: reward:  47.00, mean_100:  36.40, episodes: 58\n",
      "2185: reward:  74.00, mean_100:  37.03, episodes: 59\n",
      "2249: reward:  64.00, mean_100:  37.48, episodes: 60\n",
      "2293: reward:  44.00, mean_100:  37.59, episodes: 61\n",
      "2341: reward:  48.00, mean_100:  37.76, episodes: 62\n",
      "2369: reward:  28.00, mean_100:  37.60, episodes: 63\n",
      "2452: reward:  83.00, mean_100:  38.31, episodes: 64\n",
      "2483: reward:  31.00, mean_100:  38.20, episodes: 65\n",
      "2524: reward:  41.00, mean_100:  38.24, episodes: 66\n",
      "2537: reward:  13.00, mean_100:  37.87, episodes: 67\n",
      "2564: reward:  27.00, mean_100:  37.71, episodes: 68\n",
      "2584: reward:  20.00, mean_100:  37.45, episodes: 69\n",
      "2631: reward:  47.00, mean_100:  37.59, episodes: 70\n",
      "2770: reward: 139.00, mean_100:  39.01, episodes: 71\n",
      "2867: reward:  97.00, mean_100:  39.82, episodes: 72\n",
      "2888: reward:  21.00, mean_100:  39.56, episodes: 73\n",
      "2974: reward:  86.00, mean_100:  40.19, episodes: 74\n",
      "3054: reward:  80.00, mean_100:  40.72, episodes: 75\n",
      "3227: reward: 173.00, mean_100:  42.46, episodes: 76\n",
      "3355: reward: 128.00, mean_100:  43.57, episodes: 77\n",
      "3403: reward:  48.00, mean_100:  43.63, episodes: 78\n",
      "3516: reward: 113.00, mean_100:  44.51, episodes: 79\n",
      "3558: reward:  42.00, mean_100:  44.48, episodes: 80\n",
      "3704: reward: 146.00, mean_100:  45.73, episodes: 81\n",
      "3767: reward:  63.00, mean_100:  45.94, episodes: 82\n",
      "3788: reward:  21.00, mean_100:  45.64, episodes: 83\n",
      "3948: reward: 160.00, mean_100:  47.00, episodes: 84\n",
      "4027: reward:  79.00, mean_100:  47.38, episodes: 85\n",
      "4123: reward:  96.00, mean_100:  47.94, episodes: 86\n",
      "4320: reward: 197.00, mean_100:  49.66, episodes: 87\n",
      "4337: reward:  17.00, mean_100:  49.28, episodes: 88\n",
      "4466: reward: 129.00, mean_100:  50.18, episodes: 89\n",
      "4491: reward:  25.00, mean_100:  49.90, episodes: 90\n",
      "4587: reward:  96.00, mean_100:  50.41, episodes: 91\n",
      "4639: reward:  52.00, mean_100:  50.42, episodes: 92\n",
      "4694: reward:  55.00, mean_100:  50.47, episodes: 93\n",
      "4779: reward:  85.00, mean_100:  50.84, episodes: 94\n",
      "4838: reward:  59.00, mean_100:  50.93, episodes: 95\n",
      "4863: reward:  25.00, mean_100:  50.66, episodes: 96\n",
      "5027: reward: 164.00, mean_100:  51.82, episodes: 97\n",
      "5200: reward: 173.00, mean_100:  53.06, episodes: 98\n",
      "5219: reward:  19.00, mean_100:  52.72, episodes: 99\n",
      "5253: reward:  34.00, mean_100:  52.53, episodes: 100\n",
      "5330: reward:  77.00, mean_100:  53.08, episodes: 101\n",
      "5474: reward: 144.00, mean_100:  54.40, episodes: 102\n",
      "5674: reward: 200.00, mean_100:  56.21, episodes: 103\n",
      "5743: reward:  69.00, mean_100:  56.74, episodes: 104\n",
      "5943: reward: 200.00, mean_100:  58.59, episodes: 105\n",
      "6086: reward: 143.00, mean_100:  59.85, episodes: 106\n",
      "6136: reward:  50.00, mean_100:  60.26, episodes: 107\n",
      "6323: reward: 187.00, mean_100:  62.02, episodes: 108\n",
      "6435: reward: 112.00, mean_100:  63.03, episodes: 109\n",
      "6635: reward: 200.00, mean_100:  64.47, episodes: 110\n",
      "6774: reward: 139.00, mean_100:  65.35, episodes: 111\n",
      "6974: reward: 200.00, mean_100:  66.77, episodes: 112\n",
      "7174: reward: 200.00, mean_100:  68.48, episodes: 113\n",
      "7374: reward: 200.00, mean_100:  70.18, episodes: 114\n",
      "7574: reward: 200.00, mean_100:  71.97, episodes: 115\n",
      "7741: reward: 167.00, mean_100:  73.06, episodes: 116\n",
      "7941: reward: 200.00, mean_100:  74.79, episodes: 117\n",
      "8141: reward: 200.00, mean_100:  76.61, episodes: 118\n",
      "8215: reward:  74.00, mean_100:  77.14, episodes: 119\n",
      "8401: reward: 186.00, mean_100:  78.63, episodes: 120\n",
      "8601: reward: 200.00, mean_100:  80.32, episodes: 121\n",
      "8801: reward: 200.00, mean_100:  81.99, episodes: 122\n",
      "9001: reward: 200.00, mean_100:  83.71, episodes: 123\n",
      "9156: reward: 155.00, mean_100:  84.81, episodes: 124\n",
      "9339: reward: 183.00, mean_100:  86.20, episodes: 125\n",
      "9539: reward: 200.00, mean_100:  88.06, episodes: 126\n",
      "9739: reward: 200.00, mean_100:  89.76, episodes: 127\n",
      "9939: reward: 200.00, mean_100:  90.88, episodes: 128\n",
      "10011: reward:  72.00, mean_100:  91.07, episodes: 129\n",
      "10211: reward: 200.00, mean_100:  92.72, episodes: 130\n",
      "10411: reward: 200.00, mean_100:  94.29, episodes: 131\n",
      "10516: reward: 105.00, mean_100:  94.79, episodes: 132\n",
      "10707: reward: 191.00, mean_100:  96.54, episodes: 133\n",
      "10907: reward: 200.00, mean_100:  97.92, episodes: 134\n",
      "11107: reward: 200.00, mean_100:  99.68, episodes: 135\n",
      "11307: reward: 200.00, mean_100: 101.33, episodes: 136\n",
      "11458: reward: 151.00, mean_100: 102.34, episodes: 137\n",
      "11656: reward: 198.00, mean_100: 104.21, episodes: 138\n",
      "11856: reward: 200.00, mean_100: 105.60, episodes: 139\n",
      "12056: reward: 200.00, mean_100: 107.15, episodes: 140\n",
      "12256: reward: 200.00, mean_100: 108.64, episodes: 141\n",
      "12370: reward: 114.00, mean_100: 109.64, episodes: 142\n",
      "12451: reward:  81.00, mean_100: 109.53, episodes: 143\n",
      "12507: reward:  56.00, mean_100: 109.81, episodes: 144\n",
      "12707: reward: 200.00, mean_100: 111.53, episodes: 145\n",
      "12841: reward: 134.00, mean_100: 112.50, episodes: 146\n",
      "12969: reward: 128.00, mean_100: 112.85, episodes: 147\n",
      "13122: reward: 153.00, mean_100: 114.16, episodes: 148\n",
      "13322: reward: 200.00, mean_100: 115.86, episodes: 149\n",
      "13442: reward: 120.00, mean_100: 116.44, episodes: 150\n",
      "13642: reward: 200.00, mean_100: 118.23, episodes: 151\n",
      "13842: reward: 200.00, mean_100: 119.79, episodes: 152\n",
      "13961: reward: 119.00, mean_100: 120.22, episodes: 153\n",
      "14161: reward: 200.00, mean_100: 121.79, episodes: 154\n",
      "14244: reward:  83.00, mean_100: 122.44, episodes: 155\n",
      "14418: reward: 174.00, mean_100: 124.04, episodes: 156\n",
      "14618: reward: 200.00, mean_100: 125.54, episodes: 157\n",
      "14818: reward: 200.00, mean_100: 127.07, episodes: 158\n",
      "15018: reward: 200.00, mean_100: 128.33, episodes: 159\n",
      "15218: reward: 200.00, mean_100: 129.69, episodes: 160\n",
      "15418: reward: 200.00, mean_100: 131.25, episodes: 161\n",
      "15618: reward: 200.00, mean_100: 132.77, episodes: 162\n",
      "15818: reward: 200.00, mean_100: 134.49, episodes: 163\n",
      "16018: reward: 200.00, mean_100: 135.66, episodes: 164\n",
      "16218: reward: 200.00, mean_100: 137.35, episodes: 165\n",
      "16418: reward: 200.00, mean_100: 138.94, episodes: 166\n",
      "16618: reward: 200.00, mean_100: 140.81, episodes: 167\n",
      "16790: reward: 172.00, mean_100: 142.26, episodes: 168\n",
      "16880: reward:  90.00, mean_100: 142.96, episodes: 169\n",
      "17080: reward: 200.00, mean_100: 144.49, episodes: 170\n",
      "17276: reward: 196.00, mean_100: 145.06, episodes: 171\n",
      "17367: reward:  91.00, mean_100: 145.00, episodes: 172\n",
      "17567: reward: 200.00, mean_100: 146.79, episodes: 173\n",
      "17634: reward:  67.00, mean_100: 146.60, episodes: 174\n",
      "17713: reward:  79.00, mean_100: 146.59, episodes: 175\n",
      "17913: reward: 200.00, mean_100: 146.86, episodes: 176\n",
      "18113: reward: 200.00, mean_100: 147.58, episodes: 177\n",
      "18313: reward: 200.00, mean_100: 149.10, episodes: 178\n",
      "18512: reward: 199.00, mean_100: 149.96, episodes: 179\n",
      "18712: reward: 200.00, mean_100: 151.54, episodes: 180\n",
      "18838: reward: 126.00, mean_100: 151.34, episodes: 181\n",
      "18952: reward: 114.00, mean_100: 151.85, episodes: 182\n",
      "19152: reward: 200.00, mean_100: 153.64, episodes: 183\n",
      "19352: reward: 200.00, mean_100: 154.04, episodes: 184\n",
      "19552: reward: 200.00, mean_100: 155.25, episodes: 185\n",
      "19752: reward: 200.00, mean_100: 156.29, episodes: 186\n",
      "19952: reward: 200.00, mean_100: 156.32, episodes: 187\n",
      "20152: reward: 200.00, mean_100: 158.15, episodes: 188\n",
      "20352: reward: 200.00, mean_100: 158.86, episodes: 189\n",
      "20552: reward: 200.00, mean_100: 160.61, episodes: 190\n",
      "20752: reward: 200.00, mean_100: 161.65, episodes: 191\n",
      "20917: reward: 165.00, mean_100: 162.78, episodes: 192\n",
      "21117: reward: 200.00, mean_100: 164.23, episodes: 193\n",
      "21317: reward: 200.00, mean_100: 165.38, episodes: 194\n",
      "21517: reward: 200.00, mean_100: 166.79, episodes: 195\n",
      "21717: reward: 200.00, mean_100: 168.54, episodes: 196\n",
      "21917: reward: 200.00, mean_100: 168.90, episodes: 197\n",
      "22117: reward: 200.00, mean_100: 169.17, episodes: 198\n",
      "22317: reward: 200.00, mean_100: 170.98, episodes: 199\n",
      "22517: reward: 200.00, mean_100: 172.64, episodes: 200\n",
      "22717: reward: 200.00, mean_100: 173.87, episodes: 201\n",
      "22902: reward: 185.00, mean_100: 174.28, episodes: 202\n",
      "23061: reward: 159.00, mean_100: 173.87, episodes: 203\n",
      "23261: reward: 200.00, mean_100: 175.18, episodes: 204\n",
      "23461: reward: 200.00, mean_100: 175.18, episodes: 205\n",
      "23499: reward:  38.00, mean_100: 174.13, episodes: 206\n",
      "23615: reward: 116.00, mean_100: 174.79, episodes: 207\n",
      "23798: reward: 183.00, mean_100: 174.75, episodes: 208\n",
      "23998: reward: 200.00, mean_100: 175.63, episodes: 209\n",
      "24196: reward: 198.00, mean_100: 175.61, episodes: 210\n",
      "24396: reward: 200.00, mean_100: 176.22, episodes: 211\n",
      "24596: reward: 200.00, mean_100: 176.22, episodes: 212\n",
      "24796: reward: 200.00, mean_100: 176.22, episodes: 213\n",
      "24996: reward: 200.00, mean_100: 176.22, episodes: 214\n",
      "25196: reward: 200.00, mean_100: 176.22, episodes: 215\n",
      "25396: reward: 200.00, mean_100: 176.55, episodes: 216\n",
      "25596: reward: 200.00, mean_100: 176.55, episodes: 217\n",
      "25796: reward: 200.00, mean_100: 176.55, episodes: 218\n",
      "25996: reward: 200.00, mean_100: 177.81, episodes: 219\n",
      "26130: reward: 134.00, mean_100: 177.29, episodes: 220\n",
      "26330: reward: 200.00, mean_100: 177.29, episodes: 221\n",
      "26494: reward: 164.00, mean_100: 176.93, episodes: 222\n",
      "26635: reward: 141.00, mean_100: 176.34, episodes: 223\n",
      "26835: reward: 200.00, mean_100: 176.79, episodes: 224\n",
      "27035: reward: 200.00, mean_100: 176.96, episodes: 225\n",
      "27235: reward: 200.00, mean_100: 176.96, episodes: 226\n",
      "27435: reward: 200.00, mean_100: 176.96, episodes: 227\n",
      "27582: reward: 147.00, mean_100: 176.43, episodes: 228\n",
      "27718: reward: 136.00, mean_100: 177.07, episodes: 229\n",
      "27901: reward: 183.00, mean_100: 176.90, episodes: 230\n",
      "28084: reward: 183.00, mean_100: 176.73, episodes: 231\n",
      "28237: reward: 153.00, mean_100: 177.21, episodes: 232\n",
      "28361: reward: 124.00, mean_100: 176.54, episodes: 233\n",
      "28519: reward: 158.00, mean_100: 176.12, episodes: 234\n",
      "28719: reward: 200.00, mean_100: 176.12, episodes: 235\n",
      "28917: reward: 198.00, mean_100: 176.10, episodes: 236\n",
      "28954: reward:  37.00, mean_100: 174.96, episodes: 237\n",
      "28983: reward:  29.00, mean_100: 173.27, episodes: 238\n",
      "29183: reward: 200.00, mean_100: 173.27, episodes: 239\n",
      "29383: reward: 200.00, mean_100: 173.27, episodes: 240\n",
      "29583: reward: 200.00, mean_100: 173.27, episodes: 241\n",
      "29783: reward: 200.00, mean_100: 174.13, episodes: 242\n",
      "29983: reward: 200.00, mean_100: 175.32, episodes: 243\n",
      "30183: reward: 200.00, mean_100: 176.76, episodes: 244\n",
      "30383: reward: 200.00, mean_100: 176.76, episodes: 245\n",
      "30583: reward: 200.00, mean_100: 177.42, episodes: 246\n",
      "30783: reward: 200.00, mean_100: 178.14, episodes: 247\n",
      "30983: reward: 200.00, mean_100: 178.61, episodes: 248\n",
      "31183: reward: 200.00, mean_100: 178.61, episodes: 249\n",
      "31383: reward: 200.00, mean_100: 179.41, episodes: 250\n",
      "31583: reward: 200.00, mean_100: 179.41, episodes: 251\n",
      "31783: reward: 200.00, mean_100: 179.41, episodes: 252\n",
      "31983: reward: 200.00, mean_100: 180.22, episodes: 253\n",
      "32183: reward: 200.00, mean_100: 180.22, episodes: 254\n",
      "32383: reward: 200.00, mean_100: 181.39, episodes: 255\n",
      "32583: reward: 200.00, mean_100: 181.65, episodes: 256\n",
      "32783: reward: 200.00, mean_100: 181.65, episodes: 257\n",
      "32983: reward: 200.00, mean_100: 181.65, episodes: 258\n",
      "33183: reward: 200.00, mean_100: 181.65, episodes: 259\n",
      "33314: reward: 131.00, mean_100: 180.96, episodes: 260\n",
      "33514: reward: 200.00, mean_100: 180.96, episodes: 261\n",
      "33714: reward: 200.00, mean_100: 180.96, episodes: 262\n",
      "33914: reward: 200.00, mean_100: 180.96, episodes: 263\n",
      "34114: reward: 200.00, mean_100: 180.96, episodes: 264\n",
      "34314: reward: 200.00, mean_100: 180.96, episodes: 265\n",
      "34514: reward: 200.00, mean_100: 180.96, episodes: 266\n",
      "34714: reward: 200.00, mean_100: 180.96, episodes: 267\n",
      "34914: reward: 200.00, mean_100: 181.24, episodes: 268\n",
      "35114: reward: 200.00, mean_100: 182.34, episodes: 269\n",
      "35314: reward: 200.00, mean_100: 182.34, episodes: 270\n",
      "35514: reward: 200.00, mean_100: 182.38, episodes: 271\n",
      "35714: reward: 200.00, mean_100: 183.47, episodes: 272\n",
      "35914: reward: 200.00, mean_100: 183.47, episodes: 273\n",
      "36114: reward: 200.00, mean_100: 184.80, episodes: 274\n",
      "36314: reward: 200.00, mean_100: 186.01, episodes: 275\n",
      "36514: reward: 200.00, mean_100: 186.01, episodes: 276\n",
      "36714: reward: 200.00, mean_100: 186.01, episodes: 277\n",
      "36914: reward: 200.00, mean_100: 186.01, episodes: 278\n",
      "37114: reward: 200.00, mean_100: 186.02, episodes: 279\n",
      "37314: reward: 200.00, mean_100: 186.02, episodes: 280\n",
      "37514: reward: 200.00, mean_100: 186.76, episodes: 281\n",
      "37714: reward: 200.00, mean_100: 187.62, episodes: 282\n",
      "37914: reward: 200.00, mean_100: 187.62, episodes: 283\n",
      "38114: reward: 200.00, mean_100: 187.62, episodes: 284\n",
      "38314: reward: 200.00, mean_100: 187.62, episodes: 285\n",
      "38476: reward: 162.00, mean_100: 187.24, episodes: 286\n",
      "38676: reward: 200.00, mean_100: 187.24, episodes: 287\n",
      "38876: reward: 200.00, mean_100: 187.24, episodes: 288\n",
      "39076: reward: 200.00, mean_100: 187.24, episodes: 289\n",
      "39276: reward: 200.00, mean_100: 187.24, episodes: 290\n",
      "39476: reward: 200.00, mean_100: 187.24, episodes: 291\n",
      "39676: reward: 200.00, mean_100: 187.59, episodes: 292\n",
      "39876: reward: 200.00, mean_100: 187.59, episodes: 293\n",
      "40076: reward: 200.00, mean_100: 187.59, episodes: 294\n",
      "40276: reward: 200.00, mean_100: 187.59, episodes: 295\n",
      "40476: reward: 200.00, mean_100: 187.59, episodes: 296\n",
      "40676: reward: 200.00, mean_100: 187.59, episodes: 297\n",
      "40876: reward: 200.00, mean_100: 187.59, episodes: 298\n",
      "41076: reward: 200.00, mean_100: 187.59, episodes: 299\n",
      "41276: reward: 200.00, mean_100: 187.59, episodes: 300\n",
      "41419: reward: 143.00, mean_100: 187.02, episodes: 301\n",
      "41619: reward: 200.00, mean_100: 187.17, episodes: 302\n",
      "41819: reward: 200.00, mean_100: 187.58, episodes: 303\n",
      "42019: reward: 200.00, mean_100: 187.58, episodes: 304\n",
      "42219: reward: 200.00, mean_100: 187.58, episodes: 305\n",
      "42409: reward: 190.00, mean_100: 189.10, episodes: 306\n",
      "42609: reward: 200.00, mean_100: 189.94, episodes: 307\n",
      "42809: reward: 200.00, mean_100: 190.11, episodes: 308\n",
      "43009: reward: 200.00, mean_100: 190.11, episodes: 309\n",
      "43209: reward: 200.00, mean_100: 190.13, episodes: 310\n",
      "43409: reward: 200.00, mean_100: 190.13, episodes: 311\n",
      "43609: reward: 200.00, mean_100: 190.13, episodes: 312\n",
      "43809: reward: 200.00, mean_100: 190.13, episodes: 313\n",
      "44009: reward: 200.00, mean_100: 190.13, episodes: 314\n",
      "44142: reward: 133.00, mean_100: 189.46, episodes: 315\n",
      "44342: reward: 200.00, mean_100: 189.46, episodes: 316\n",
      "44542: reward: 200.00, mean_100: 189.46, episodes: 317\n",
      "44742: reward: 200.00, mean_100: 189.46, episodes: 318\n",
      "44942: reward: 200.00, mean_100: 189.46, episodes: 319\n",
      "45093: reward: 151.00, mean_100: 189.63, episodes: 320\n",
      "45293: reward: 200.00, mean_100: 189.63, episodes: 321\n",
      "45493: reward: 200.00, mean_100: 189.99, episodes: 322\n",
      "45693: reward: 200.00, mean_100: 190.58, episodes: 323\n",
      "45893: reward: 200.00, mean_100: 190.58, episodes: 324\n",
      "46093: reward: 200.00, mean_100: 190.58, episodes: 325\n",
      "46293: reward: 200.00, mean_100: 190.58, episodes: 326\n",
      "46493: reward: 200.00, mean_100: 190.58, episodes: 327\n",
      "46693: reward: 200.00, mean_100: 191.11, episodes: 328\n",
      "46893: reward: 200.00, mean_100: 191.75, episodes: 329\n",
      "47093: reward: 200.00, mean_100: 191.92, episodes: 330\n",
      "47280: reward: 187.00, mean_100: 191.96, episodes: 331\n",
      "47480: reward: 200.00, mean_100: 192.43, episodes: 332\n",
      "47579: reward:  99.00, mean_100: 192.18, episodes: 333\n",
      "47779: reward: 200.00, mean_100: 192.60, episodes: 334\n",
      "47979: reward: 200.00, mean_100: 192.60, episodes: 335\n",
      "48179: reward: 200.00, mean_100: 192.62, episodes: 336\n",
      "48379: reward: 200.00, mean_100: 194.25, episodes: 337\n",
      "48579: reward: 200.00, mean_100: 195.96, episodes: 338\n",
      "Solved in 48579 steps and 338 episodes!\n"
     ]
    }
   ],
   "source": [
    "for step_idx, exp in enumerate(exp_source):\n",
    "    reward_sum += exp.reward\n",
    "    baseline = reward_sum / (step_idx + 1)\n",
    "    writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "    batch_states.append(exp.state)\n",
    "    batch_actions.append(int(exp.action))\n",
    "    batch_scales.append(exp.reward - baseline)\n",
    "    \n",
    "    # handle new rewards\n",
    "    new_rewards = exp_source.pop_total_rewards()\n",
    "    if new_rewards:\n",
    "        done_episodes += 1\n",
    "        reward = new_rewards[0]\n",
    "        total_rewards.append(reward)\n",
    "        mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "        print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\"\n",
    "              % (step_idx, reward, mean_rewards, done_episodes))\n",
    "        writer.add_scalar(\"reward\", reward, step_idx)\n",
    "        writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "        writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "        if mean_rewards > 195:\n",
    "            print(\"Solved in %d steps and %d episodes!\"\n",
    "                  % (step_idx, done_episodes))\n",
    "            break\n",
    "    \n",
    "    if len(batch_states) < BATCH_SIZE:\n",
    "        continue\n",
    "\n",
    "    states_v = torch.FloatTensor(batch_states)\n",
    "    batch_actions_t = torch.LongTensor(batch_actions)\n",
    "    batch_scale_v = torch.FloatTensor(batch_scales)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    logits_v = net(states_v)\n",
    "    log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "    log_prob_actions_v = batch_scale_v * log_prob_v[range(BATCH_SIZE), batch_actions_t]\n",
    "    loss_policy_v = -log_prob_actions_v.mean()\n",
    "    \n",
    "    prob_v = F.softmax(logits_v, dim=1)\n",
    "    entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "    entropy_loss_v = -ENTROPY_BETA * entropy_v\n",
    "    loss_v = loss_policy_v + entropy_loss_v\n",
    "    \n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # cal KL-div\n",
    "    new_logits_v = net(states_v)\n",
    "    new_prob_v = F.softmax(new_logits_v, dim=1)\n",
    "    kl_div_v = -((new_prob_v / prob_v).log() * prob_v).sum(dim=1).mean()\n",
    "    writer.add_scalar(\"kl\", kl_div_v.item(), step_idx)\n",
    "    \n",
    "    grad_max = 0.0\n",
    "    grad_means = 0.0\n",
    "    grad_count = 0\n",
    "    for p in net.parameters():\n",
    "        grad_max = max(grad_max, p.grad.abs().max().item())\n",
    "        grad_means += (p.grad ** 2).mean().sqrt().item()\n",
    "        grad_count += 1\n",
    "    \n",
    "    bs_smoothed = smooth(bs_smoothed, np.mean(batch_scales))\n",
    "    entropy = smooth(entropy, entropy_v.item())\n",
    "    l_entropy = smooth(l_entropy, entropy_loss_v.item())\n",
    "    l_policy = smooth(l_policy, loss_policy_v.item())\n",
    "    l_total = smooth(l_total, loss_v.item())\n",
    "    \n",
    "    writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "    writer.add_scalar(\"entropy\", entropy, step_idx)\n",
    "    writer.add_scalar(\"loss_entropy\", l_entropy, step_idx)\n",
    "    writer.add_scalar(\"loss_policy\", l_policy, step_idx)\n",
    "    writer.add_scalar(\"loss_total\", l_total, step_idx)\n",
    "    writer.add_scalar(\"grad_12\", grad_means / grad_count, step_idx)\n",
    "    writer.add_scalar(\"grad_max\", grad_max, step_idx)\n",
    "    writer.add_scalar(\"batch_scales\", bs_smoothed, step_idx)\n",
    "    \n",
    "    batch_states.clear()\n",
    "    batch_actions.clear()\n",
    "    batch_scales.clear()\n",
    "    \n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
