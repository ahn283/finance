{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\woojin\\anaconda3\\envs\\reinforce\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "# specifying how many complete episodes we will use for training\n",
    "EPISODE_TO_TRAIN = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_qvals(rewards):\n",
    "    res = []\n",
    "    sum_r = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= GAMMA\n",
    "        sum_r += r \n",
    "        res.append(sum_r)\n",
    "    return list(reversed(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "writer = SummaryWriter(comment=\"-cartpole-reinforce\")\n",
    "\n",
    "net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor,\n",
    "                               apply_softmax=True)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)\n",
    "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "step_idx = 0\n",
    "done_episodes = 0\n",
    "\n",
    "batch_episodes = 0\n",
    "batch_states, batch_actions, batch_qvals = [], [], []\n",
    "cur_rewards = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12: reward:  12.00, mean_100:  12.00, episodes: 1\n",
      "28: reward:  16.00, mean_100:  14.00, episodes: 2\n",
      "60: reward:  32.00, mean_100:  20.00, episodes: 3\n",
      "86: reward:  26.00, mean_100:  21.50, episodes: 4\n",
      "97: reward:  11.00, mean_100:  19.40, episodes: 5\n",
      "114: reward:  17.00, mean_100:  19.00, episodes: 6\n",
      "128: reward:  14.00, mean_100:  18.29, episodes: 7\n",
      "143: reward:  15.00, mean_100:  17.88, episodes: 8\n",
      "162: reward:  19.00, mean_100:  18.00, episodes: 9\n",
      "172: reward:  10.00, mean_100:  17.20, episodes: 10\n",
      "187: reward:  15.00, mean_100:  17.00, episodes: 11\n",
      "203: reward:  16.00, mean_100:  16.92, episodes: 12\n",
      "217: reward:  14.00, mean_100:  16.69, episodes: 13\n",
      "234: reward:  17.00, mean_100:  16.71, episodes: 14\n",
      "253: reward:  19.00, mean_100:  16.87, episodes: 15\n",
      "264: reward:  11.00, mean_100:  16.50, episodes: 16\n",
      "275: reward:  11.00, mean_100:  16.18, episodes: 17\n",
      "285: reward:  10.00, mean_100:  15.83, episodes: 18\n",
      "301: reward:  16.00, mean_100:  15.84, episodes: 19\n",
      "317: reward:  16.00, mean_100:  15.85, episodes: 20\n",
      "327: reward:  10.00, mean_100:  15.57, episodes: 21\n",
      "339: reward:  12.00, mean_100:  15.41, episodes: 22\n",
      "352: reward:  13.00, mean_100:  15.30, episodes: 23\n",
      "364: reward:  12.00, mean_100:  15.17, episodes: 24\n",
      "372: reward:   8.00, mean_100:  14.88, episodes: 25\n",
      "382: reward:  10.00, mean_100:  14.69, episodes: 26\n",
      "393: reward:  11.00, mean_100:  14.56, episodes: 27\n",
      "406: reward:  13.00, mean_100:  14.50, episodes: 28\n",
      "420: reward:  14.00, mean_100:  14.48, episodes: 29\n",
      "430: reward:  10.00, mean_100:  14.33, episodes: 30\n",
      "446: reward:  16.00, mean_100:  14.39, episodes: 31\n",
      "457: reward:  11.00, mean_100:  14.28, episodes: 32\n",
      "466: reward:   9.00, mean_100:  14.12, episodes: 33\n",
      "479: reward:  13.00, mean_100:  14.09, episodes: 34\n",
      "487: reward:   8.00, mean_100:  13.91, episodes: 35\n",
      "499: reward:  12.00, mean_100:  13.86, episodes: 36\n",
      "508: reward:   9.00, mean_100:  13.73, episodes: 37\n",
      "517: reward:   9.00, mean_100:  13.61, episodes: 38\n",
      "531: reward:  14.00, mean_100:  13.62, episodes: 39\n",
      "543: reward:  12.00, mean_100:  13.57, episodes: 40\n",
      "556: reward:  13.00, mean_100:  13.56, episodes: 41\n",
      "566: reward:  10.00, mean_100:  13.48, episodes: 42\n",
      "575: reward:   9.00, mean_100:  13.37, episodes: 43\n",
      "598: reward:  23.00, mean_100:  13.59, episodes: 44\n",
      "608: reward:  10.00, mean_100:  13.51, episodes: 45\n",
      "620: reward:  12.00, mean_100:  13.48, episodes: 46\n",
      "631: reward:  11.00, mean_100:  13.43, episodes: 47\n",
      "644: reward:  13.00, mean_100:  13.42, episodes: 48\n",
      "653: reward:   9.00, mean_100:  13.33, episodes: 49\n",
      "661: reward:   8.00, mean_100:  13.22, episodes: 50\n",
      "673: reward:  12.00, mean_100:  13.20, episodes: 51\n",
      "692: reward:  19.00, mean_100:  13.31, episodes: 52\n",
      "701: reward:   9.00, mean_100:  13.23, episodes: 53\n",
      "712: reward:  11.00, mean_100:  13.19, episodes: 54\n",
      "724: reward:  12.00, mean_100:  13.16, episodes: 55\n",
      "734: reward:  10.00, mean_100:  13.11, episodes: 56\n",
      "743: reward:   9.00, mean_100:  13.04, episodes: 57\n",
      "758: reward:  15.00, mean_100:  13.07, episodes: 58\n",
      "767: reward:   9.00, mean_100:  13.00, episodes: 59\n",
      "785: reward:  18.00, mean_100:  13.08, episodes: 60\n",
      "801: reward:  16.00, mean_100:  13.13, episodes: 61\n",
      "819: reward:  18.00, mean_100:  13.21, episodes: 62\n",
      "831: reward:  12.00, mean_100:  13.19, episodes: 63\n",
      "844: reward:  13.00, mean_100:  13.19, episodes: 64\n",
      "853: reward:   9.00, mean_100:  13.12, episodes: 65\n",
      "863: reward:  10.00, mean_100:  13.08, episodes: 66\n",
      "876: reward:  13.00, mean_100:  13.07, episodes: 67\n",
      "885: reward:   9.00, mean_100:  13.01, episodes: 68\n",
      "903: reward:  18.00, mean_100:  13.09, episodes: 69\n",
      "919: reward:  16.00, mean_100:  13.13, episodes: 70\n",
      "929: reward:  10.00, mean_100:  13.08, episodes: 71\n",
      "938: reward:   9.00, mean_100:  13.03, episodes: 72\n",
      "949: reward:  11.00, mean_100:  13.00, episodes: 73\n",
      "961: reward:  12.00, mean_100:  12.99, episodes: 74\n",
      "973: reward:  12.00, mean_100:  12.97, episodes: 75\n",
      "986: reward:  13.00, mean_100:  12.97, episodes: 76\n",
      "995: reward:   9.00, mean_100:  12.92, episodes: 77\n",
      "1016: reward:  21.00, mean_100:  13.03, episodes: 78\n",
      "1029: reward:  13.00, mean_100:  13.03, episodes: 79\n",
      "1042: reward:  13.00, mean_100:  13.03, episodes: 80\n",
      "1054: reward:  12.00, mean_100:  13.01, episodes: 81\n",
      "1063: reward:   9.00, mean_100:  12.96, episodes: 82\n",
      "1076: reward:  13.00, mean_100:  12.96, episodes: 83\n",
      "1099: reward:  23.00, mean_100:  13.08, episodes: 84\n",
      "1111: reward:  12.00, mean_100:  13.07, episodes: 85\n",
      "1139: reward:  28.00, mean_100:  13.24, episodes: 86\n",
      "1152: reward:  13.00, mean_100:  13.24, episodes: 87\n",
      "1173: reward:  21.00, mean_100:  13.33, episodes: 88\n",
      "1183: reward:  10.00, mean_100:  13.29, episodes: 89\n",
      "1196: reward:  13.00, mean_100:  13.29, episodes: 90\n",
      "1210: reward:  14.00, mean_100:  13.30, episodes: 91\n",
      "1223: reward:  13.00, mean_100:  13.29, episodes: 92\n",
      "1236: reward:  13.00, mean_100:  13.29, episodes: 93\n",
      "1261: reward:  25.00, mean_100:  13.41, episodes: 94\n",
      "1273: reward:  12.00, mean_100:  13.40, episodes: 95\n",
      "1287: reward:  14.00, mean_100:  13.41, episodes: 96\n",
      "1303: reward:  16.00, mean_100:  13.43, episodes: 97\n",
      "1318: reward:  15.00, mean_100:  13.45, episodes: 98\n",
      "1337: reward:  19.00, mean_100:  13.51, episodes: 99\n",
      "1352: reward:  15.00, mean_100:  13.52, episodes: 100\n",
      "1374: reward:  22.00, mean_100:  13.62, episodes: 101\n",
      "1395: reward:  21.00, mean_100:  13.67, episodes: 102\n",
      "1413: reward:  18.00, mean_100:  13.53, episodes: 103\n",
      "1426: reward:  13.00, mean_100:  13.40, episodes: 104\n",
      "1453: reward:  27.00, mean_100:  13.56, episodes: 105\n",
      "1499: reward:  46.00, mean_100:  13.85, episodes: 106\n",
      "1512: reward:  13.00, mean_100:  13.84, episodes: 107\n",
      "1523: reward:  11.00, mean_100:  13.80, episodes: 108\n",
      "1549: reward:  26.00, mean_100:  13.87, episodes: 109\n",
      "1569: reward:  20.00, mean_100:  13.97, episodes: 110\n",
      "1609: reward:  40.00, mean_100:  14.22, episodes: 111\n",
      "1660: reward:  51.00, mean_100:  14.57, episodes: 112\n",
      "1674: reward:  14.00, mean_100:  14.57, episodes: 113\n",
      "1733: reward:  59.00, mean_100:  14.99, episodes: 114\n",
      "1757: reward:  24.00, mean_100:  15.04, episodes: 115\n",
      "1792: reward:  35.00, mean_100:  15.28, episodes: 116\n",
      "1808: reward:  16.00, mean_100:  15.33, episodes: 117\n",
      "1875: reward:  67.00, mean_100:  15.90, episodes: 118\n",
      "1897: reward:  22.00, mean_100:  15.96, episodes: 119\n",
      "1928: reward:  31.00, mean_100:  16.11, episodes: 120\n",
      "1961: reward:  33.00, mean_100:  16.34, episodes: 121\n",
      "1998: reward:  37.00, mean_100:  16.59, episodes: 122\n",
      "2056: reward:  58.00, mean_100:  17.04, episodes: 123\n",
      "2083: reward:  27.00, mean_100:  17.19, episodes: 124\n",
      "2133: reward:  50.00, mean_100:  17.61, episodes: 125\n",
      "2164: reward:  31.00, mean_100:  17.82, episodes: 126\n",
      "2188: reward:  24.00, mean_100:  17.95, episodes: 127\n",
      "2214: reward:  26.00, mean_100:  18.08, episodes: 128\n",
      "2230: reward:  16.00, mean_100:  18.10, episodes: 129\n",
      "2251: reward:  21.00, mean_100:  18.21, episodes: 130\n",
      "2310: reward:  59.00, mean_100:  18.64, episodes: 131\n",
      "2323: reward:  13.00, mean_100:  18.66, episodes: 132\n",
      "2336: reward:  13.00, mean_100:  18.70, episodes: 133\n",
      "2360: reward:  24.00, mean_100:  18.81, episodes: 134\n",
      "2385: reward:  25.00, mean_100:  18.98, episodes: 135\n",
      "2410: reward:  25.00, mean_100:  19.11, episodes: 136\n",
      "2435: reward:  25.00, mean_100:  19.27, episodes: 137\n",
      "2471: reward:  36.00, mean_100:  19.54, episodes: 138\n",
      "2505: reward:  34.00, mean_100:  19.74, episodes: 139\n",
      "2546: reward:  41.00, mean_100:  20.03, episodes: 140\n",
      "2561: reward:  15.00, mean_100:  20.05, episodes: 141\n",
      "2581: reward:  20.00, mean_100:  20.15, episodes: 142\n",
      "2600: reward:  19.00, mean_100:  20.25, episodes: 143\n",
      "2639: reward:  39.00, mean_100:  20.41, episodes: 144\n",
      "2663: reward:  24.00, mean_100:  20.55, episodes: 145\n",
      "2679: reward:  16.00, mean_100:  20.59, episodes: 146\n",
      "2704: reward:  25.00, mean_100:  20.73, episodes: 147\n",
      "2755: reward:  51.00, mean_100:  21.11, episodes: 148\n",
      "2787: reward:  32.00, mean_100:  21.34, episodes: 149\n",
      "2813: reward:  26.00, mean_100:  21.52, episodes: 150\n",
      "2830: reward:  17.00, mean_100:  21.57, episodes: 151\n",
      "2859: reward:  29.00, mean_100:  21.67, episodes: 152\n",
      "2879: reward:  20.00, mean_100:  21.78, episodes: 153\n",
      "2910: reward:  31.00, mean_100:  21.98, episodes: 154\n",
      "2961: reward:  51.00, mean_100:  22.37, episodes: 155\n",
      "2982: reward:  21.00, mean_100:  22.48, episodes: 156\n",
      "3008: reward:  26.00, mean_100:  22.65, episodes: 157\n",
      "3050: reward:  42.00, mean_100:  22.92, episodes: 158\n",
      "3069: reward:  19.00, mean_100:  23.02, episodes: 159\n",
      "3100: reward:  31.00, mean_100:  23.15, episodes: 160\n",
      "3114: reward:  14.00, mean_100:  23.13, episodes: 161\n",
      "3148: reward:  34.00, mean_100:  23.29, episodes: 162\n",
      "3175: reward:  27.00, mean_100:  23.44, episodes: 163\n",
      "3195: reward:  20.00, mean_100:  23.51, episodes: 164\n",
      "3295: reward: 100.00, mean_100:  24.42, episodes: 165\n",
      "3321: reward:  26.00, mean_100:  24.58, episodes: 166\n",
      "3352: reward:  31.00, mean_100:  24.76, episodes: 167\n",
      "3374: reward:  22.00, mean_100:  24.89, episodes: 168\n",
      "3418: reward:  44.00, mean_100:  25.15, episodes: 169\n",
      "3462: reward:  44.00, mean_100:  25.43, episodes: 170\n",
      "3534: reward:  72.00, mean_100:  26.05, episodes: 171\n",
      "3570: reward:  36.00, mean_100:  26.32, episodes: 172\n",
      "3623: reward:  53.00, mean_100:  26.74, episodes: 173\n",
      "3675: reward:  52.00, mean_100:  27.14, episodes: 174\n",
      "3800: reward: 125.00, mean_100:  28.27, episodes: 175\n",
      "3838: reward:  38.00, mean_100:  28.52, episodes: 176\n",
      "3888: reward:  50.00, mean_100:  28.93, episodes: 177\n",
      "3998: reward: 110.00, mean_100:  29.82, episodes: 178\n",
      "4115: reward: 117.00, mean_100:  30.86, episodes: 179\n",
      "4156: reward:  41.00, mean_100:  31.14, episodes: 180\n",
      "4219: reward:  63.00, mean_100:  31.65, episodes: 181\n",
      "4376: reward: 157.00, mean_100:  33.13, episodes: 182\n",
      "4418: reward:  42.00, mean_100:  33.42, episodes: 183\n",
      "4474: reward:  56.00, mean_100:  33.75, episodes: 184\n",
      "4520: reward:  46.00, mean_100:  34.09, episodes: 185\n",
      "4588: reward:  68.00, mean_100:  34.49, episodes: 186\n",
      "4667: reward:  79.00, mean_100:  35.15, episodes: 187\n",
      "4773: reward: 106.00, mean_100:  36.00, episodes: 188\n",
      "4789: reward:  16.00, mean_100:  36.06, episodes: 189\n",
      "4818: reward:  29.00, mean_100:  36.22, episodes: 190\n",
      "4868: reward:  50.00, mean_100:  36.58, episodes: 191\n",
      "4928: reward:  60.00, mean_100:  37.05, episodes: 192\n",
      "4994: reward:  66.00, mean_100:  37.58, episodes: 193\n",
      "5044: reward:  50.00, mean_100:  37.83, episodes: 194\n",
      "5084: reward:  40.00, mean_100:  38.11, episodes: 195\n",
      "5134: reward:  50.00, mean_100:  38.47, episodes: 196\n",
      "5163: reward:  29.00, mean_100:  38.60, episodes: 197\n",
      "5201: reward:  38.00, mean_100:  38.83, episodes: 198\n",
      "5256: reward:  55.00, mean_100:  39.19, episodes: 199\n",
      "5299: reward:  43.00, mean_100:  39.47, episodes: 200\n",
      "5328: reward:  29.00, mean_100:  39.54, episodes: 201\n",
      "5354: reward:  26.00, mean_100:  39.59, episodes: 202\n",
      "5381: reward:  27.00, mean_100:  39.68, episodes: 203\n",
      "5409: reward:  28.00, mean_100:  39.83, episodes: 204\n",
      "5440: reward:  31.00, mean_100:  39.87, episodes: 205\n",
      "5463: reward:  23.00, mean_100:  39.64, episodes: 206\n",
      "5486: reward:  23.00, mean_100:  39.74, episodes: 207\n",
      "5520: reward:  34.00, mean_100:  39.97, episodes: 208\n",
      "5548: reward:  28.00, mean_100:  39.99, episodes: 209\n",
      "5585: reward:  37.00, mean_100:  40.16, episodes: 210\n",
      "5609: reward:  24.00, mean_100:  40.00, episodes: 211\n",
      "5634: reward:  25.00, mean_100:  39.74, episodes: 212\n",
      "5662: reward:  28.00, mean_100:  39.88, episodes: 213\n",
      "5678: reward:  16.00, mean_100:  39.45, episodes: 214\n",
      "5705: reward:  27.00, mean_100:  39.48, episodes: 215\n",
      "5739: reward:  34.00, mean_100:  39.47, episodes: 216\n",
      "5792: reward:  53.00, mean_100:  39.84, episodes: 217\n",
      "5833: reward:  41.00, mean_100:  39.58, episodes: 218\n",
      "5883: reward:  50.00, mean_100:  39.86, episodes: 219\n",
      "5931: reward:  48.00, mean_100:  40.03, episodes: 220\n",
      "5951: reward:  20.00, mean_100:  39.90, episodes: 221\n",
      "5996: reward:  45.00, mean_100:  39.98, episodes: 222\n",
      "6025: reward:  29.00, mean_100:  39.69, episodes: 223\n",
      "6070: reward:  45.00, mean_100:  39.87, episodes: 224\n",
      "6094: reward:  24.00, mean_100:  39.61, episodes: 225\n",
      "6123: reward:  29.00, mean_100:  39.59, episodes: 226\n",
      "6156: reward:  33.00, mean_100:  39.68, episodes: 227\n",
      "6190: reward:  34.00, mean_100:  39.76, episodes: 228\n",
      "6229: reward:  39.00, mean_100:  39.99, episodes: 229\n",
      "6336: reward: 107.00, mean_100:  40.85, episodes: 230\n",
      "6436: reward: 100.00, mean_100:  41.26, episodes: 231\n",
      "6537: reward: 101.00, mean_100:  42.14, episodes: 232\n",
      "6651: reward: 114.00, mean_100:  43.15, episodes: 233\n",
      "6751: reward: 100.00, mean_100:  43.91, episodes: 234\n",
      "6787: reward:  36.00, mean_100:  44.02, episodes: 235\n",
      "6897: reward: 110.00, mean_100:  44.87, episodes: 236\n",
      "7048: reward: 151.00, mean_100:  46.13, episodes: 237\n",
      "7169: reward: 121.00, mean_100:  46.98, episodes: 238\n",
      "7206: reward:  37.00, mean_100:  47.01, episodes: 239\n",
      "7244: reward:  38.00, mean_100:  46.98, episodes: 240\n",
      "7414: reward: 170.00, mean_100:  48.53, episodes: 241\n",
      "7614: reward: 200.00, mean_100:  50.33, episodes: 242\n",
      "7756: reward: 142.00, mean_100:  51.56, episodes: 243\n",
      "7946: reward: 190.00, mean_100:  53.07, episodes: 244\n",
      "8027: reward:  81.00, mean_100:  53.64, episodes: 245\n",
      "8227: reward: 200.00, mean_100:  55.48, episodes: 246\n",
      "8269: reward:  42.00, mean_100:  55.65, episodes: 247\n",
      "8376: reward: 107.00, mean_100:  56.21, episodes: 248\n",
      "8576: reward: 200.00, mean_100:  57.89, episodes: 249\n",
      "8776: reward: 200.00, mean_100:  59.63, episodes: 250\n",
      "8976: reward: 200.00, mean_100:  61.46, episodes: 251\n",
      "9176: reward: 200.00, mean_100:  63.17, episodes: 252\n",
      "9376: reward: 200.00, mean_100:  64.97, episodes: 253\n",
      "9468: reward:  92.00, mean_100:  65.58, episodes: 254\n",
      "9668: reward: 200.00, mean_100:  67.07, episodes: 255\n",
      "9868: reward: 200.00, mean_100:  68.86, episodes: 256\n",
      "9999: reward: 131.00, mean_100:  69.91, episodes: 257\n",
      "10176: reward: 177.00, mean_100:  71.26, episodes: 258\n",
      "10376: reward: 200.00, mean_100:  73.07, episodes: 259\n",
      "10558: reward: 182.00, mean_100:  74.58, episodes: 260\n",
      "10758: reward: 200.00, mean_100:  76.44, episodes: 261\n",
      "10872: reward: 114.00, mean_100:  77.24, episodes: 262\n",
      "11072: reward: 200.00, mean_100:  78.97, episodes: 263\n",
      "11156: reward:  84.00, mean_100:  79.61, episodes: 264\n",
      "11356: reward: 200.00, mean_100:  80.61, episodes: 265\n",
      "11556: reward: 200.00, mean_100:  82.35, episodes: 266\n",
      "11667: reward: 111.00, mean_100:  83.15, episodes: 267\n",
      "11867: reward: 200.00, mean_100:  84.93, episodes: 268\n",
      "12067: reward: 200.00, mean_100:  86.49, episodes: 269\n",
      "12209: reward: 142.00, mean_100:  87.47, episodes: 270\n",
      "12409: reward: 200.00, mean_100:  88.75, episodes: 271\n",
      "12543: reward: 134.00, mean_100:  89.73, episodes: 272\n",
      "12743: reward: 200.00, mean_100:  91.20, episodes: 273\n",
      "12938: reward: 195.00, mean_100:  92.63, episodes: 274\n",
      "13096: reward: 158.00, mean_100:  92.96, episodes: 275\n",
      "13272: reward: 176.00, mean_100:  94.34, episodes: 276\n",
      "13472: reward: 200.00, mean_100:  95.84, episodes: 277\n",
      "13672: reward: 200.00, mean_100:  96.74, episodes: 278\n",
      "13774: reward: 102.00, mean_100:  96.59, episodes: 279\n",
      "13974: reward: 200.00, mean_100:  98.18, episodes: 280\n",
      "14115: reward: 141.00, mean_100:  98.96, episodes: 281\n",
      "14315: reward: 200.00, mean_100:  99.39, episodes: 282\n",
      "14514: reward: 199.00, mean_100: 100.96, episodes: 283\n",
      "14714: reward: 200.00, mean_100: 102.40, episodes: 284\n",
      "14914: reward: 200.00, mean_100: 103.94, episodes: 285\n",
      "14998: reward:  84.00, mean_100: 104.10, episodes: 286\n",
      "15198: reward: 200.00, mean_100: 105.31, episodes: 287\n",
      "15398: reward: 200.00, mean_100: 106.25, episodes: 288\n",
      "15598: reward: 200.00, mean_100: 108.09, episodes: 289\n",
      "15798: reward: 200.00, mean_100: 109.80, episodes: 290\n",
      "15998: reward: 200.00, mean_100: 111.30, episodes: 291\n",
      "16198: reward: 200.00, mean_100: 112.70, episodes: 292\n",
      "16398: reward: 200.00, mean_100: 114.04, episodes: 293\n",
      "16533: reward: 135.00, mean_100: 114.89, episodes: 294\n",
      "16733: reward: 200.00, mean_100: 116.49, episodes: 295\n",
      "16933: reward: 200.00, mean_100: 117.99, episodes: 296\n",
      "17110: reward: 177.00, mean_100: 119.47, episodes: 297\n",
      "17310: reward: 200.00, mean_100: 121.09, episodes: 298\n",
      "17498: reward: 188.00, mean_100: 122.42, episodes: 299\n",
      "17698: reward: 200.00, mean_100: 123.99, episodes: 300\n",
      "17898: reward: 200.00, mean_100: 125.70, episodes: 301\n",
      "18098: reward: 200.00, mean_100: 127.44, episodes: 302\n",
      "18157: reward:  59.00, mean_100: 127.76, episodes: 303\n",
      "18357: reward: 200.00, mean_100: 129.48, episodes: 304\n",
      "18531: reward: 174.00, mean_100: 130.91, episodes: 305\n",
      "18731: reward: 200.00, mean_100: 132.68, episodes: 306\n",
      "18931: reward: 200.00, mean_100: 134.45, episodes: 307\n",
      "18998: reward:  67.00, mean_100: 134.78, episodes: 308\n",
      "19167: reward: 169.00, mean_100: 136.19, episodes: 309\n",
      "19354: reward: 187.00, mean_100: 137.69, episodes: 310\n",
      "19554: reward: 200.00, mean_100: 139.45, episodes: 311\n",
      "19754: reward: 200.00, mean_100: 141.20, episodes: 312\n",
      "19828: reward:  74.00, mean_100: 141.66, episodes: 313\n",
      "20028: reward: 200.00, mean_100: 143.50, episodes: 314\n",
      "20228: reward: 200.00, mean_100: 145.23, episodes: 315\n",
      "20428: reward: 200.00, mean_100: 146.89, episodes: 316\n",
      "20621: reward: 193.00, mean_100: 148.29, episodes: 317\n",
      "20703: reward:  82.00, mean_100: 148.70, episodes: 318\n",
      "20903: reward: 200.00, mean_100: 150.20, episodes: 319\n",
      "21103: reward: 200.00, mean_100: 151.72, episodes: 320\n",
      "21303: reward: 200.00, mean_100: 153.52, episodes: 321\n",
      "21503: reward: 200.00, mean_100: 155.07, episodes: 322\n",
      "21694: reward: 191.00, mean_100: 156.69, episodes: 323\n",
      "21835: reward: 141.00, mean_100: 157.65, episodes: 324\n",
      "21861: reward:  26.00, mean_100: 157.67, episodes: 325\n",
      "22010: reward: 149.00, mean_100: 158.87, episodes: 326\n",
      "22195: reward: 185.00, mean_100: 160.39, episodes: 327\n",
      "22349: reward: 154.00, mean_100: 161.59, episodes: 328\n",
      "22375: reward:  26.00, mean_100: 161.46, episodes: 329\n",
      "22489: reward: 114.00, mean_100: 161.53, episodes: 330\n",
      "22515: reward:  26.00, mean_100: 160.79, episodes: 331\n",
      "22652: reward: 137.00, mean_100: 161.15, episodes: 332\n",
      "22759: reward: 107.00, mean_100: 161.08, episodes: 333\n",
      "22777: reward:  18.00, mean_100: 160.26, episodes: 334\n",
      "22908: reward: 131.00, mean_100: 161.21, episodes: 335\n",
      "22926: reward:  18.00, mean_100: 160.29, episodes: 336\n",
      "22975: reward:  49.00, mean_100: 159.27, episodes: 337\n",
      "23072: reward:  97.00, mean_100: 159.03, episodes: 338\n",
      "23119: reward:  47.00, mean_100: 159.13, episodes: 339\n",
      "23246: reward: 127.00, mean_100: 160.02, episodes: 340\n",
      "23264: reward:  18.00, mean_100: 158.50, episodes: 341\n",
      "23327: reward:  63.00, mean_100: 157.13, episodes: 342\n",
      "23435: reward: 108.00, mean_100: 156.79, episodes: 343\n",
      "23503: reward:  68.00, mean_100: 155.57, episodes: 344\n",
      "23607: reward: 104.00, mean_100: 155.80, episodes: 345\n",
      "23655: reward:  48.00, mean_100: 154.28, episodes: 346\n",
      "23764: reward: 109.00, mean_100: 154.95, episodes: 347\n",
      "23872: reward: 108.00, mean_100: 154.96, episodes: 348\n",
      "23898: reward:  26.00, mean_100: 153.22, episodes: 349\n",
      "24011: reward: 113.00, mean_100: 152.35, episodes: 350\n",
      "24110: reward:  99.00, mean_100: 151.34, episodes: 351\n",
      "24226: reward: 116.00, mean_100: 150.50, episodes: 352\n",
      "24339: reward: 113.00, mean_100: 149.63, episodes: 353\n",
      "24450: reward: 111.00, mean_100: 149.82, episodes: 354\n",
      "24556: reward: 106.00, mean_100: 148.88, episodes: 355\n",
      "24684: reward: 128.00, mean_100: 148.16, episodes: 356\n",
      "24821: reward: 137.00, mean_100: 148.22, episodes: 357\n",
      "24965: reward: 144.00, mean_100: 147.89, episodes: 358\n",
      "25074: reward: 109.00, mean_100: 146.98, episodes: 359\n",
      "25170: reward:  96.00, mean_100: 146.12, episodes: 360\n",
      "25286: reward: 116.00, mean_100: 145.28, episodes: 361\n",
      "25412: reward: 126.00, mean_100: 145.40, episodes: 362\n",
      "25541: reward: 129.00, mean_100: 144.69, episodes: 363\n",
      "25673: reward: 132.00, mean_100: 145.17, episodes: 364\n",
      "25809: reward: 136.00, mean_100: 144.53, episodes: 365\n",
      "25931: reward: 122.00, mean_100: 143.75, episodes: 366\n",
      "26060: reward: 129.00, mean_100: 143.93, episodes: 367\n",
      "26184: reward: 124.00, mean_100: 143.17, episodes: 368\n",
      "26320: reward: 136.00, mean_100: 142.53, episodes: 369\n",
      "26444: reward: 124.00, mean_100: 142.35, episodes: 370\n",
      "26593: reward: 149.00, mean_100: 141.84, episodes: 371\n",
      "26731: reward: 138.00, mean_100: 141.88, episodes: 372\n",
      "26888: reward: 157.00, mean_100: 141.45, episodes: 373\n",
      "27047: reward: 159.00, mean_100: 141.09, episodes: 374\n",
      "27186: reward: 139.00, mean_100: 140.90, episodes: 375\n",
      "27331: reward: 145.00, mean_100: 140.59, episodes: 376\n",
      "27461: reward: 130.00, mean_100: 139.89, episodes: 377\n",
      "27627: reward: 166.00, mean_100: 139.55, episodes: 378\n",
      "27813: reward: 186.00, mean_100: 140.39, episodes: 379\n",
      "27969: reward: 156.00, mean_100: 139.95, episodes: 380\n",
      "28167: reward: 198.00, mean_100: 140.52, episodes: 381\n",
      "28354: reward: 187.00, mean_100: 140.39, episodes: 382\n",
      "28513: reward: 159.00, mean_100: 139.99, episodes: 383\n",
      "28713: reward: 200.00, mean_100: 139.99, episodes: 384\n",
      "28897: reward: 184.00, mean_100: 139.83, episodes: 385\n",
      "29074: reward: 177.00, mean_100: 140.76, episodes: 386\n",
      "29274: reward: 200.00, mean_100: 140.76, episodes: 387\n",
      "29474: reward: 200.00, mean_100: 140.76, episodes: 388\n",
      "29667: reward: 193.00, mean_100: 140.69, episodes: 389\n",
      "29867: reward: 200.00, mean_100: 140.69, episodes: 390\n",
      "30047: reward: 180.00, mean_100: 140.49, episodes: 391\n",
      "30240: reward: 193.00, mean_100: 140.42, episodes: 392\n",
      "30440: reward: 200.00, mean_100: 140.42, episodes: 393\n",
      "30640: reward: 200.00, mean_100: 141.07, episodes: 394\n",
      "30840: reward: 200.00, mean_100: 141.07, episodes: 395\n",
      "31040: reward: 200.00, mean_100: 141.07, episodes: 396\n",
      "31240: reward: 200.00, mean_100: 141.30, episodes: 397\n",
      "31440: reward: 200.00, mean_100: 141.30, episodes: 398\n",
      "31640: reward: 200.00, mean_100: 141.42, episodes: 399\n",
      "31840: reward: 200.00, mean_100: 141.42, episodes: 400\n",
      "32040: reward: 200.00, mean_100: 141.42, episodes: 401\n",
      "32240: reward: 200.00, mean_100: 141.42, episodes: 402\n",
      "32440: reward: 200.00, mean_100: 142.83, episodes: 403\n",
      "32640: reward: 200.00, mean_100: 142.83, episodes: 404\n",
      "32840: reward: 200.00, mean_100: 143.09, episodes: 405\n",
      "33040: reward: 200.00, mean_100: 143.09, episodes: 406\n",
      "33240: reward: 200.00, mean_100: 143.09, episodes: 407\n",
      "33440: reward: 200.00, mean_100: 144.42, episodes: 408\n",
      "33640: reward: 200.00, mean_100: 144.73, episodes: 409\n",
      "33840: reward: 200.00, mean_100: 144.86, episodes: 410\n",
      "34040: reward: 200.00, mean_100: 144.86, episodes: 411\n",
      "34240: reward: 200.00, mean_100: 144.86, episodes: 412\n",
      "34440: reward: 200.00, mean_100: 146.12, episodes: 413\n",
      "34640: reward: 200.00, mean_100: 146.12, episodes: 414\n",
      "34840: reward: 200.00, mean_100: 146.12, episodes: 415\n",
      "35040: reward: 200.00, mean_100: 146.12, episodes: 416\n",
      "35240: reward: 200.00, mean_100: 146.19, episodes: 417\n",
      "35440: reward: 200.00, mean_100: 147.37, episodes: 418\n",
      "35640: reward: 200.00, mean_100: 147.37, episodes: 419\n",
      "35840: reward: 200.00, mean_100: 147.37, episodes: 420\n",
      "36040: reward: 200.00, mean_100: 147.37, episodes: 421\n",
      "36240: reward: 200.00, mean_100: 147.37, episodes: 422\n",
      "36440: reward: 200.00, mean_100: 147.46, episodes: 423\n",
      "36640: reward: 200.00, mean_100: 148.05, episodes: 424\n",
      "36840: reward: 200.00, mean_100: 149.79, episodes: 425\n",
      "37040: reward: 200.00, mean_100: 150.30, episodes: 426\n",
      "37240: reward: 200.00, mean_100: 150.45, episodes: 427\n",
      "37440: reward: 200.00, mean_100: 150.91, episodes: 428\n",
      "37640: reward: 200.00, mean_100: 152.65, episodes: 429\n",
      "37840: reward: 200.00, mean_100: 153.51, episodes: 430\n",
      "38040: reward: 200.00, mean_100: 155.25, episodes: 431\n",
      "38240: reward: 200.00, mean_100: 155.88, episodes: 432\n",
      "38440: reward: 200.00, mean_100: 156.81, episodes: 433\n",
      "38640: reward: 200.00, mean_100: 158.63, episodes: 434\n",
      "38840: reward: 200.00, mean_100: 159.32, episodes: 435\n",
      "39040: reward: 200.00, mean_100: 161.14, episodes: 436\n",
      "39240: reward: 200.00, mean_100: 162.65, episodes: 437\n",
      "39440: reward: 200.00, mean_100: 163.68, episodes: 438\n",
      "39640: reward: 200.00, mean_100: 165.21, episodes: 439\n",
      "39840: reward: 200.00, mean_100: 165.94, episodes: 440\n",
      "40040: reward: 200.00, mean_100: 167.76, episodes: 441\n",
      "40240: reward: 200.00, mean_100: 169.13, episodes: 442\n",
      "40440: reward: 200.00, mean_100: 170.05, episodes: 443\n",
      "40640: reward: 200.00, mean_100: 171.37, episodes: 444\n",
      "40840: reward: 200.00, mean_100: 172.33, episodes: 445\n",
      "41040: reward: 200.00, mean_100: 173.85, episodes: 446\n",
      "41240: reward: 200.00, mean_100: 174.76, episodes: 447\n",
      "41440: reward: 200.00, mean_100: 175.68, episodes: 448\n",
      "41640: reward: 200.00, mean_100: 177.42, episodes: 449\n",
      "41840: reward: 200.00, mean_100: 178.29, episodes: 450\n",
      "42040: reward: 200.00, mean_100: 179.30, episodes: 451\n",
      "42240: reward: 200.00, mean_100: 180.14, episodes: 452\n",
      "42440: reward: 200.00, mean_100: 181.01, episodes: 453\n",
      "42640: reward: 200.00, mean_100: 181.90, episodes: 454\n",
      "42840: reward: 200.00, mean_100: 182.84, episodes: 455\n",
      "43040: reward: 200.00, mean_100: 183.56, episodes: 456\n",
      "43240: reward: 200.00, mean_100: 184.19, episodes: 457\n",
      "43440: reward: 200.00, mean_100: 184.75, episodes: 458\n",
      "43640: reward: 200.00, mean_100: 185.66, episodes: 459\n",
      "43840: reward: 200.00, mean_100: 186.70, episodes: 460\n",
      "44040: reward: 200.00, mean_100: 187.54, episodes: 461\n",
      "44240: reward: 200.00, mean_100: 188.28, episodes: 462\n",
      "44440: reward: 200.00, mean_100: 188.99, episodes: 463\n",
      "44640: reward: 200.00, mean_100: 189.67, episodes: 464\n",
      "44840: reward: 200.00, mean_100: 190.31, episodes: 465\n",
      "45040: reward: 200.00, mean_100: 191.09, episodes: 466\n",
      "45240: reward: 200.00, mean_100: 191.80, episodes: 467\n",
      "45440: reward: 200.00, mean_100: 192.56, episodes: 468\n",
      "45640: reward: 200.00, mean_100: 193.20, episodes: 469\n",
      "45840: reward: 200.00, mean_100: 193.96, episodes: 470\n",
      "46040: reward: 200.00, mean_100: 194.47, episodes: 471\n",
      "46240: reward: 200.00, mean_100: 195.09, episodes: 472\n",
      "Solved in 46240 steps and 472 episodes!\n"
     ]
    }
   ],
   "source": [
    "for step_idx, exp in enumerate(exp_source):\n",
    "    batch_states.append(exp.state)\n",
    "    batch_actions.append(int(exp.action))\n",
    "    cur_rewards.append(exp.reward)\n",
    "    \n",
    "    if exp.last_state is None:\n",
    "        batch_qvals.extend(calc_qvals(cur_rewards))\n",
    "        cur_rewards.clear()\n",
    "        batch_episodes += 1\n",
    "    \n",
    "    # handle new rewards\n",
    "    new_rewards = exp_source.pop_rewards_steps()\n",
    "    if new_rewards:\n",
    "        done_episodes += 1\n",
    "        reward = new_rewards[0][0]\n",
    "        total_rewards.append(reward)\n",
    "        mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "        print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\"\n",
    "              % (step_idx, reward, mean_rewards, done_episodes))\n",
    "        writer.add_scalar(\"reward\", reward, step_idx)\n",
    "        writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "        writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "        if mean_rewards > 195:\n",
    "            print(\"Solved in %d steps and %d episodes!\"\n",
    "                  %(step_idx, done_episodes))\n",
    "            break\n",
    "    \n",
    "    # when enough episodes have passed since the last training step,\n",
    "    # we perform optimiation on the gathered exmaples\n",
    "    # we convert states, actions, and Q-value into the appropriate PyTorch form\n",
    "    if batch_episodes < EPISODE_TO_TRAIN:\n",
    "        continue\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    states_v = torch.FloatTensor(batch_states)\n",
    "    batch_actions_t = torch.LongTensor(batch_actions)\n",
    "    batch_qvals_v = torch.FloatTensor(batch_qvals)\n",
    "    \n",
    "    # we calculate the loss from the steps.\n",
    "    logits_v = net(states_v)        # network calculates states into logits\n",
    "    log_prob_v = F.log_softmax(logits_v, dim=1)     # calculate the logartithm + softmax of them\n",
    "    log_prov_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]      # select log probabilities from the action taken and scale them with Q-values\n",
    "    loss_v = -log_prov_actions_v.mean()         # average those scaled values and do negation to obtain the loss to minimize.\n",
    "    \n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    batch_episodes = 0\n",
    "    batch_states.clear()\n",
    "    batch_actions.clear()\n",
    "    batch_qvals.clear()\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
