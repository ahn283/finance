{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src =\"./img/cover.jpg\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decluttering the jargon linked to reinforcement learnming\n",
    "\n",
    "## Uncertainty\n",
    "- This means the problem under consideration involved random variavblee that evovle over time. The technical term for this is <i>stochastic processes</i>.\n",
    "\n",
    "## Optimal decision\n",
    "- This refers to the technical term Optimization. This means there is a well-defined quantity to be maximized (the 'goal').\n",
    "\n",
    "## Sequential\n",
    "- This refers to the fact that as we move forward in time, the relevant random variables' value evolve, and the optimal decisions have to be adjusted to the changing circustances.\n",
    "\n",
    "## Control\n",
    "- Putting together the three notions, these problems have the common feature that one needs to overpower the uncertainty by persistent steering towards the goal. This brings us to the term control.\n",
    "- These problems are often characterized by the technical term <i>stochastic control</i>.\n",
    "\n",
    "<img src='./img/many_faces_rl.png' width=400>\n",
    "\n",
    "## Reonforcement learning vs other machine learning alrogithms\n",
    "\n",
    "- ML generally refers to the broad set of techniques to infer mathematicall models/functions by acquiring (learning) knowledge of patterns and properties in the presented data.\n",
    "- RL does fit this definition. However, unlike the other branches of ML, RL is a lof more ambitious - it not only learns the patterns and properties of the presented data, it also learns about the appropriate behaviors to be exercised as as to drive towards the optimization objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Markov decition process (MDP) framework\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "- The framework of Markov Decision Process indicates the Agent and the Environment interact in a time-sequenced loop.\n",
    "\n",
    "- The term <i>Agent</i> refers to an algorithm and the term <i>Environment</i> refers to an abstract entity that serves up uncertain outcomes to the Agent.\n",
    "\n",
    "<img src='./img/MDP-model.png' width=400>\n",
    "\n",
    "- At each time step $t$, the Agent observes an abstract piece of information (which is called State) and a numerical quantity that we call Reward.\n",
    "\n",
    "- Upon observing a State and a Reward at time step $t$, the Agent responds by taking an Action that responds to observations (State and Reward) served by the Environment with time step $t+1$.\n",
    "\n",
    "\n",
    "- The State is assumed to have the Markov Property.\n",
    "\n",
    "    - The next State/Reward depends only on Current State (for a given Action).\n",
    "\n",
    "    - The current State encapsulates all relevant information from the history of the interaction between the Agent and the Environment\n",
    "    \n",
    "    - The current State is a sufficient statistic of the future (for a given Action) \n",
    "\n",
    "- The goal of the Agentat at any point in time is to maximize the Expected Sum of all future Rewards by controlling the Action as a function of the observed State. This function from a State to Action at any time step is known as the Policy function. So the agent's job is exercise control by determining the Optimal Policy function.\n",
    "\n",
    "\n",
    "## Mathematical notation\n",
    "\n",
    "- We denote time steps at $t=1,2,3,...$. \n",
    "\n",
    "- Markov State at time $t$ is denoted as $S_t\\in \\mathcal{S}$, where $\\mathcal{S}$ is referred to as the *State Space*.\n",
    "\n",
    "- Action at time $t$ is denoted as $A_t\\in\\mathcal{A}$, where $\\mathcal{A}$ is referred to as the *Action Space*.\n",
    "\n",
    "- Reward at time $t$ is denoted as $R_t\\in\\mathcal{D}$, where $\\mathcal{D}$ is a countable subset of \\mathbb{R}$ (representing the numerical feedback served by the Environment, along with the State, at each time step $t$).\n",
    "\n",
    "- We present the transition probabilites from one time step to the next:\n",
    "\n",
    "$$ p(r,s^\\prime|s,a)=\\mathbb{P}[(R_{t+1}=r, S_{t+1}=s^\\prime)|S_t,=s A_t=a]$$\n",
    "\n",
    "- $\\gamma\\in [0, 1]$ is known as the discounted factor used to discout Rewards when accumulating Rewards:\n",
    "\n",
    "$$\\text{Return }G_t=R_{t+1}+\\gamma\\cdot R_{t+2}+\\gamma^2\\cdot R_{t+3}+\\cdots$$\n",
    "\n",
    "- The goal is to find a $\\text{Policy } \\pi: \\mathcal{S}\\rightarrow \\mathcal{A}$ maximizes $\\mathbb{E}[G_t|S_t=s]$ for all $\n",
    "s\\in\\mathcal{S}$.\n",
    "\n",
    "- Policies are stochastic, i.e, functions that take as input a state and output a probability distribution of actions (rather than a single action).\n",
    "\n",
    "- The intuition is that the two entities *Agent* and *Environment* interact in a time sequenced loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
