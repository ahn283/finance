{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, gym.spaces\n",
    "from collections import namedtuple\n",
    "import numpy as np \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = gym.make(\"FrozenLake-v0\")\n",
    "e.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "e.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space,\n",
    "                          gym.spaces.Discrete)\n",
    "        shape = (env.observation_space.n, )\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            0.0, 1.0, shape, dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "    \n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step: step.action, example.steps))\n",
    "    \n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v0\"))\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "writer = SummaryWriter(comment=\"-frozenlake-naive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "1: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "2: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "3: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "4: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "5: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "6: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "7: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "8: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "9: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "10: loss=0.059, reward_mean=0.0, reward_bound=0.0\n",
      "11: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "12: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "13: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "14: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "15: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "16: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "17: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "18: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "19: loss=0.041, reward_mean=0.0, reward_bound=0.0\n",
      "20: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "21: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "22: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "23: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "24: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "25: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "26: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "27: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "28: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "29: loss=0.068, reward_mean=0.0, reward_bound=0.0\n",
      "30: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "31: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "32: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "33: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "34: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "35: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "36: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "37: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "38: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "39: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "40: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "41: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "42: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "43: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "44: loss=0.055, reward_mean=0.0, reward_bound=0.0\n",
      "45: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "46: loss=0.063, reward_mean=0.0, reward_bound=0.0\n",
      "47: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "48: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "49: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "50: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "51: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "52: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "53: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "54: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "55: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "56: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "57: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "58: loss=0.044, reward_mean=0.0, reward_bound=0.0\n",
      "59: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "60: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "61: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "62: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "63: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "64: loss=0.049, reward_mean=0.0, reward_bound=0.0\n",
      "65: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "66: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "67: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "68: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "69: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "70: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "71: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "72: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "73: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "74: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "75: loss=0.044, reward_mean=0.0, reward_bound=0.0\n",
      "76: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "77: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "78: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "79: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "80: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "81: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "82: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "83: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "84: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "85: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "86: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "87: loss=0.032, reward_mean=0.0, reward_bound=0.0\n",
      "88: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "89: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "90: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "91: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "92: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "93: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "94: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "95: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "96: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "97: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "98: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "99: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "100: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "101: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "102: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "103: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "104: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "105: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "106: loss=0.036, reward_mean=0.0, reward_bound=0.0\n",
      "107: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "108: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "109: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "110: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "111: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "112: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "113: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "114: loss=0.052, reward_mean=0.0, reward_bound=0.0\n",
      "115: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "116: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "117: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "118: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "119: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "120: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "121: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "122: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "123: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "124: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "125: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "126: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "127: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "128: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "129: loss=0.001, reward_mean=0.0, reward_bound=0.0\n",
      "130: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "131: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "132: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "133: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "134: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "135: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "136: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "137: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "138: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "139: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "140: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "141: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "142: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "143: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "144: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "145: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "146: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "147: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "148: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "149: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "150: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "151: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "152: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "153: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "154: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "155: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "156: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "157: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "158: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "159: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "160: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "161: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "162: loss=0.000, reward_mean=0.0, reward_bound=0.0\n",
      "163: loss=0.040, reward_mean=0.0, reward_bound=0.0\n",
      "164: loss=0.000, reward_mean=0.0, reward_bound=0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_no, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(iterate_batches(env, net, BATCH_SIZE)):\n\u001b[0;32m      2\u001b[0m     obs_v, acts_v, reward_b, reward_m \u001b[38;5;241m=\u001b[39m filter_batch(batch, PERCENTILE)\n\u001b[0;32m      3\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[1;32mIn[11], line 9\u001b[0m, in \u001b[0;36miterate_batches\u001b[1;34m(env, net, batch_size)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     obs_v \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([obs])\n\u001b[1;32m----> 9\u001b[0m     act_probs_v \u001b[38;5;241m=\u001b[39m sm(\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_v\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     10\u001b[0m     act_probs \u001b[38;5;241m=\u001b[39m act_probs_v\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     11\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mlen\u001b[39m(act_probs), p\u001b[38;5;241m=\u001b[39mact_probs)\n",
      "File \u001b[1;32mc:\\Users\\woojin\\anaconda3\\envs\\reinforce\\lib\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "Cell \u001b[1;32mIn[9], line 11\u001b[0m, in \u001b[0;36mNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\woojin\\anaconda3\\envs\\reinforce\\lib\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\woojin\\anaconda3\\envs\\reinforce\\lib\\site-packages\\torch\\nn\\modules\\container.py:117\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 117\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\woojin\\anaconda3\\envs\\reinforce\\lib\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32mc:\\Users\\woojin\\anaconda3\\envs\\reinforce\\lib\\site-packages\\torch\\nn\\modules\\linear.py:93\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\woojin\\anaconda3\\envs\\reinforce\\lib\\site-packages\\torch\\nn\\functional.py:1690\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, tens_ops, \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[0;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1689\u001b[0m     \u001b[38;5;66;03m# fused op is marginally faster\u001b[39;00m\n\u001b[1;32m-> 1690\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1692\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mmatmul(weight\u001b[38;5;241m.\u001b[39mt())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    obs_v, acts_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, acts_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\"\n",
    "          % (iter_no, loss_v.item(), reward_m, reward_b))\n",
    "    writer.add_scalar(\"loss\", loss_v.item(), iter_no)\n",
    "    writer.add_scalar(\"reward_bound\", reward_b, iter_no)\n",
    "    writer.add_scalar(\"reward_mean\", reward_m, iter_no)\n",
    "    if reward_m > 0.8:\n",
    "        print(\"Solved!\")\n",
    "        break\n",
    "\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
