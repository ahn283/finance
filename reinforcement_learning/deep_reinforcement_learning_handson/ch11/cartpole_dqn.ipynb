{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_STOP = 0.02\n",
    "EPLSION_STEP = 5000\n",
    "\n",
    "REPLAY_BUFFER = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions) -> None:\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_target(net, local_reward, next_state):\n",
    "    if next_state is None:\n",
    "        return local_reward\n",
    "    state_v = torch.tensor([next_state], dtype=torch.float32)\n",
    "    next_q_v = net(state_v)\n",
    "    best_q = next_q_v.max(dim=1)[0].item()\n",
    "    return local_reward + GAMMA * best_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "writer = SummaryWriter(comment=\"-cartpole=dqn\")\n",
    "\n",
    "net = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ptan.actions.EpsilonGreedyActionSelector(epsilon=EPSILON_START)\n",
    "agent = ptan.agent.DQNAgent(net, selector, preprocessor=ptan.agent.float32_preprocessor)\n",
    "exp_source = ptan.experience.ExperienceSourceFirstLast(env, agent, gamma=GAMMA)\n",
    "replay_buffer = ptan.experience.ExperienceReplayBuffer(exp_source, REPLAY_BUFFER)\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr=LEARNING_RATE)\n",
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards = []\n",
    "step_idx = 0\n",
    "done_episodes = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: reward:  15.00, mean_100:  15.00, epsilon: 1.00, episodes: 1\n",
      "39: reward:  23.00, mean_100:  19.00, epsilon: 0.99, episodes: 2\n",
      "52: reward:  13.00, mean_100:  17.00, epsilon: 0.99, episodes: 3\n",
      "65: reward:  13.00, mean_100:  16.00, epsilon: 0.99, episodes: 4\n",
      "77: reward:  12.00, mean_100:  15.20, epsilon: 0.98, episodes: 5\n",
      "89: reward:  12.00, mean_100:  14.67, epsilon: 0.98, episodes: 6\n",
      "105: reward:  16.00, mean_100:  14.86, epsilon: 0.98, episodes: 7\n",
      "122: reward:  17.00, mean_100:  15.12, epsilon: 0.98, episodes: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\woojin\\AppData\\Local\\Temp\\ipykernel_16732\\2317137858.py:4: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  state_v = torch.tensor([next_state], dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156: reward:  34.00, mean_100:  17.22, epsilon: 0.97, episodes: 9\n",
      "170: reward:  14.00, mean_100:  16.90, epsilon: 0.97, episodes: 10\n",
      "197: reward:  27.00, mean_100:  17.82, epsilon: 0.96, episodes: 11\n",
      "218: reward:  21.00, mean_100:  18.08, epsilon: 0.96, episodes: 12\n",
      "249: reward:  31.00, mean_100:  19.08, epsilon: 0.95, episodes: 13\n",
      "275: reward:  26.00, mean_100:  19.57, epsilon: 0.94, episodes: 14\n",
      "301: reward:  26.00, mean_100:  20.00, epsilon: 0.94, episodes: 15\n",
      "317: reward:  16.00, mean_100:  19.75, epsilon: 0.94, episodes: 16\n",
      "333: reward:  16.00, mean_100:  19.53, epsilon: 0.93, episodes: 17\n",
      "367: reward:  34.00, mean_100:  20.33, epsilon: 0.93, episodes: 18\n",
      "390: reward:  23.00, mean_100:  20.47, epsilon: 0.92, episodes: 19\n",
      "406: reward:  16.00, mean_100:  20.25, epsilon: 0.92, episodes: 20\n",
      "419: reward:  13.00, mean_100:  19.90, epsilon: 0.92, episodes: 21\n",
      "433: reward:  14.00, mean_100:  19.64, epsilon: 0.91, episodes: 22\n",
      "469: reward:  36.00, mean_100:  20.35, epsilon: 0.91, episodes: 23\n",
      "485: reward:  16.00, mean_100:  20.17, epsilon: 0.90, episodes: 24\n",
      "520: reward:  35.00, mean_100:  20.76, epsilon: 0.90, episodes: 25\n",
      "548: reward:  28.00, mean_100:  21.04, epsilon: 0.89, episodes: 26\n",
      "569: reward:  21.00, mean_100:  21.04, epsilon: 0.89, episodes: 27\n",
      "644: reward:  75.00, mean_100:  22.96, epsilon: 0.87, episodes: 28\n",
      "671: reward:  27.00, mean_100:  23.10, epsilon: 0.87, episodes: 29\n",
      "710: reward:  39.00, mean_100:  23.63, epsilon: 0.86, episodes: 30\n",
      "742: reward:  32.00, mean_100:  23.90, epsilon: 0.85, episodes: 31\n",
      "755: reward:  13.00, mean_100:  23.56, epsilon: 0.85, episodes: 32\n",
      "801: reward:  46.00, mean_100:  24.24, epsilon: 0.84, episodes: 33\n",
      "870: reward:  69.00, mean_100:  25.56, epsilon: 0.83, episodes: 34\n",
      "880: reward:  10.00, mean_100:  25.11, epsilon: 0.82, episodes: 35\n",
      "908: reward:  28.00, mean_100:  25.19, epsilon: 0.82, episodes: 36\n",
      "932: reward:  24.00, mean_100:  25.16, epsilon: 0.81, episodes: 37\n",
      "943: reward:  11.00, mean_100:  24.79, epsilon: 0.81, episodes: 38\n",
      "961: reward:  18.00, mean_100:  24.62, epsilon: 0.81, episodes: 39\n",
      "982: reward:  21.00, mean_100:  24.52, epsilon: 0.80, episodes: 40\n",
      "1005: reward:  23.00, mean_100:  24.49, epsilon: 0.80, episodes: 41\n",
      "1025: reward:  20.00, mean_100:  24.38, epsilon: 0.80, episodes: 42\n",
      "1047: reward:  22.00, mean_100:  24.33, epsilon: 0.79, episodes: 43\n",
      "1057: reward:  10.00, mean_100:  24.00, epsilon: 0.79, episodes: 44\n",
      "1070: reward:  13.00, mean_100:  23.76, epsilon: 0.79, episodes: 45\n",
      "1082: reward:  12.00, mean_100:  23.50, epsilon: 0.78, episodes: 46\n",
      "1126: reward:  44.00, mean_100:  23.94, epsilon: 0.77, episodes: 47\n",
      "1158: reward:  32.00, mean_100:  24.10, epsilon: 0.77, episodes: 48\n",
      "1202: reward:  44.00, mean_100:  24.51, epsilon: 0.76, episodes: 49\n",
      "1218: reward:  16.00, mean_100:  24.34, epsilon: 0.76, episodes: 50\n",
      "1262: reward:  44.00, mean_100:  24.73, epsilon: 0.75, episodes: 51\n",
      "1277: reward:  15.00, mean_100:  24.54, epsilon: 0.74, episodes: 52\n",
      "1313: reward:  36.00, mean_100:  24.75, epsilon: 0.74, episodes: 53\n",
      "1327: reward:  14.00, mean_100:  24.56, epsilon: 0.73, episodes: 54\n",
      "1364: reward:  37.00, mean_100:  24.78, epsilon: 0.73, episodes: 55\n",
      "1435: reward:  71.00, mean_100:  25.61, epsilon: 0.71, episodes: 56\n",
      "1463: reward:  28.00, mean_100:  25.65, epsilon: 0.71, episodes: 57\n",
      "1508: reward:  45.00, mean_100:  25.98, epsilon: 0.70, episodes: 58\n",
      "1626: reward: 118.00, mean_100:  27.54, epsilon: 0.67, episodes: 59\n",
      "1658: reward:  32.00, mean_100:  27.62, epsilon: 0.67, episodes: 60\n",
      "1731: reward:  73.00, mean_100:  28.36, epsilon: 0.65, episodes: 61\n",
      "1820: reward:  89.00, mean_100:  29.34, epsilon: 0.64, episodes: 62\n",
      "1872: reward:  52.00, mean_100:  29.70, epsilon: 0.63, episodes: 63\n",
      "1978: reward: 106.00, mean_100:  30.89, epsilon: 0.60, episodes: 64\n",
      "2021: reward:  43.00, mean_100:  31.08, epsilon: 0.60, episodes: 65\n",
      "2105: reward:  84.00, mean_100:  31.88, epsilon: 0.58, episodes: 66\n",
      "2193: reward:  88.00, mean_100:  32.72, epsilon: 0.56, episodes: 67\n",
      "2217: reward:  24.00, mean_100:  32.59, epsilon: 0.56, episodes: 68\n",
      "2254: reward:  37.00, mean_100:  32.65, epsilon: 0.55, episodes: 69\n",
      "2287: reward:  33.00, mean_100:  32.66, epsilon: 0.54, episodes: 70\n",
      "2338: reward:  51.00, mean_100:  32.92, epsilon: 0.53, episodes: 71\n",
      "2393: reward:  55.00, mean_100:  33.22, epsilon: 0.52, episodes: 72\n",
      "2573: reward: 180.00, mean_100:  35.23, epsilon: 0.49, episodes: 73\n",
      "2715: reward: 142.00, mean_100:  36.68, epsilon: 0.46, episodes: 74\n",
      "2750: reward:  35.00, mean_100:  36.65, epsilon: 0.45, episodes: 75\n",
      "2876: reward: 126.00, mean_100:  37.83, epsilon: 0.42, episodes: 76\n",
      "2944: reward:  68.00, mean_100:  38.22, epsilon: 0.41, episodes: 77\n",
      "2957: reward:  13.00, mean_100:  37.90, epsilon: 0.41, episodes: 78\n",
      "2971: reward:  14.00, mean_100:  37.59, epsilon: 0.41, episodes: 79\n",
      "3135: reward: 164.00, mean_100:  39.17, epsilon: 0.37, episodes: 80\n",
      "3255: reward: 120.00, mean_100:  40.17, epsilon: 0.35, episodes: 81\n",
      "3398: reward: 143.00, mean_100:  41.43, epsilon: 0.32, episodes: 82\n",
      "3548: reward: 150.00, mean_100:  42.73, epsilon: 0.29, episodes: 83\n",
      "3869: reward: 321.00, mean_100:  46.05, epsilon: 0.23, episodes: 84\n",
      "4030: reward: 161.00, mean_100:  47.40, epsilon: 0.19, episodes: 85\n",
      "4307: reward: 277.00, mean_100:  50.07, epsilon: 0.14, episodes: 86\n",
      "4476: reward: 169.00, mean_100:  51.44, epsilon: 0.10, episodes: 87\n",
      "4690: reward: 214.00, mean_100:  53.28, epsilon: 0.06, episodes: 88\n",
      "4851: reward: 161.00, mean_100:  54.49, epsilon: 0.03, episodes: 89\n",
      "5095: reward: 244.00, mean_100:  56.60, epsilon: 0.02, episodes: 90\n",
      "5290: reward: 195.00, mean_100:  58.12, epsilon: 0.02, episodes: 91\n",
      "5551: reward: 261.00, mean_100:  60.33, epsilon: 0.02, episodes: 92\n",
      "5780: reward: 229.00, mean_100:  62.14, epsilon: 0.02, episodes: 93\n",
      "6005: reward: 225.00, mean_100:  63.87, epsilon: 0.02, episodes: 94\n",
      "6231: reward: 226.00, mean_100:  65.58, epsilon: 0.02, episodes: 95\n",
      "6439: reward: 208.00, mean_100:  67.06, epsilon: 0.02, episodes: 96\n",
      "6673: reward: 234.00, mean_100:  68.78, epsilon: 0.02, episodes: 97\n",
      "6862: reward: 189.00, mean_100:  70.01, epsilon: 0.02, episodes: 98\n",
      "7267: reward: 405.00, mean_100:  73.39, epsilon: 0.02, episodes: 99\n",
      "7464: reward: 197.00, mean_100:  74.63, epsilon: 0.02, episodes: 100\n",
      "7646: reward: 182.00, mean_100:  76.30, epsilon: 0.02, episodes: 101\n",
      "7843: reward: 197.00, mean_100:  78.04, epsilon: 0.02, episodes: 102\n",
      "8068: reward: 225.00, mean_100:  80.16, epsilon: 0.02, episodes: 103\n",
      "8356: reward: 288.00, mean_100:  82.91, epsilon: 0.02, episodes: 104\n",
      "8628: reward: 272.00, mean_100:  85.51, epsilon: 0.02, episodes: 105\n",
      "8898: reward: 270.00, mean_100:  88.09, epsilon: 0.02, episodes: 106\n",
      "9124: reward: 226.00, mean_100:  90.19, epsilon: 0.02, episodes: 107\n",
      "9488: reward: 364.00, mean_100:  93.66, epsilon: 0.02, episodes: 108\n",
      "9730: reward: 242.00, mean_100:  95.74, epsilon: 0.02, episodes: 109\n",
      "10027: reward: 297.00, mean_100:  98.57, epsilon: 0.02, episodes: 110\n",
      "10527: reward: 500.00, mean_100: 103.30, epsilon: 0.02, episodes: 111\n",
      "10813: reward: 286.00, mean_100: 105.95, epsilon: 0.02, episodes: 112\n",
      "11016: reward: 203.00, mean_100: 107.67, epsilon: 0.02, episodes: 113\n",
      "11266: reward: 250.00, mean_100: 109.91, epsilon: 0.02, episodes: 114\n",
      "11569: reward: 303.00, mean_100: 112.68, epsilon: 0.02, episodes: 115\n",
      "11916: reward: 347.00, mean_100: 115.99, epsilon: 0.02, episodes: 116\n",
      "12232: reward: 316.00, mean_100: 118.99, epsilon: 0.02, episodes: 117\n",
      "12657: reward: 425.00, mean_100: 122.90, epsilon: 0.02, episodes: 118\n",
      "12968: reward: 311.00, mean_100: 125.78, epsilon: 0.02, episodes: 119\n",
      "13231: reward: 263.00, mean_100: 128.25, epsilon: 0.02, episodes: 120\n",
      "13512: reward: 281.00, mean_100: 130.93, epsilon: 0.02, episodes: 121\n",
      "13871: reward: 359.00, mean_100: 134.38, epsilon: 0.02, episodes: 122\n",
      "14119: reward: 248.00, mean_100: 136.50, epsilon: 0.02, episodes: 123\n",
      "14418: reward: 299.00, mean_100: 139.33, epsilon: 0.02, episodes: 124\n",
      "14649: reward: 231.00, mean_100: 141.29, epsilon: 0.02, episodes: 125\n",
      "14873: reward: 224.00, mean_100: 143.25, epsilon: 0.02, episodes: 126\n",
      "15098: reward: 225.00, mean_100: 145.29, epsilon: 0.02, episodes: 127\n",
      "15362: reward: 264.00, mean_100: 147.18, epsilon: 0.02, episodes: 128\n",
      "15642: reward: 280.00, mean_100: 149.71, epsilon: 0.02, episodes: 129\n",
      "15872: reward: 230.00, mean_100: 151.62, epsilon: 0.02, episodes: 130\n",
      "16017: reward: 145.00, mean_100: 152.75, epsilon: 0.02, episodes: 131\n",
      "16218: reward: 201.00, mean_100: 154.63, epsilon: 0.02, episodes: 132\n",
      "16456: reward: 238.00, mean_100: 156.55, epsilon: 0.02, episodes: 133\n",
      "16625: reward: 169.00, mean_100: 157.55, epsilon: 0.02, episodes: 134\n",
      "16873: reward: 248.00, mean_100: 159.93, epsilon: 0.02, episodes: 135\n",
      "17127: reward: 254.00, mean_100: 162.19, epsilon: 0.02, episodes: 136\n",
      "17502: reward: 375.00, mean_100: 165.70, epsilon: 0.02, episodes: 137\n",
      "17781: reward: 279.00, mean_100: 168.38, epsilon: 0.02, episodes: 138\n",
      "18077: reward: 296.00, mean_100: 171.16, epsilon: 0.02, episodes: 139\n",
      "18369: reward: 292.00, mean_100: 173.87, epsilon: 0.02, episodes: 140\n",
      "18548: reward: 179.00, mean_100: 175.43, epsilon: 0.02, episodes: 141\n",
      "18826: reward: 278.00, mean_100: 178.01, epsilon: 0.02, episodes: 142\n",
      "19092: reward: 266.00, mean_100: 180.45, epsilon: 0.02, episodes: 143\n",
      "19317: reward: 225.00, mean_100: 182.60, epsilon: 0.02, episodes: 144\n",
      "19548: reward: 231.00, mean_100: 184.78, epsilon: 0.02, episodes: 145\n",
      "19825: reward: 277.00, mean_100: 187.43, epsilon: 0.02, episodes: 146\n",
      "19992: reward: 167.00, mean_100: 188.66, epsilon: 0.02, episodes: 147\n",
      "20151: reward: 159.00, mean_100: 189.93, epsilon: 0.02, episodes: 148\n",
      "20386: reward: 235.00, mean_100: 191.84, epsilon: 0.02, episodes: 149\n",
      "20598: reward: 212.00, mean_100: 193.80, epsilon: 0.02, episodes: 150\n",
      "20819: reward: 221.00, mean_100: 195.57, epsilon: 0.02, episodes: 151\n",
      "Solved in 20819 steps and 151 episodes!\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    step_idx += 1\n",
    "    selector.epsilon = max(EPSILON_STOP, EPSILON_START - step_idx / EPLSION_STEP)\n",
    "    replay_buffer.populate(1)\n",
    "    \n",
    "    if len(replay_buffer) < BATCH_SIZE:\n",
    "        continue\n",
    "    \n",
    "    # sample batch\n",
    "    batch = replay_buffer.sample(BATCH_SIZE)\n",
    "    batch_states = [exp.state for exp in batch]\n",
    "    batch_actions = [exp.action for exp in batch]\n",
    "    batch_targets = [calc_target(net, exp.reward, exp.last_state) for exp in batch]\n",
    "    \n",
    "    # train\n",
    "    optimizer.zero_grad()\n",
    "    states_v = torch.FloatTensor(batch_states)\n",
    "    net_q_v = net(states_v)\n",
    "    target_q = net_q_v.data.numpy().copy()\n",
    "    target_q[range(BATCH_SIZE), batch_actions] = batch_targets\n",
    "    target_q_v = torch.tensor(target_q)\n",
    "    loss_v = mse_loss(net_q_v, target_q_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # handle new rewards\n",
    "    new_rewards = exp_source.pop_total_rewards()\n",
    "    if new_rewards:\n",
    "        done_episodes += 1\n",
    "        reward = new_rewards[0]\n",
    "        total_rewards.append(reward)\n",
    "        mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "        print(\"%d: reward: %6.2f, mean_100: %6.2f, epsilon: %.2f, episodes: %d\"\n",
    "              % (step_idx, reward, mean_rewards, selector.epsilon, done_episodes))\n",
    "        writer.add_scalar(\"reward\", reward, step_idx)\n",
    "        writer.add_scalar(\"reward_100\", reward, step_idx)\n",
    "        writer.add_scalar(\"epsilon\", selector.epsilon, step_idx)\n",
    "        writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "        if mean_rewards > 195:\n",
    "            print(\"Solved in %d steps and %d episodes!\"\n",
    "                  % (step_idx, done_episodes))\n",
    "            break\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "network",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
