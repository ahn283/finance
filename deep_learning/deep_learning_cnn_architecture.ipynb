{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahn283/finance/blob/main/deep_learning_cnn_architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PwsswQlPh787"
      },
      "source": [
        "# **Import Library:** 분석에 사용할 모듈 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKH1jn1Bh4gA"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!python -m pip install --user --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnAzEVeSh_vA"
      },
      "outputs": [],
      "source": [
        "# Tensorflow 및 Keras 설치하기\n",
        "!pip install tensorflow==2.10.*\n",
        "# !pip install --upgrade --user tensorflow\n",
        "!pip install -U tensorflow-addons\n",
        "!pip install keras\n",
        "# !pip install --upgrade --user keras\n",
        "!pip install keras-tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDENvAuwiBHm"
      },
      "outputs": [],
      "source": [
        "# Ignore the warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# System related and data input controls\n",
        "import os\n",
        "\n",
        "# Data manipulation and visiualization\n",
        "import pandas as pd\n",
        "pd.options.display.float_format = '{:,.2f}'.format\n",
        "pd.options.display.max_rows = 20\n",
        "pd.options.display.max_columns = 20\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "\n",
        "## Modeling algorithms\n",
        "# General\n",
        "import statsmodels.api as sm\n",
        "from scipy import stats\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "tf.get_logger().warning('test')\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers, callbacks\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, Flatten, Dropout, Reshape\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Model selection\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Evaluation metrics\n",
        "from sklearn import metrics\n",
        "# for regression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import roc_auc_score, auc, precision_recall_curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WayPoqPXKAg"
      },
      "outputs": [],
      "source": [
        "# Module regression\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "import statsmodels.api as sm \n",
        "from scipy import stats \n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "\n",
        "\n",
        "### Boston Housing Price\n",
        "# 자료형 변환 + 결측값 처리\n",
        "def prep(df_origin):\n",
        "    df = df_origin.copy()\n",
        "    \n",
        "    # 자료형 변환\n",
        "    col = []\n",
        "    if df['ZN'].dtype == 'object':\n",
        "        for i in df['ZN']:\n",
        "            col.append(float(i[1:-1]))\n",
        "        df['ZN'] = col\n",
        "\n",
        "    col = []\n",
        "    if df['CHAS'].dtype == 'object':\n",
        "        for i in df['CHAS']:\n",
        "            col.append(float(i[1:-1]))\n",
        "        df['CHAS'] = col\n",
        "\n",
        "    # 결측값 처리\n",
        "    for i in df.columns[df.isnull().sum() != 0]:\n",
        "        if i not in ['INDUS', 'RM']:\n",
        "            df[i].fillna(df[i].mean(), inplace=True)\n",
        "        else:\n",
        "            df[i].fillna(df[i].median(), inplace=True)\n",
        "            \n",
        "    return df\n",
        "\n",
        "# 데이터 변환\n",
        "def feature_engineering(df_origin):\n",
        "    df = df_origin.copy()\n",
        "    \n",
        "    interval = [100, 200, 300, 400, 500, 600, 700, 800]\n",
        "    if df['TAX'].max() >= 100:\n",
        "        df['TAX'] = np.digitize(df['TAX'], bins=interval)\n",
        "        \n",
        "    if 'TAX' in df.columns:\n",
        "        df_dummy = pd.get_dummies(df['TAX'], prefix='TAX', drop_first=True)\n",
        "        df = pd.concat([df, df_dummy], axis=1)\n",
        "        del df['TAX']\n",
        "\n",
        "    if 'CHAS' in df.columns:\n",
        "        df['CHAS'] = df['CHAS'].astype(int)\n",
        "        df_dummy = pd.get_dummies(df['CHAS'], prefix='CHAS', drop_first=False)\n",
        "        df = pd.concat([df, df_dummy], axis=1)\n",
        "        del df['CHAS']\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# 데이터 분리\n",
        "def datasplit(df, Y_colname, test_size=0.2, random_state=123):\n",
        "    X_colname = [x for x in df.columns if x not in Y_colname]\n",
        "       \n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(df[X_colname], df[Y_colname],\n",
        "                                                        test_size=test_size, random_state=random_state)\n",
        "    print(X_train.shape, Y_train.shape)\n",
        "    print(X_test.shape, Y_test.shape)\n",
        "    \n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "# 데이터 변환 후 X_train, X_test의 변수 갯수 일치\n",
        "def col_mapping(X_train, X_test):\n",
        "    X_tr = X_train.copy()\n",
        "    X_te = X_test.copy()\n",
        "    \n",
        "    # Train & Test 변수명 체크\n",
        "    X_te_noncol = [i for i in X_tr.columns if i not in X_te.columns]\n",
        "    X_tr_noncol = [i for i in X_te.columns if i not in X_tr.columns]\n",
        "\n",
        "    # 변수 갯수 일치\n",
        "    if X_te_noncol != []:\n",
        "        for i in X_te_noncol:\n",
        "            X_te[i] = 0\n",
        "            X_te = X_te[X_tr.columns].copy()\n",
        "            \n",
        "    if X_tr_noncol != []:\n",
        "        for i in X_tr_noncol:\n",
        "            X_tr[i] = 0\n",
        "            X_tr = X_tr[X_te.columns].copy()\n",
        "            \n",
        "    return X_tr, X_te\n",
        "\n",
        "# 스케쥴 조정\n",
        "def scale(scaler, X_train, X_test):\n",
        "    scaler_fit = scaler.fit(X_train)\n",
        "    X_train_scaling = pd.DataFrame(scaler_fit.transform(X_train), \n",
        "                                   index=X_train.index, columns=X_train.columns)\n",
        "    X_test_scaling = pd.DataFrame(scaler_fit.transform(X_test), \n",
        "                                  index=X_test.index, columns=X_test.columns)\n",
        "    \n",
        "    return X_train_scaling, X_test_scaling\n",
        "\n",
        "# 실제 Y와 예측치 시각화\n",
        "def plot_prediction(Y_true_pred):\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.plot(Y_true_pred, linewidth=5, label=Y_true_pred.columns)\n",
        "    plt.xticks(fontsize=25, rotation=0)\n",
        "    plt.yticks(fontsize=25)\n",
        "    plt.xlabel('Index', fontname='serif', fontsize=28)\n",
        "    plt.legend(fontsize=20)\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# 검증 함수화\n",
        "def evaluation_reg(Y_real, Y_pred):\n",
        "    MAE = mean_absolute_error(Y_real, Y_pred)\n",
        "    MSE = mean_squared_error(Y_real, Y_pred)\n",
        "    MAPE = mean_absolute_percentage_error(Y_real, Y_pred)\n",
        "    Score = pd.DataFrame([MAE, MSE, MAPE], index=['MAE', 'MSE', 'MAPE'], columns=['Score']).T\n",
        "    \n",
        "    return Score\n",
        "\n",
        "# Train & Test 모두의 검증 함수화\n",
        "def evaluation_reg_trte(Y_real_tr, Y_pred_tr, Y_real_te, Y_pred_te):\n",
        "    Score_tr = evaluation_reg(Y_real_tr, Y_pred_tr)\n",
        "    Score_te = evaluation_reg(Y_real_te, Y_pred_te)\n",
        "    Score_trte = pd.concat([Score_tr, Score_te], axis=0)\n",
        "    Score_trte.index = ['Train', 'Test']\n",
        "\n",
        "    return Score_trte\n",
        "\n",
        "# 에러 분석\n",
        "def error_analysis(X_Data, Y_Pred, Residual, graph_on=False):\n",
        "    if graph_on == True:\n",
        "        ##### 시각화\n",
        "        # 잔차의 정규본포성 확인\n",
        "        # sns.displot(Residual, norm_hist='True', fit=stats.norm)\n",
        "        sns.displot(Residual, kind='hist')\n",
        "        plt.show()\n",
        "\n",
        "        # 잔차의 등분산성 확인\n",
        "        temp = pd.concat([Y_Pred, Residual.reset_index().iloc[:,[1]]], axis=1)\n",
        "        sns.scatterplot(x='Pred', y='Error', data=temp)\n",
        "        plt.show()\n",
        "        \n",
        "        # 잔차의 자기상관성 확인\n",
        "        sm.graphics.tsa.plot_acf(Residual, lags=50, use_vlines=True)\n",
        "        plt.show()\n",
        "\n",
        "    ##### 통계량\n",
        "    # 정규분포\n",
        "    # Null Hypothesis: The residuals are normally distributed\n",
        "    Normality = pd.DataFrame([stats.shapiro(Residual)], \n",
        "                             index=['Normality'], columns=['Test Statistics', 'p-value']).T\n",
        "\n",
        "    # 등분산성\n",
        "    # Null Hypothesis: Error terms are homoscedastic\n",
        "    Heteroscedasticity = pd.DataFrame([sm.stats.diagnostic.het_goldfeldquandt(Residual, X_Data.values, alternative='two-sided')],\n",
        "                                      index=['Heteroscedasticity'], \n",
        "                                      columns=['Test Statistics', 'p-value', 'Alternative']).T\n",
        "    \n",
        "    # 자기상관\n",
        "    # Null Hypothesis: Autocorrelation is absent\n",
        "    Autocorrelation = pd.concat([pd.DataFrame(sm.stats.diagnostic.acorr_ljungbox(Residual, lags=[10,50]).iloc[:,0]),\n",
        "                             pd.DataFrame(sm.stats.diagnostic.acorr_ljungbox(Residual, lags=[10,50]).iloc[:,1])], axis=1).T\n",
        "    Autocorrelation.index = ['Test Statistics', 'p-value']\n",
        "    Autocorrelation.columns = ['Autocorr(lag10)', 'Autocorr(lag50)']\n",
        "    \n",
        "    Error_Analysis = pd.concat([Normality, Heteroscedasticity, Autocorrelation], join='outer', axis=1)\n",
        "    \n",
        "    return Error_Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dx-7N0MoXR4m"
      },
      "outputs": [],
      "source": [
        "# Module classificaion\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import confusion_matrix, classification_report \n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
        "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
        "\n",
        "\n",
        "\n",
        "### Titanic\n",
        "# 결측값 처리\n",
        "def fillnull(df):\n",
        "    ## Age\n",
        "    df_agem = df[df.Sex == 'male']\n",
        "    df_agem = df_agem[['Age']].fillna(df_agem[['Age']].mean())\n",
        "\n",
        "    df_agefm = df[df.Sex == 'female']\n",
        "    df_agefm = df_agefm[['Age']].fillna(df_agefm[['Age']].mean())\n",
        "    df[['Age']] = pd.concat([df_agem, df_agefm], axis=0).sort_index()\n",
        "\n",
        "    ## Embarked\n",
        "    df[['Embarked']] = df[['Embarked']].fillna(df['Embarked'].mode()[0], axis=0)\n",
        "\n",
        "    ## Cabin\n",
        "    df['Cabin'] = df['Cabin'].str.slice(0,1)\n",
        "    df['Cabin'] = df['Cabin'].fillna('Temp')\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# 데이터 정리\n",
        "def prep(df):\n",
        "    # Name\n",
        "    name_sub = df['Name'].str.split(',', expand=True)[1]\n",
        "    name_sub = name_sub.str.split('. ', expand=True)[0]\n",
        "    name_sub.replace([' Mr'], 'Mr', inplace=True)\n",
        "    name_sub.replace([' Mrs'], 'Mrs', inplace=True)\n",
        "    name_sub.replace([' Ms', ' Miss'], 'Miss', inplace=True)\n",
        "    name_sub.replace([' Don', ' Mme', ' Lady', 'Sir', 'Mlle', ' th', ' Jonkheer'], 'Noble', inplace=True)\n",
        "    name_sub.replace([' Master', ' Major', ' Col', ' Capt', ' Sir', ' Mlle', ' Jonkheer'], 'Officer', inplace=True)\n",
        "    name_sub.replace([' Dr', ' Rev'], 'Priest', inplace=True)\n",
        "    df['Name'] = name_sub.copy()\n",
        "\n",
        "    # Ticket\n",
        "    df['Ticket'] = df['Ticket'].str.replace('.','').str.replace('/','')\n",
        "    df['Ticket'] = df['Ticket'].str.strip().str.split(' ').str[0]\n",
        "    df.loc[df['Ticket'].str.isdigit(), 'Ticket'] = 'NUM'\n",
        "\n",
        "    # Age\n",
        "    age_interval = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
        "    if df['Age'].max() > len(age_interval):\n",
        "        df['Age'] = np.digitize(df['Age'], bins=age_interval)\n",
        "        \n",
        "    return df\n",
        "\n",
        "\n",
        "# 데이터 분리\n",
        "def datasplit(df, Y_colname, test_size=0.2, random_state=123):\n",
        "    X_colname = [x for x in df.columns if x not in Y_colname]\n",
        "       \n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(df[X_colname], df[Y_colname],\n",
        "                                                        test_size=test_size, random_state=random_state)\n",
        "    print(X_train.shape, Y_train.shape)\n",
        "    print(X_test.shape, Y_test.shape)\n",
        "    \n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "\n",
        "# 데이터 변환\n",
        "def label_encoding(X_train, X_test, colname, encoder):\n",
        "    X_train_le = X_train.copy()\n",
        "    X_test_le = X_test.copy()\n",
        "    \n",
        "    for i in colname: # 대상 변수들마다\n",
        "        encoder = encoder.fit(X_train[i])\n",
        "\n",
        "        for label in X_test[i].unique():\n",
        "            if label not in encoder.classes_:\n",
        "                encoder.classes_ = np.append(encoder.classes_, label)\n",
        "\n",
        "        X_train_le[i] = encoder.transform(X_train[i])\n",
        "        X_test_le[i] = encoder.transform(X_test[i])\n",
        "        \n",
        "    return X_train_le, X_test_le\n",
        "\n",
        "\n",
        "def onehot_encoding(df, colname):\n",
        "    df_ohe = df.copy()\n",
        "    \n",
        "    for i in colname:\n",
        "        if i in df.columns:\n",
        "            df_ohe = pd.get_dummies(df_ohe, columns=[i], drop_first=True)\n",
        "            \n",
        "    return df_ohe\n",
        "\n",
        "\n",
        "# 데이터 변환 후 X_train과 X_test의 변수 갯수 일치\n",
        "def col_mapping(X_train, X_test):\n",
        "    X_tr = X_train.copy()\n",
        "    X_te = X_test.copy()\n",
        "    \n",
        "    # Train & Test 변수명 체크\n",
        "    X_te_noncol = [i for i in X_tr.columns if i not in X_te.columns]\n",
        "    X_tr_noncol = [i for i in X_te.columns if i not in X_tr.columns]\n",
        "\n",
        "    # 변수 갯수 일치\n",
        "    if X_te_noncol != []:\n",
        "        for i in X_te_noncol:\n",
        "            X_te[i] = 0\n",
        "            X_te = X_te[X_tr.columns].copy()\n",
        "            \n",
        "    if X_tr_noncol != []:\n",
        "        for i in X_tr_noncol:\n",
        "            X_tr[i] = 0\n",
        "            X_tr = X_tr[X_te.columns].copy()\n",
        "            \n",
        "    return X_tr, X_te\n",
        "\n",
        "\n",
        "# 스케일 조정\n",
        "def scale(scaler, X_train, X_test):\n",
        "    scaler_fit = scaler.fit(X_train)\n",
        "    X_train_scaling = pd.DataFrame(scaler_fit.transform(X_train), \n",
        "                                   index=X_train.index, columns=X_train.columns)\n",
        "    X_test_scaling = pd.DataFrame(scaler_fit.transform(X_test), \n",
        "                                  index=X_test.index, columns=X_test.columns)\n",
        "    \n",
        "    return X_train_scaling, X_test_scaling\n",
        "\n",
        "\n",
        "# Confusion Matrix 함수화\n",
        "def evaluation_class_ConfusionMatrix(Y_true, Y_pred):\n",
        "    conf_mat = confusion_matrix(Y_true, Y_pred)\n",
        "    index_name = ['True '+str(i) for i in np.unique(np.array(Y_true))]\n",
        "    column_name = ['Pred '+str(i) for i in np.unique(np.array(Y_true))]\n",
        "    conf_mat = pd.DataFrame(conf_mat, index=index_name, columns=column_name)\n",
        "    \n",
        "    return conf_mat\n",
        "\n",
        "\n",
        "# Classification Report 함수화\n",
        "def evaluation_class_ClassificationReport(Y_true, Y_pred):\n",
        "    print(classification_report(Y_true, Y_pred, \n",
        "                                target_names=['class 0', 'class 1']))\n",
        "\n",
        "\n",
        "# ROC Curve & AUC 함수화\n",
        "def evaluation_class_ROCAUC(Y_true, P_pred, figsize=(10,5), label='Logistic Regression'):\n",
        "    fpr, tpr, thresholds = roc_curve(Y_true, P_pred)\n",
        "    cm = evaluation_class_ConfusionMatrix(Y_true, P_pred>=0.5)\n",
        "    recall = cm.iloc[1,1] / cm.sum(axis=1).values[1]\n",
        "    fallout = cm.iloc[0,1] / cm.sum(axis=1).values[0]\n",
        "    \n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.plot(fpr, tpr, label=label)\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.plot([fallout], [recall], 'ro', ms=10)\n",
        "    plt.title('AUC: ' + str(auc(fpr, tpr)), fontsize=15)\n",
        "    plt.xlabel('False Positive Rate', fontsize=15)\n",
        "    plt.ylabel('True Positive Rate', fontsize=15)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.legend(loc='best', fontsize=12)\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "# Precision & Recall Curve 함수화\n",
        "def evaluation_class_PrecisionRecall(Y_true, P_pred, figsize=(10,5)):\n",
        "    pre, rec, thresholds = precision_recall_curve(Y_true, P_pred)\n",
        "    \n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.plot(thresholds, pre[:-1], label='Precision')\n",
        "    plt.plot(thresholds, rec[1:], label='Recall')\n",
        "    plt.xlabel('Threshold', fontsize=15)\n",
        "    plt.ylabel('', fontsize=15)\n",
        "    plt.xticks(fontsize=12)\n",
        "    plt.yticks(fontsize=12)\n",
        "    plt.legend(loc='best', fontsize=12)\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "# Accuracy & AUC 함수화\n",
        "def evaluation_class_AccuracyAUC(Y_train, Y_trpred, Y_test, Y_tepred):\n",
        "    score = pd.DataFrame([[accuracy_score(Y_train, Y_trpred), \n",
        "                           roc_auc_score(Y_train, Y_trpred)],\n",
        "                          [accuracy_score(Y_test, Y_tepred), \n",
        "                           roc_auc_score(Y_test, Y_tepred)]],\n",
        "                         index=['Train', 'Test'], columns=['Accuracy', 'AUC'])\n",
        "    return score"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ga_7waN0XamG"
      },
      "source": [
        "# **Problem:** 숫자이미지 분류"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4b7u1YGKXbhC"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7SjuR-tXdUC"
      },
      "outputs": [],
      "source": [
        "# MNIST(Modified Institute of Standards and Technology)를 train, test로 로딩\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIkc0NKtXvEY"
      },
      "outputs": [],
      "source": [
        "# 데이터의 갯수와 크기 확인\n",
        "print('---------train----------')\n",
        "print('X: ', X_train.shape, X_train.min(), X_train.max())\n",
        "print('Y: ', Y_train.shape)\n",
        "print('--------test--------')\n",
        "print('X: ', X_test.shape, X_test.min(), X_test.max())\n",
        "print('Y: ', Y_test.shape)    # (10000, ) 표시 이슈 : scala가 아니고 vector 임을 표시하기 위함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqjjurdHYSVI"
      },
      "outputs": [],
      "source": [
        "# 샘플 데이터 출력\n",
        "# 0에서 255 사이의 값을 가진 데이터\n",
        "for i in range(X_train[0].shape[0]):\n",
        "  for j in range(X_train[0].shape[1]):\n",
        "    print('{:4d}'.format(X_train[0][i][j]), end='')\n",
        "  print()   # for line breaking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BtmNdAWYxUG"
      },
      "outputs": [],
      "source": [
        "# 샘플 데이터 시각화\n",
        "# 0에서 255 사이의 값을 가진 데이터 시각화\n",
        "for i in range(4):\n",
        "  plt.figure(figsize=(2,2))\n",
        "  plt.imshow(X_train[i], cmap='Greys')\n",
        "  plt.show()\n",
        "\n",
        "print('Label of train: ', Y_train[:4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMFOCm4xa3SQ"
      },
      "outputs": [],
      "source": [
        "# 데이터 스케일링\n",
        "X_train = X_train.astype('float32')/255\n",
        "X_test = X_test.astype('float32')/255\n",
        "print('X_train: ', X_train.shape, 'Y_train: ', Y_train.shape)\n",
        "print('X_test: ', X_test.shape, 'Y_test: ', Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMg6hgije6qg"
      },
      "outputs": [],
      "source": [
        "# Reshape\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)   # X_trina을 2차원(0인덱스는 X_train.shape[0]으로 나머지는 1인덱스에 자동 계산)으로 변경\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "print('X_train: ', X_train.shape, 'Y_train: ', Y_train.shape)\n",
        "print('X_test: ', X_test.shape, 'Y_test: ', Y_test.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Jih_tZr4bskq"
      },
      "source": [
        "## MLP\n",
        "\n",
        "<img src='https://github.com/ahn283/finance/blob/main/machine_learning/img/DL_MLP_Custom.PNG?raw=true' width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT4V1cqGbqah"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter\n",
        "\n",
        "tf.random.set_seed(1)\n",
        "HIDDEN_ACTIVATION = 'relu'\n",
        "OUTPUT_ACTIVATION ='softmax'\n",
        "LOSS = 'sparse_categorical_crossentropy'\n",
        "LEARNING_RATE = 0.01\n",
        "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "METRICS = ['accuracy']\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "VERBOSE = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I27k4HQKczAh"
      },
      "outputs": [],
      "source": [
        "# Building network\n",
        "inputs = Input(shape=(X_train.shape[1], ))\n",
        "hiddens = Dense(256, activation=HIDDEN_ACTIVATION)(inputs)\n",
        "hiddens = Dense(128, activation=HIDDEN_ACTIVATION)(hiddens)\n",
        "output = Dense(10, activation=OUTPUT_ACTIVATION)(hiddens)\n",
        "model = Model(inputs, output)\n",
        "model.summary()\n",
        "plot_model(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svjbhupUdbID"
      },
      "outputs": [],
      "source": [
        "# 데이터 학습하기\n",
        "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
        "model_fit = model.fit(X_train, Y_train, validation_split=0.2,\n",
        "                      batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE)\n",
        "plt.plot(pd.DataFrame(model_fit.history[METRICS[0]]))\n",
        "plt.plot(pd.DataFrame(model_fit.history['val_'+METRICS[0]]))\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SArGik9ifhte"
      },
      "source": [
        "### Prediction and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVAMxanrepIL"
      },
      "outputs": [],
      "source": [
        "# Train 데이터 예측하기\n",
        "Y_trpred = model.predict(X_train)\n",
        "print(Y_trpred, '\\n\\n', np.argmax(Y_trpred, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiFDH4pjfu2i"
      },
      "outputs": [],
      "source": [
        "# Test 데이터 예측하기\n",
        "Y_tepred = model.predict(X_test)\n",
        "print(Y_tepred, '\\n\\n', np.argmax(Y_tepred, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSDW6gedf7Nb"
      },
      "outputs": [],
      "source": [
        "# Train 예측 성능 평가하기\n",
        "# 학습과정의 결과와 유사\n",
        "model.evaluate(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fDXxNlogDR0"
      },
      "outputs": [],
      "source": [
        "# Train 예측 성능 평가하기\n",
        "# 학습과정의 결과와 유사\n",
        "pd.crosstab(Y_train, np.argmax(model.predict(X_train), axis=1),\n",
        "            rownames=['True'], colnames=['Pred'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qahDKfwcgVfr"
      },
      "outputs": [],
      "source": [
        "# Test 예측 성능 평가하기\n",
        "model.evaluate(X_test, Y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZbibbwaghhw"
      },
      "outputs": [],
      "source": [
        "# Test 예측 성능 평가하기\n",
        "pd.crosstab(Y_test, np.argmax(model.predict(X_test), axis=1),\n",
        "            rownames=['True'], colnames=['Pred'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zftn44wWgulR"
      },
      "outputs": [],
      "source": [
        "# 임의 데이터를 예측하고 정답과 비굑하기\n",
        "idx_rand = np.random.randint(0, X_train.shape[0])\n",
        "print('목표값: ', Y_train[idx_rand])\n",
        "print('예측값: ', np.argmax(model.predict(X_train[idx_rand][np.newaxis, :])))   # 새로운 데이터 입력해서 예측시 2차원 형식으로 입력 필요 (1, 784)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qBt8MuI3iCbe"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMyy7u4gh_cF"
      },
      "outputs": [],
      "source": [
        "# 하이퍼파라미터\n",
        "tf.random.set_seed(1)\n",
        "HIDDEN_ACTIVATION = 'relu'\n",
        "OUTPUT_ACTIVATION = 'softmax'\n",
        "LOSS = 'sparse_categorical_crossentropy'\n",
        "LEARNING_RATE = 0.01\n",
        "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "METRICS = ['accuracy']\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "VERBOSE = 1\n",
        "\n",
        "# Train, Test로 로딩\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# 데이터 스케일링\n",
        "X_train = X_train.astype(\"float32\")/255\n",
        "X_test = X_test.astype(\"float32\")/255\n",
        "print('normalized X: ', X_train.shape, X_train.min(), X_train.max())\n",
        "print('normalized X: ', X_test.shape, X_test.min(), X_test.max())\n",
        "\n",
        "# Reshape\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "print('X_train:', X_train.shape, 'Y_train:', Y_train.shape)\n",
        "print('X_test:', X_test.shape, 'Y_test:', Y_test.shape)\n",
        "\n",
        "# 네트워크 구축\n",
        "inputs = Input(shape=(X_train.shape[1],))   \n",
        "hiddens = Dense(256, activation=HIDDEN_ACTIVATION)(inputs)\n",
        "hiddens = Dense(128, activation=HIDDEN_ACTIVATION)(hiddens)\n",
        "output = Dense(10, activation=OUTPUT_ACTIVATION)(hiddens)  \n",
        "model = Model(inputs, output)  \n",
        "model.summary() \n",
        "plot_model(model) \n",
        "\n",
        "# 데이터 학습하기\n",
        "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
        "model_fit = model.fit(X_train, Y_train, validation_split=0.2,\n",
        "                      batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE)\n",
        "plt.plot(pd.DataFrame(model_fit.history[METRICS[0]]))\n",
        "plt.plot(pd.DataFrame(model_fit.history['val_'+METRICS[0]]))\n",
        "plt.legend([METRICS[0], 'val_'+METRICS[0]])\n",
        "plt.show()\n",
        "\n",
        "# Train 예측 성능 평가하기\n",
        "model.evaluate(X_train, Y_train)\n",
        "\n",
        "# Test 예측 성능 평가하기\n",
        "model.evaluate(X_test, Y_test)\n",
        "print('\\nTest Confusion Maxtrix: ')\n",
        "pd.crosstab(Y_test, np.argmax(model.predict(X_test), axis=1),\n",
        "            rownames=['True'], colnames=['Pred'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "k-ZB2QpaiSm9"
      },
      "source": [
        "## CNN\n",
        "\n",
        "<img src='https://github.com/ahn283/finance/blob/main/machine_learning/img/CNN_Example.png?raw=true' width=900>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gd2NhFkHigST"
      },
      "source": [
        "### Convolution and Pooling\n",
        "\n",
        "<img src='https://github.com/ahn283/finance/blob/main/machine_learning/img/CNN_Process_BlackImage.png?raw=true' width=800>\n",
        "\n",
        "- **Convolution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yIvgEfEnidBo"
      },
      "outputs": [],
      "source": [
        "# 임의 데이터 생성\n",
        "## tensorflow에서는 pandas 데이터를 사용할 수없음\n",
        "X = tf.random.normal(shape=(128,28,28,1), mean=1, stddev=1.)    # [batch size, W, H, channel]\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_U1m0foi3qN"
      },
      "outputs": [],
      "source": [
        "# 합성곱을 통과한 데이터\n",
        "hiddens = Conv2D(filters=32, kernel_size=(3,3), activation='relu')(X)\n",
        "print('After Convolution: ', hiddens.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36EWfmcXjFoe"
      },
      "outputs": [],
      "source": [
        "# STRIDE 이동을 빠르게 할 경우의 데이터\n",
        "hiddens = Conv2D(filters=32, kernel_size=(3,3), strides=2, activation='relu')(X)\n",
        "print('After Convolution: ', hiddens.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PKxffMT0NB6"
      },
      "outputs": [],
      "source": [
        "# 입력과 출력의 크기가 같은 데이터\n",
        "hiddens = Conv2D(filters=32, kernel_size=(3,3), strides=1, padding='same', activation='relu')(X)\n",
        "print('After Convolution: ', hiddens.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHpdTceXjiJK"
      },
      "outputs": [],
      "source": [
        "# 입력과 출력의 크기가 같은 데이터\n",
        "## stride=2\n",
        "hiddens = Conv2D(filters=32, kernel_size=(3,3), strides=2, padding='same', activation='relu')(X)\n",
        "print('After Convolution: ', hiddens.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "st0LRzCBj3LJ"
      },
      "source": [
        "- **Pooling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zLBhVpwj06S"
      },
      "outputs": [],
      "source": [
        "# 임의 데이터 생성\n",
        "X = tf.random.normal(shape=(128,28,28,1), mean=0, stddev=1.)\n",
        "hiddens = Conv2D(filters=32, kernel_size=(3,3), strides=2, padding='same', activation='relu')(X)\n",
        "print('After Convolution: ', hiddens.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsS26gRokvxu"
      },
      "outputs": [],
      "source": [
        "# Max pooling 이후의 데이터\n",
        "hidden_pool = MaxPooling2D(pool_size=(2,2), strides=(2,2))(hiddens)\n",
        "print('After Pooling: ', hidden_pool.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZe99-4DlNtf"
      },
      "outputs": [],
      "source": [
        "# Max Pooling 이후의 데이터\n",
        "hiddens_pool = MaxPooling2D(pool_size=(4,4), strides=(4,4))(hiddens)\n",
        "print('After Pooling: ', hiddens_pool.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1TBveJplo4C"
      },
      "outputs": [],
      "source": [
        "# Max Pooling 이후의 데이터\n",
        "hiddens_pool = MaxPooling2D(pool_size=(4,4), strides=(4,4), padding='same')(hiddens)\n",
        "print('After Pooling: ', hiddens_pool.shape)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "WmS06sTvly0U"
      },
      "source": [
        "### Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fioI2FV_lxJQ"
      },
      "outputs": [],
      "source": [
        "# 하이퍼파라미터\n",
        "tf.random.set_seed(1)\n",
        "KERNEL_SIZE = (3,3)\n",
        "STRIDE = 1\n",
        "POOL_SIZE = (2,2)\n",
        "POOL_STRIDE = 2\n",
        "PADDING = 'same'\n",
        "HIDDEN_ACTIVATION = 'relu'\n",
        "OUTPUT_ACTIVATION = 'softmax'\n",
        "LOSS = 'sparse_categorical_crossentropy'\n",
        "LEARNING_RATE = 0.01\n",
        "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "METRICS = ['accuracy']\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "VERBOSE = 1\n",
        "\n",
        "# Train, Test로 로딩\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# 데이터 스케일링\n",
        "X_train = X_train.astype(\"float32\")/255\n",
        "X_test = X_test.astype(\"float32\")/255\n",
        "print('normalized X: ', X_train.shape, X_train.min(), X_train.max())\n",
        "print('normalized X: ', X_test.shape, X_test.min(), X_test.max())\n",
        "\n",
        "# Reshape\n",
        "## 3차원의 데이터로 변경 (row, width, height, depth(channel))\n",
        "X_train = X_train.reshape(-1, X_train.shape[1], X_train.shape[2], 1)\n",
        "X_test = X_test.reshape(-1, X_test.shape[1], X_train.shape[2], 1)\n",
        "print('X_train:', X_train.shape, 'Y_train:', Y_train.shape)\n",
        "print('X_test:', X_test.shape, 'Y_test:', Y_test.shape)\n",
        "\n",
        "# 네트워크 구축\n",
        "## 입력값을 width: X_train.shape[1], height : X_train.shape[2], depth: X_train.depth[3]\n",
        "inputs = Input(shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]))   \n",
        "hiddens = Conv2D(128, kernel_size=KERNEL_SIZE, strides=STRIDE, padding=PADDING, \n",
        "                 activation=HIDDEN_ACTIVATION)(inputs)\n",
        "hiddens = MaxPooling2D(pool_size=POOL_SIZE, strides=POOL_STRIDE)(hiddens)\n",
        "## 1차원 데이터로 변경\n",
        "hiddens = Flatten()(hiddens)\n",
        "output = Dense(10, activation=OUTPUT_ACTIVATION)(hiddens)  \n",
        "model = Model(inputs, output)  \n",
        "model.summary() \n",
        "plot_model(model) \n",
        "\n",
        "# 데이터 학습하기\n",
        "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
        "model_fit = model.fit(X_train, Y_train, validation_split=0.2,\n",
        "                      batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE)\n",
        "plt.plot(pd.DataFrame(model_fit.history[METRICS[0]]))\n",
        "plt.plot(pd.DataFrame(model_fit.history['val_'+METRICS[0]]))\n",
        "plt.legend([METRICS[0], 'val_'+METRICS[0]])\n",
        "plt.show()\n",
        "\n",
        "# Train 예측 성능 평가하기\n",
        "model.evaluate(X_train, Y_train)\n",
        "\n",
        "# Test 예측 성능 평가하기\n",
        "model.evaluate(X_test, Y_test)\n",
        "print('\\nTest Confusion Maxtrix: ')\n",
        "pd.crosstab(Y_test, np.argmax(model.predict(X_test), axis=1),\n",
        "            rownames=['True'], colnames=['Pred'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CyU0Qt61mKig"
      },
      "source": [
        "## CNN with Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uksnCOBBmMLp"
      },
      "outputs": [],
      "source": [
        "# 하이퍼파라미터\n",
        "tf.random.set_seed(1)\n",
        "KERNEL_SIZE = (3,3)\n",
        "STRIDE = 1\n",
        "POOL_SIZE = (2,2)\n",
        "POOL_STRIDE = 2\n",
        "PADDING = 'same'\n",
        "HIDDEN_ACTIVATION = 'relu'\n",
        "OUTPUT_ACTIVATION = 'softmax'\n",
        "DROPOUT_RATIO = 0.25\n",
        "LOSS = 'sparse_categorical_crossentropy'\n",
        "LEARNING_RATE = 0.01\n",
        "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "METRICS = ['accuracy']\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5\n",
        "VERBOSE = 1\n",
        "################\n",
        "EARLYSTOP_PATIENT = 10\n",
        "MONITOR = 'val_loss'\n",
        "FILENAME = os.path.join(os.getcwd(),'Model','Number_MNIST_CNN.h5')\n",
        "CALLBACK = [callbacks.EarlyStopping(monitor=MONITOR, mode='min', \n",
        "                                    patience=EARLYSTOP_PATIENT, verbose=1),\n",
        "            tfa.callbacks.TQDMProgressBar(show_epoch_progress=False),\n",
        "            callbacks.ModelCheckpoint(monitor=MONITOR, mode='min', save_best_only=True, \n",
        "                                      filepath=FILENAME)]\n",
        "################\n",
        "\n",
        "# Train, Test로 로딩\n",
        "mnist = keras.datasets.mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# 데이터 스케일링\n",
        "X_train = X_train.astype(\"float32\")/255\n",
        "X_test = X_test.astype(\"float32\")/255\n",
        "print('normalized X: ', X_train.shape, X_train.min(), X_train.max())\n",
        "print('normalized X: ', X_test.shape, X_test.min(), X_test.max())\n",
        "\n",
        "# Reshape\n",
        "X_train = X_train.reshape(-1, X_train.shape[1], X_train.shape[2], 1)\n",
        "X_test = X_test.reshape(-1, X_test.shape[1], X_train.shape[2], 1)\n",
        "print('X_train:', X_train.shape, 'Y_train:', Y_train.shape)\n",
        "print('X_test:', X_test.shape, 'Y_test:', Y_test.shape)\n",
        "\n",
        "# 네트워크 구축\n",
        "inputs = Input(shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]))   \n",
        "hiddens = Conv2D(128, kernel_size=KERNEL_SIZE, strides=STRIDE, padding=PADDING, \n",
        "                 activation=HIDDEN_ACTIVATION)(inputs)\n",
        "hiddens = MaxPooling2D(pool_size=POOL_SIZE, strides=POOL_STRIDE)(hiddens)\n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "hiddens = Conv2D(64, kernel_size=KERNEL_SIZE, strides=STRIDE, padding=PADDING, \n",
        "                 activation=HIDDEN_ACTIVATION)(hiddens)\n",
        "hiddens = MaxPooling2D(pool_size=POOL_SIZE, strides=POOL_STRIDE)(hiddens)\n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "hiddens = Flatten()(hiddens)\n",
        "hiddens = Dense(32, activation=HIDDEN_ACTIVATION)(hiddens)  \n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "output = Dense(10, activation=OUTPUT_ACTIVATION)(hiddens)  \n",
        "model = Model(inputs, output)  \n",
        "model.summary() \n",
        "plot_model(model) \n",
        "\n",
        "# 데이터 학습하기\n",
        "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
        "model_fit = model.fit(X_train, Y_train, validation_split=0.2,\n",
        "                      batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE,\n",
        "                      callbacks=CALLBACK)\n",
        "plt.plot(pd.DataFrame(model_fit.history[METRICS[0]]))\n",
        "plt.plot(pd.DataFrame(model_fit.history['val_'+METRICS[0]]))\n",
        "plt.legend([METRICS[0], 'val_'+METRICS[0]])\n",
        "plt.show()\n",
        "\n",
        "# Train 예측 성능 평가하기\n",
        "model.evaluate(X_train, Y_train)\n",
        "\n",
        "# Test 예측 성능 평가하기\n",
        "model.evaluate(X_test, Y_test)\n",
        "print('\\nTest Confusion Maxtrix: ')\n",
        "pd.crosstab(Y_test, np.argmax(model.predict(X_test), axis=1),\n",
        "            rownames=['True'], colnames=['Pred'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPfaNmY6mnhR"
      },
      "outputs": [],
      "source": [
        "# 임의 데이터를 예측하고 정답과 비교하기\n",
        "idx_rand = np.random.randint(0, X_test.shape[0])\n",
        "print('목표값: ')\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_test[idx_rand], cmap='gray')\n",
        "plt.show()\n",
        "print('예측값: ', np.argmax(model.predict(X_test[idx_rand][np.newaxis,:])))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kL0wsgbkmk00"
      },
      "source": [
        "# **Problem:** 패션이미지 분류"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8RUTmN0mmqI8"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bh3wwijEmsIn"
      },
      "outputs": [],
      "source": [
        "# Train, Test 로딩\n",
        "mnist = keras.datasets.fashion_mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTKYwtqrpOZ4"
      },
      "outputs": [],
      "source": [
        "# 데이터의 갯수와 크기 확인\n",
        "print('---------------train----------------')\n",
        "print('X: ', X_train.shape, X_train.min(), X_train.max())\n",
        "print('Y: ', Y_train.shape)\n",
        "print('----------------test----------------')\n",
        "print('X: ', X_test.shape, X_test.min(), X_test.max())\n",
        "print('Y: ', Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdHMU1YMpSEP"
      },
      "outputs": [],
      "source": [
        "# 샘플 데이터 출력\n",
        "# 0에서 255 사이의 값을 가진 데이터\n",
        "for i in range(X_train[0].shape[0]):\n",
        "    for j in range(X_train[0].shape[1]):\n",
        "        print('{:4d}'.format(X_train[0][i][j]), end='')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO92UREgwJu7"
      },
      "outputs": [],
      "source": [
        "# 샘플 데이터 시각화\n",
        "# 0에서 255 사이의 값을 가진 데이터 시각화\n",
        "for i in range(4):\n",
        "  plt.figure(figsize=(2,2))\n",
        "  plt.imshow(X_train[i], cmap='Greys')\n",
        "  plt.show()\n",
        "Y_label_list = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
        "print('Label of train: ', [Y_label_list[num] for num in Y_train[:4]])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TDdcL4o5w13F"
      },
      "source": [
        "## `categorical_crossentropy?`\n",
        "\n",
        "**\"일반적으로 다중 클래스/라벨 분류시, `categorical_crossentropy` 또는 `sparse_categorical_crossentropy`를 사용할 수 있음\"**\n",
        "\n",
        "- 숫자이미지 데이터의 Y값은 `0에서 9사이의 값이 출력될 1자리의 숫자`\n",
        "\n",
        "```python\n",
        "---------------train----------------\n",
        "X:  (60000, 28, 28) 0 255\n",
        "Y:  (60000,)\n",
        "----------------test----------------\n",
        "X:  (10000, 28, 28) 0 255\n",
        "Y:  (10000,)\n",
        "```\n",
        "\n",
        "- 모델링의 출력형태는 `0에서 9사이의 값이 출력될 각각의 확률 10자리의 숫자`\n",
        "\n",
        "```python\n",
        "output = Dense(10, activation=OUTPUT_ACTIVATION)(hiddens) \n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5g9qJOnwyS6"
      },
      "outputs": [],
      "source": [
        "# sparse_categorical_crossentropy 사용시 입출력\n",
        "# Y값은 0에서 2사이의 값이 출력될 1자리의 숫자 2개 레이블\n",
        "Y_true = [1, 2]\n",
        "\n",
        "# 출력 Y는 0에서 2사이의 값이 출력될 각각의 확률 3자리의 숫자\n",
        "## 첫번째 list [0일 확률, 1일 확률, 2일 확률], 두번째 list [0일 확률, 1일 확률, 2일 확률]\n",
        "Y_pred = [[0, 0.9, 0.1], [0.6, 0.8, 0.1]]\n",
        "loss = keras.metrics.sparse_categorical_crossentropy(Y_true, Y_pred)\n",
        "\n",
        "## loss : 첫번째 정답과 확률, 두번째 정답과 확률 비교 계산\n",
        "## numpy=array([0.10536056, 2.7080503 ], dtype=float32)\n",
        "loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DeFP04dOxwRz"
      },
      "source": [
        "```python\n",
        "# Y값은 0에서 2사이의 값이 출력될 각각의 확률 3자리의 숫자로(One-hot Encoding 방식)으로 받을 시 출력\n",
        "Y_true = [[0,1,0], [0,0,1]]\n",
        "# 출력 Y는 0에서 2사이의 값이 출력될 각각의 확률 3자리의 숫자\n",
        "Y_pred = [[0, 0.9, 0.1], [0.6, 0.8, 0.1]]\n",
        "loss = keras.metrics.sparse_categorical_crossentropy(Y_true, Y_pred)\n",
        "loss\n",
        "\n",
        "- sparse_categorical_crossentropy를 쓰려면 입력값은 한자리(hot hot encoding이 아니어야 함)여야 하며, 출력값은 클래스 갯수 만큼 있여야 함\n",
        "\n",
        "# 출력 메시지\n",
        "---------------------------------------------------------------------------\n",
        "ValueError                                Traceback (most recent call last)\n",
        "~\\AppData\\Local\\Temp\\ipykernel_22364\\934507019.py in <module>\n",
        "      1 Y_true = [[0,1,0], [1,0,1]]\n",
        "      2 Y_pred = [[0, 0.9, 0.1], [0.7, 0.8, 0.1]]\n",
        "----> 3 loss = keras.metrics.sparse_categorical_crossentropy(Y_true, Y_pred)\n",
        "      4 loss.shape\n",
        "\n",
        "~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py in error_handler(*args, **kwargs)\n",
        "    151     except Exception as e:\n",
        "    152       filtered_tb = _process_traceback_frames(e.__traceback__)\n",
        "--> 153       raise e.with_traceback(filtered_tb) from None\n",
        "    154     finally:\n",
        "    155       del filtered_tb\n",
        "\n",
        "~\\anaconda3\\lib\\site-packages\\keras\\losses.py in sparse_categorical_crossentropy(y_true, y_pred, from_logits, axis, ignore_class)\n",
        "   2082       Sparse categorical crossentropy loss value.\n",
        "   2083     \"\"\"\n",
        "-> 2084     return backend.sparse_categorical_crossentropy(\n",
        "   2085         y_true,\n",
        "   2086         y_pred,\n",
        "\n",
        "~\\anaconda3\\lib\\site-packages\\keras\\backend.py in sparse_categorical_crossentropy(target, output, from_logits, axis, ignore_class)\n",
        "   5632             )\n",
        "   5633     else:\n",
        "-> 5634         res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "   5635             labels=target, logits=output\n",
        "   5636         )\n",
        "\n",
        "ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(6,) and logits.shape=(2, 3)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tG0mCWaxoZg"
      },
      "outputs": [],
      "source": [
        "# Y값은 0에서 2사이의 값이 출력될 각각의 확률 3자리의 숫자로(One-hot Encoding 방식)으로 받을 시 출력\n",
        "Y_true = [[0,1,0], [0,0,1]]\n",
        "# 출력 Y는 0에서 2사이의 값이 출력될 각각의 확률 3자리의 숫자\n",
        "Y_pred = [[0, 0.9, 0.1], [0.6, 0.8, 0.1]]\n",
        "loss = keras.metrics.categorical_crossentropy(Y_true, Y_pred)\n",
        "loss"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "65c0Sx7xyQ9Z"
      },
      "source": [
        "| **다중분류 비용함수 종류** | **Y 입력형태** | **Y 출력형태** | **결론** |\n",
        "|:-----:|:-----:|:-----:|:-----:|\n",
        "| `sparse_categorical_crossentropy` | `[1, 2]` | `[[0, 0.9, 0.1], [0.6, 0.8, 0.1]]` | 입력이 `정수형태`일 때 사용 |\n",
        "| `categorical_crossentropy` | `[[0,1,0], [0,0,1]]` | `[[0, 0.9, 0.1], [0.6, 0.8, 0.1]]` | 입력이 `One-hot 벡터형태`일 때 사용 |\n",
        "\n",
        "- 어떤 비용함수를 사용하든 `정확성 성능은 동일`하지만, `sparse_categorical_crossentropy`를 사용하면 One-hot Encoding을 사용하지 않기 때문에 `메모리를 적게 사용하는 이점`"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "V3-8LCkTyh3V"
      },
      "source": [
        "## `One-hot Encoding?`\n",
        "\n",
        "**1) Binning(구간화):** `연속형 변수를 범주형` 변수로 변환\n",
        "\n",
        "- 숫자로 구성된 `연속형 값이 넓을 경우` 그룹을 지어 이해도를 높임\n",
        "\n",
        "- 변수의 선형적 특성 이외에 `비선형적 특성을 반영`\n",
        "\n",
        "**2) Label Encoding:** `범주형 변수`의 값들을 `숫자 값(레이블)`로 변경\n",
        "\n",
        "<img src='https://github.com/ahn283/finance/blob/main/time_series/img/Label_Encoding.png?raw=true' width=250>\n",
        "\n",
        "**3) Dummy Variable(가변수, $D_i$)**: `범주형 변수`를 `0 또는 1값`을 가진 `하나 이상의 새로운 변수`로 변경(One-hot Encoding)\n",
        "\n",
        "**생성법:** `계절변수`가 봄/여름/가을/겨울 이라는 값을 포함하는 경우, `계절_봄`, `계절_여름`, `계절_가을`, `계절_겨울` 총 4개의 변수를 생성\n",
        "\n",
        "(1) 범주형 변수의 `독립 값을 확인` (봄/여름/가을/겨울)\n",
        "\n",
        "(2) 독립 값의 `갯수만큼 더미변수`를 생성 ($D_1$ = 봄, $D_2$ = 여름, $D_3$ = 가을, $D_3$ = 겨울) \n",
        "\n",
        "  - *더미변수의 갯수는 최대 1개까지 줄일 수 있음*\n",
        "\n",
        "(3) 각 `더미변수들의 값`은 변수의 정의와 `같으면 1`이고 `나머지는 0`으로 채움   \n",
        "\n",
        "<img src='https://github.com/ahn283/finance/blob/main/time_series/img/Dummy_Engineering.png?raw=true' width=500>"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TOT0VMT_zmU8"
      },
      "source": [
        "## MLP\n",
        "\n",
        "<img src='https://github.com/ahn283/finance/blob/main/deep_learning/img/DL_MLP_Custom.PNG?raw=true' width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZjyorqpyH2O"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "tf.random.set_seed(1)\n",
        "KERNEL_SIZE = (3, 3)\n",
        "STRIDE = 1\n",
        "POOL_SIZE = (2, 2)\n",
        "POOL_STRIDE = 2\n",
        "PADDING = 'same'\n",
        "HIDDEN_ACTIVATION = 'relu'\n",
        "OUTPUT_ACTIVATION = 'softmax'\n",
        "DROPOUT_RATIO = 0.25\n",
        "LOSS = 'categorical_crossentropy'\n",
        "LEARNING_RATE = 0.01\n",
        "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "METRICS = ['accuracy']\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "VERBOSE = 1\n",
        "##########\n",
        "EARLYSTOP_PATIENT = 10\n",
        "MONITOR = 'val_loss'\n",
        "FILENAME = os.path.join(os.getcwd(), 'Model', 'Fashion_MNIST_MLP.h5')\n",
        "CALLBACK = [callbacks.EarlyStopping(monitor=MONITOR, mode='min',\n",
        "                                    patience=EARLYSTOP_PATIENT, verbose=VERBOSE),\n",
        "            tfa.callbacks.TQDMProgressBar(show_epoch_progress=False),\n",
        "            callbacks.ModelCheckpoint(monitor=MONITOR, mode='min', save_best_only=True,\n",
        "                                      filepath=FILENAME)]\n",
        "#########\n",
        "\n",
        "# train, test 로딩\n",
        "mnist = keras.datasets.fashion_mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# 데이터 스케일\n",
        "X_train = X_train.astype('float32')/255\n",
        "X_test = X_test.astype('float32')/255\n",
        "print('normalized X: ', X_train.shape, X_train.min(), X_train.max())\n",
        "print('normalized X: ', X_test.shape, X_test.min(), X_test.min())\n",
        "\n",
        "# Reshape\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "Y_train = np_utils.to_categorical(Y_train)\n",
        "Y_test = np_utils.to_categorical(Y_test)\n",
        "print('X_train: ', X_train.shape, 'Y_train: ', Y_train.shape)\n",
        "print('X_test: ', X_test.shape, 'Y_test: ', Y_test.shape)\n",
        "\n",
        "# Building network\n",
        "inputs = Input(shape=(X_train.shape[1],))   # scala가 아니고 vector 이기 때문에 (X_train[1], ) 형태\n",
        "hiddens = Dense(256, activation=HIDDEN_ACTIVATION)(inputs)\n",
        "hiddens = Dense(128, activation=HIDDEN_ACTIVATION)(hiddens)\n",
        "output = Dense(10, activation=OUTPUT_ACTIVATION)(hiddens)\n",
        "model = Model(inputs, output)\n",
        "model.summary()\n",
        "plot_model(model)\n",
        "\n",
        "# fit dataset\n",
        "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
        "model_fit = model.fit(X_train, Y_train, validation_split=0.2,\n",
        "                      batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE)\n",
        "plt.plot(pd.DataFrame(model_fit.history[METRICS[0]]))\n",
        "plt.plot(pd.DataFrame(model_fit.history['val_'+METRICS[0]]))\n",
        "plt.legend([METRICS[0], 'val_'+METRICS[0]])\n",
        "plt.show()\n",
        "\n",
        "# Train 예측 성능 평가하기\n",
        "model.evaluate(X_train, Y_train)\n",
        "\n",
        "# Test 예측 성능 평가하기\n",
        "model.evaluate(X_test, Y_test)\n",
        "print('\\nTest Confusion Matrix: ')\n",
        "pd.crosstab(np.argmax(Y_test, axis=1), np.argmax(model.predict(X_test), axis=1),\n",
        "            rownames=['True'], colnames=['Pred'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kuYcAdic973U"
      },
      "source": [
        "## CNN\n",
        "\n",
        "<img src='https://github.com/ahn283/finance/blob/main/machine_learning/img/CNN_Example.png?raw=true' width=900>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cQgzLe59EMF"
      },
      "outputs": [],
      "source": [
        "# 하이퍼파라미터\n",
        "tf.random.set_seed(1)\n",
        "KERNEL_SIZE = (3,3)\n",
        "STRIDE = 1\n",
        "POOL_SIZE = (2,2)\n",
        "POOL_STRIDE = 2\n",
        "PADDING = 'same'\n",
        "HIDDEN_ACTIVATION = 'relu'\n",
        "OUTPUT_ACTIVATION = 'softmax'\n",
        "DROPOUT_RATIO = 0.25\n",
        "LOSS = 'categorical_crossentropy'\n",
        "LEARNING_RATE = 0.01\n",
        "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "METRICS = ['accuracy']\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "VERBOSE = 1\n",
        "################\n",
        "EARLYSTOP_PATIENT = 10\n",
        "MONITOR = 'val_loss'\n",
        "FILENAME = os.path.join(os.getcwd(),'Model','Fashion_MNIST_CNN.h5')\n",
        "CALLBACK = [callbacks.EarlyStopping(monitor=MONITOR, mode='min', \n",
        "                                    patience=EARLYSTOP_PATIENT, verbose=1),\n",
        "            tfa.callbacks.TQDMProgressBar(show_epoch_progress=False),\n",
        "            callbacks.ModelCheckpoint(monitor=MONITOR, mode='min', save_best_only=True, \n",
        "                                      filepath=FILENAME)]\n",
        "################\n",
        "\n",
        "# Train, Test로 로딩\n",
        "mnist = keras.datasets.fashion_mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# 데이터 스케일링\n",
        "X_train = X_train.astype(\"float32\")/255\n",
        "X_test = X_test.astype(\"float32\")/255\n",
        "print('normalized X: ', X_train.shape, X_train.min(), X_train.max())\n",
        "print('normalized X: ', X_test.shape, X_test.min(), X_test.max())\n",
        "\n",
        "# Reshape\n",
        "X_train = X_train.reshape(-1, X_train.shape[1], X_train.shape[2], 1)\n",
        "X_test = X_test.reshape(-1, X_test.shape[1], X_train.shape[2], 1)\n",
        "## cross_entropy Loss를 쓰기 위해서는 Y도 갯수만큼 필요\n",
        "## to_cateogirucal 10자리로 변경\n",
        "Y_train = np_utils.to_categorical(Y_train)\n",
        "Y_test = np_utils.to_categorical(Y_test)\n",
        "print('X_train:', X_train.shape, 'Y_train:', Y_train.shape)\n",
        "print('X_test:', X_test.shape, 'Y_test:', Y_test.shape)\n",
        "\n",
        "# 네트워크 구축\n",
        "inputs = Input(shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]))   \n",
        "hiddens = Conv2D(128, kernel_size=KERNEL_SIZE, strides=STRIDE, padding=PADDING, \n",
        "                 activation=HIDDEN_ACTIVATION)(inputs)\n",
        "hiddens = MaxPooling2D(pool_size=POOL_SIZE, strides=POOL_STRIDE)(hiddens)\n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "hiddens = Conv2D(64, kernel_size=KERNEL_SIZE, strides=STRIDE, padding=PADDING, \n",
        "                 activation=HIDDEN_ACTIVATION)(hiddens)\n",
        "hiddens = MaxPooling2D(pool_size=POOL_SIZE, strides=POOL_STRIDE)(hiddens)\n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "hiddens = Flatten()(hiddens)\n",
        "hiddens = Dense(32, activation=HIDDEN_ACTIVATION)(hiddens)  \n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "output = Dense(10, activation=OUTPUT_ACTIVATION)(hiddens)  \n",
        "model = Model(inputs, output)  \n",
        "model.summary() \n",
        "plot_model(model) \n",
        "\n",
        "# 데이터 학습하기\n",
        "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
        "model_fit = model.fit(X_train, Y_train, validation_split=0.2,\n",
        "                      batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE,\n",
        "                      callbacks=CALLBACK)\n",
        "plt.plot(pd.DataFrame(model_fit.history[METRICS[0]]))\n",
        "plt.plot(pd.DataFrame(model_fit.history['val_'+METRICS[0]]))\n",
        "plt.legend([METRICS[0], 'val_'+METRICS[0]])\n",
        "plt.show()\n",
        "\n",
        "# Train 예측 성능 평가하기\n",
        "model.evaluate(X_train, Y_train)\n",
        "\n",
        "# Test 예측 성능 평가하기\n",
        "model.evaluate(X_test, Y_test)\n",
        "print('\\nTest Confusion Maxtrix: ')\n",
        "pd.crosstab(np.argmax(Y_test, axis=1), np.argmax(model.predict(X_test), axis=1),\n",
        "            rownames=['True'], colnames=['Pred'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7TnuAc2-rjl"
      },
      "outputs": [],
      "source": [
        "# 베스트 모델 로딩\n",
        "model = load_model(FILENAME)\n",
        "\n",
        "# 임의 데이터를 예측하고 정답과 비교하기\n",
        "idx_rand = np.random.randint(0, X_test.shape[0])\n",
        "print('목표값: ')\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_test[idx_rand], cmap='gray')\n",
        "plt.show()\n",
        "Y_label_list = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
        "print('예측값: ', Y_label_list[np.argmax(model.predict(X_test[idx_rand][np.newaxis,:]))])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BT1j_VH9-1Mp"
      },
      "source": [
        "## ResNet50\n",
        "\n",
        "<img src='https://github.com/ahn283/finance/blob/main/machine_learning/img/CNN_ResNet.png?raw=true' width=1000>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jz406YZS_AlB"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "tf.random.set_seed(1)\n",
        "KERNEL_SIZE = (3, 3)\n",
        "STRIDE = 1\n",
        "POOL_SIZE = (2, 2)\n",
        "PADDING = 'smae'\n",
        "HIDDEN_ACTIVATION = 'relu'\n",
        "OUTPUT_ACTIVATION = 'softmax'\n",
        "DROPOUT_RATIO = 0.25\n",
        "LOSS = 'categorical_crossentropy'\n",
        "LEARNING_RATE = 0.01\n",
        "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "METRICS = ['accuracy']\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "VERBOSE = 1\n",
        "################\n",
        "EARLYSTOP_PATIENT = 10\n",
        "MONITOR = 'val_loss'\n",
        "FILENAME = os.path.join(os.getcwd(),'Model','Fashion_MNIST_ResNet50.h5')\n",
        "CALLBACK = [callbacks.EarlyStopping(monitor=MONITOR, mode='min', \n",
        "                                    patience=EARLYSTOP_PATIENT, verbose=1),\n",
        "            tfa.callbacks.TQDMProgressBar(show_epoch_progress=False),\n",
        "            callbacks.ModelCheckpoint(monitor=MONITOR, mode='min', save_best_only=True, \n",
        "                                      filepath=FILENAME)]\n",
        "################\n",
        "\n",
        "# Train, Test로 로딩\n",
        "mnist = keras.datasets.fashion_mnist\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# 데이터 스케일링\n",
        "X_train = X_train.astype(\"float32\")/255\n",
        "X_test = X_test.astype(\"float32\")/255\n",
        "print('normalized X: ', X_train.shape, X_train.min(), X_train.max())\n",
        "print('normalized X: ', X_test.shape, X_test.min(), X_test.max())\n",
        "\n",
        "# Reshape\n",
        "X_train = X_train.reshape(-1, X_train.shape[1], X_train.shape[2], 1)\n",
        "X_test = X_test.reshape(-1, X_test.shape[1], X_train.shape[2], 1)\n",
        "Y_train = np_utils.to_categorical(Y_train)\n",
        "Y_test = np_utils.to_categorical(Y_test)\n",
        "print('X_train:', X_train.shape, 'Y_train:', Y_train.shape)\n",
        "print('X_test:', X_test.shape, 'Y_test:', Y_test.shape)\n",
        "\n",
        "# Building network\n",
        "inputs = Input(shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]))\n",
        "## ResNet50\n",
        "### include_top : 맨 탑층에 Fully connected layer 포함 여부\n",
        "### classes : Y_train.shape[1] 10개\n",
        "model = ResNet50(input_tensor=inputs, pooling='max', include_top=True,\n",
        "                 weights=None, classes=Y_train.shape[1])\n",
        "model.summary()\n",
        "plot_model(model)\n",
        "\n",
        "# fit dataset\n",
        "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
        "model_fit = model.fit(X_train, Y_train, validation_split=0.2,\n",
        "                      batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE,\n",
        "                      callbacks=CALLBACK)\n",
        "plt.plot(pd.DataFrame(model_fit.history[METRICS[0]]))\n",
        "plt.plot(pd.DataFrame(model_fit.history['val_'+METRICS[0]]))\n",
        "plt.show()\n",
        "\n",
        "# Train 예측 성능 평가하기\n",
        "model.evaluate(X_train, Y_train)\n",
        "\n",
        "# Test 예측 성능 평가하기\n",
        "model.evaluate(X_test, Y_test)\n",
        "print('\\nTest Confusion Maxtrix: ')\n",
        "pd.crosstab(np.argmax(Y_test, axis=1), np.argmax(model.predict(X_test), axis=1),\n",
        "            rownames=['True'], colnames=['Pred'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "712IXV5zCI7l"
      },
      "outputs": [],
      "source": [
        "# 베스트 모델 로딩\n",
        "model = load_model(FILENAME)\n",
        "\n",
        "# 임의 데이터를 예측하고 정답과 비교하기\n",
        "idx_rand = np.random.randint(0, X_test.shape[0])\n",
        "print('목표값: ')\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_test[idx_rand], cmap='gray')\n",
        "plt.show()\n",
        "Y_label_list = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
        "print('예측값: ', Y_label_list[np.argmax(model.predict(X_test[idx_rand][np.newaxis,:]))])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "UIUr5VYSCPOh"
      },
      "source": [
        "# **Problem:** 컬러사물이미지 분류"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tHvc5nnGCRhr"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p13eWoCbCTGu"
      },
      "outputs": [],
      "source": [
        "# Train, Test 로딩\n",
        "mnist = keras.datasets.cifar10\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "534JiHXamHz2"
      },
      "outputs": [],
      "source": [
        "# 데이터의 갯수와 크기 확인\n",
        "print('---------train----------')\n",
        "print('X: ', X_train.shape, X_train.min(), X_train.max())\n",
        "print('Y: ', Y_train.shape)\n",
        "print('---------test---------')\n",
        "print('X: ', X_test.shape, X_test.min(), X_test.max())\n",
        "print('Y: ', Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ujap6rBcmnA7"
      },
      "outputs": [],
      "source": [
        "# 샘플 데이터 시각화\n",
        "# 0에서 255 사이의 값을 가진 데이터 시각화\n",
        "for i in range(4):\n",
        "  plt.figure(figsize=(2,2))\n",
        "  plt.imshow(X_train[i])\n",
        "  plt.show()\n",
        "\n",
        "Y_label_list = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
        "                'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
        "print('Label of train: ', [Y_label_list[num] for num in Y_train.flatten()[:4]])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "heI10LIJnY60"
      },
      "source": [
        "## MLP\n",
        "\n",
        "<img src='https://github.com/ahn283/finance/blob/main/deep_learning/img/DL_MLP_Custom.PNG?raw=true' width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0mWT6ZinXVX"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "tf.random.set_seed(1)\n",
        "KERNEL_SIZE = (3, 3)\n",
        "STRIDE = 1\n",
        "POOL_SIZE = (2, 2)\n",
        "POOL_STRIDE = 2\n",
        "PADDING = 'same'\n",
        "HIDDEN_ACTIVATION = 'relu'    # vashing gradient 해결\n",
        "HIDDEN_ACTIVATION = 'softmax' # multilabel classification\n",
        "DROPOUT_RATE = 0.25\n",
        "LOSS = 'categorical_crossentropy' # multilabel classification\n",
        "LEARNING_RATE = 0.01\n",
        "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "METRICS = ['accuracy']\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "VERBOSE = 1\n",
        "##########\n",
        "EARLTSTOP_PATIENT = 10\n",
        "MONITOR = 'val_loss'\n",
        "FILENAME = os.path.join(os.getcwd(), 'Model', 'ColorObject_MNIST_MLP_h5')\n",
        "CALLBACK = [callbacks.EarlyStopping(monitor=MONITOR, mode='min',\n",
        "                                    patience=EARLYSTOP_PATIENT, verbose=1),\n",
        "            tfa.callbacks.TQDMProgressBar(show_epoch_progress=False),\n",
        "            callbacks.ModelCheckpoint(monitor=MONITOR, mode='min', save_best_only=True,\n",
        "                                      filepath=FILENAME)]\n",
        "##########\n",
        "\n",
        "# train, test 로딩\n",
        "mnist = keras.datasets.cifar10\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# 데이터 스케일링\n",
        "X_train = X_train.astype('float32')/255\n",
        "X_test = X_test.astype('float32')/255\n",
        "print('normalized X: ', X_train.shape, X_train.min(), X_train.max())\n",
        "print('normalized X: ', X_test.shape, X_test.min(), X_test.max())\n",
        "\n",
        "# Reshape\n",
        "X_train = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test = X_test.reshape(X_test.shape[0], -1)\n",
        "Y_train = np_utils.to_categorical(Y_train)\n",
        "Y_test = np_utils.to_categorical(Y_test)\n",
        "print('X_train: ', X_train.shape, 'Y_train: ', Y_train.shape)\n",
        "print('X_test: ', X_test.shape, 'Y_test: ', Y_test.shape)\n",
        "\n",
        "# Build network\n",
        "inputs = Input(shape=(X_train.shape[1], ))\n",
        "hiddens = Dense(256, activation=HIDDEN_ACTIVATION)(inputs)\n",
        "hiddens = Dense(128, activation=HIDDEN_ACTIVATION)(hiddens)\n",
        "output = Dense(10, activation=OUTPUT_ACTIVATION)(hiddens)\n",
        "\n",
        "# 네트워크 구축\n",
        "inputs = Input(shape=(X_train.shape[1],))   \n",
        "hiddens = Dense(256, activation=HIDDEN_ACTIVATION)(inputs)\n",
        "hiddens = Dense(128, activation=HIDDEN_ACTIVATION)(hiddens)\n",
        "output = Dense(10, activation=OUTPUT_ACTIVATION)(hiddens)  \n",
        "model = Model(inputs, output)  \n",
        "model.summary() \n",
        "plot_model(model) \n",
        "\n",
        "# 데이터 학습하기\n",
        "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
        "model_fit = model.fit(X_train, Y_train, validation_split=0.2,\n",
        "                      batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE)\n",
        "plt.plot(pd.DataFrame(model_fit.history[METRICS[0]]))\n",
        "plt.plot(pd.DataFrame(model_fit.history['val_'+METRICS[0]]))\n",
        "plt.legend([METRICS[0], 'val_'+METRICS[0]])\n",
        "plt.show()\n",
        "\n",
        "# Train 예측 성능 평가하기\n",
        "model.evaluate(X_train, Y_train)\n",
        "\n",
        "# Test 예측 성능 평가하기\n",
        "model.evaluate(X_test, Y_test)\n",
        "print('\\nTest Confusion Maxtrix: ')\n",
        "pd.crosstab(np.argmax(Y_test, axis=1), np.argmax(model.predict(X_test), axis=1),\n",
        "            rownames=['True'], colnames=['Pred'])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "M8RFTwoGvbZf"
      },
      "source": [
        "## CNN\n",
        "\n",
        "<img src='https://github.com/ahn283/finance/blob/main/machine_learning/img/CNN_Example.png?raw=true' width=900>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3tIhAGOvVu3"
      },
      "outputs": [],
      "source": [
        "# 하이퍼파라미터\n",
        "tf.random.set_seed(1)\n",
        "KERNEL_SIZE = (3,3)\n",
        "STRIDE = 1\n",
        "POOL_SIZE = (2,2)\n",
        "POOL_STRIDE = 2\n",
        "PADDING = 'same'\n",
        "HIDDEN_ACTIVATION = 'relu'\n",
        "OUTPUT_ACTIVATION = 'softmax'\n",
        "DROPOUT_RATIO = 0.25\n",
        "LOSS = 'categorical_crossentropy'\n",
        "LEARNING_RATE = 0.01\n",
        "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "METRICS = ['accuracy']\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "VERBOSE = 1\n",
        "################\n",
        "EARLYSTOP_PATIENT = 10\n",
        "MONITOR = 'val_loss'\n",
        "FILENAME = os.path.join(os.getcwd(),'Model','ColorObject_MNIST_CNN.h5')\n",
        "CALLBACK = [callbacks.EarlyStopping(monitor=MONITOR, mode='min', \n",
        "                                    patience=EARLYSTOP_PATIENT, verbose=1),\n",
        "            tfa.callbacks.TQDMProgressBar(show_epoch_progress=False),\n",
        "            callbacks.ModelCheckpoint(monitor=MONITOR, mode='min', save_best_only=True, \n",
        "                                      filepath=FILENAME)]\n",
        "################\n",
        "\n",
        "# Train, Test로 로딩\n",
        "mnist = keras.datasets.cifar10\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# 데이터 스케일링\n",
        "X_train = X_train.astype(\"float32\")/255\n",
        "X_test = X_test.astype(\"float32\")/255\n",
        "print('normalized X: ', X_train.shape, X_train.min(), X_train.max())\n",
        "print('normalized X: ', X_test.shape, X_test.min(), X_test.max())\n",
        "\n",
        "# Reshape\n",
        "Y_train = np_utils.to_categorical(Y_train)\n",
        "Y_test = np_utils.to_categorical(Y_test)\n",
        "print('X_train:', X_train.shape, 'Y_train:', Y_train.shape)\n",
        "print('X_test:', X_test.shape, 'Y_test:', Y_test.shape)\n",
        "\n",
        "# 네트워크 구축\n",
        "inputs = Input(shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]))   \n",
        "hiddens = Conv2D(128, kernel_size=KERNEL_SIZE, strides=STRIDE, padding=PADDING, \n",
        "                 activation=HIDDEN_ACTIVATION)(inputs)\n",
        "hiddens = MaxPooling2D(pool_size=POOL_SIZE, strides=POOL_STRIDE)(hiddens)\n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "hiddens = Conv2D(64, kernel_size=KERNEL_SIZE, strides=STRIDE, padding=PADDING, \n",
        "                 activation=HIDDEN_ACTIVATION)(hiddens)\n",
        "hiddens = MaxPooling2D(pool_size=POOL_SIZE, strides=POOL_STRIDE)(hiddens)\n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "hiddens = Flatten()(hiddens)\n",
        "hiddens = Dense(32, activation=HIDDEN_ACTIVATION)(hiddens)  \n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "output = Dense(10, activation=OUTPUT_ACTIVATION)(hiddens)  \n",
        "model = Model(inputs, output)  \n",
        "model.summary() \n",
        "plot_model(model) \n",
        "\n",
        "# 데이터 학습하기\n",
        "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
        "model_fit = model.fit(X_train, Y_train, validation_split=0.2,\n",
        "                      batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE,\n",
        "                      callbacks=CALLBACK)\n",
        "plt.plot(pd.DataFrame(model_fit.history[METRICS[0]]))\n",
        "plt.plot(pd.DataFrame(model_fit.history['val_'+METRICS[0]]))\n",
        "plt.legend([METRICS[0], 'val_'+METRICS[0]])\n",
        "plt.show()\n",
        "\n",
        "# Train 예측 성능 평가하기\n",
        "model.evaluate(X_train, Y_train)\n",
        "\n",
        "# Test 예측 성능 평가하기\n",
        "model.evaluate(X_test, Y_test)\n",
        "print('\\nTest Confusion Maxtrix: ')\n",
        "pd.crosstab(np.argmax(Y_test, axis=1), np.argmax(model.predict(X_test), axis=1),\n",
        "            rownames=['True'], colnames=['Pred'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8nKnwY-v75U"
      },
      "outputs": [],
      "source": [
        "# 베스트 모델 로딩\n",
        "model = load_model(FILENAME)\n",
        "\n",
        "# 임의 데이터를 예측하고 정답과 비교하기\n",
        "idx_rand = np.random.randint(0, X_test.shape[0])\n",
        "print('목표값: ')\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_test[idx_rand], cmap='gray')\n",
        "plt.show()\n",
        "Y_label_list = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
        "                'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
        "print('예측값: ', Y_label_list[np.argmax(model.predict(X_test[idx_rand][np.newaxis,:]))])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PYOx96XVv_pW"
      },
      "source": [
        "## ResNet50\n",
        "\n",
        "<img src='https://github.com/ahn283/finance/blob/main/machine_learning/img/CNN_ResNet.png?raw=true' width=1000>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Re10PbTdwIxW"
      },
      "outputs": [],
      "source": [
        "# 하이퍼파라미터\n",
        "tf.random.set_seed(1)\n",
        "KERNEL_SIZE = (3,3)\n",
        "STRIDE = 1\n",
        "POOL_SIZE = (2,2)\n",
        "POOL_STRIDE = 2\n",
        "PADDING = 'same'\n",
        "HIDDEN_ACTIVATION = 'relu'\n",
        "OUTPUT_ACTIVATION = 'softmax'\n",
        "DROPOUT_RATIO = 0.25\n",
        "LOSS = 'categorical_crossentropy'\n",
        "LEARNING_RATE = 0.01\n",
        "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "METRICS = ['accuracy']\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 30\n",
        "VERBOSE = 1\n",
        "################\n",
        "EARLYSTOP_PATIENT = 10\n",
        "MONITOR = 'val_loss'\n",
        "FILENAME = os.path.join(os.getcwd(),'Model','ColorObject_MNIST_ResNet50.h5')\n",
        "CALLBACK = [callbacks.EarlyStopping(monitor=MONITOR, mode='min', \n",
        "                                    patience=EARLYSTOP_PATIENT, verbose=1),\n",
        "            tfa.callbacks.TQDMProgressBar(show_epoch_progress=False),\n",
        "            callbacks.ModelCheckpoint(monitor=MONITOR, mode='min', save_best_only=True, \n",
        "                                      filepath=FILENAME)]\n",
        "################\n",
        "\n",
        "# Train, Test로 로딩\n",
        "mnist = keras.datasets.cifar10\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "\n",
        "# 데이터 스케일링\n",
        "X_train = X_train.astype(\"float32\")/255\n",
        "X_test = X_test.astype(\"float32\")/255\n",
        "print('normalized X: ', X_train.shape, X_train.min(), X_train.max())\n",
        "print('normalized X: ', X_test.shape, X_test.min(), X_test.max())\n",
        "\n",
        "# Reshape\n",
        "Y_train = np_utils.to_categorical(Y_train)\n",
        "Y_test = np_utils.to_categorical(Y_test)\n",
        "print('X_train:', X_train.shape, 'Y_train:', Y_train.shape)\n",
        "print('X_test:', X_test.shape, 'Y_test:', Y_test.shape)\n",
        "\n",
        "\n",
        "# Build network\n",
        "inputs = Input(shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]))\n",
        "model = ResNet50(input_tensor=inputs, pooling='max', include_top=True,\n",
        "                 weights=None, classes=Y_train.shape[1])\n",
        "model.summary()\n",
        "plot_model(model)\n",
        "\n",
        "# 데이터 학습하기\n",
        "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
        "model_fit = model.fit(X_train, Y_train, validation_split=0.2,\n",
        "                      batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE,\n",
        "                      callbacks=CALLBACK)\n",
        "plt.plot(pd.DataFrame(model_fit.history[METRICS[0]]))\n",
        "plt.plot(pd.DataFrame(model_fit.history['val_'+METRICS[0]]))\n",
        "plt.legend([METRICS[0], 'val_'+METRICS[0]])\n",
        "plt.show()\n",
        "\n",
        "# Train 예측 성능 평가하기\n",
        "model.evaluate(X_train, Y_train)\n",
        "\n",
        "# Test 예측 성능 평가하기\n",
        "model.evaluate(X_test, Y_test)\n",
        "print('\\nTest Confusion Maxtrix: ')\n",
        "pd.crosstab(np.argmax(Y_test, axis=1), np.argmax(model.predict(X_test), axis=1),\n",
        "            rownames=['True'], colnames=['Pred'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Aalghp9w7K6"
      },
      "outputs": [],
      "source": [
        "# 베스트 모델 로딩\n",
        "model = load_model(FILENAME)\n",
        "\n",
        "# 임의 데이터를 예측하고 정답과 비교하기\n",
        "idx_rand = np.random.randint(0, X_test.shape[0])\n",
        "print('목표값: ')\n",
        "plt.figure(figsize=(2,2))\n",
        "plt.imshow(X_test[idx_rand], cmap='gray')\n",
        "plt.show()\n",
        "Y_label_list = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer',\n",
        "                'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
        "print('예측값: ', Y_label_list[np.argmax(model.predict(X_test[idx_rand][np.newaxis,:]))])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "P9dYkzGkw-tB"
      },
      "source": [
        "# **Problem:** 보스턴 집값 예측\n",
        "\n",
        "|   **변수**  |              **설명**              |\n",
        "|-----|-----|\n",
        "|  **Price**  |            주택 가격           |\n",
        "|   **CRIM**  |        1인당 범죄 발생률       |\n",
        "|    **ZN**   |       주택용 토지의 비율       |\n",
        "|  **INDUS**  |      비소매 상업지역 비율      |\n",
        "|   **CHAS**  |    찰스강 인접 여부(인접=1)    |\n",
        "|   **NOX**   |         일산화질소 농도        |\n",
        "|    **RM**   |            방의 개수           |\n",
        "|   **AGE**   | 1940년 이전에 건축된 주택 비율 |\n",
        "|   **DIS**   |     주요 5대 회사와의 거리     |\n",
        "|   **RAD**   |         고속도로 접근성        |\n",
        "|   **TAX**   |             재산세             |\n",
        "| **PTRATIO** |       교사와 학생의 비율       |\n",
        "|    **B**    |         흑인 거주 비율         |\n",
        "|  **LSTAT**  |          저소득층 비율         |"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hNhRyd8BxJAW"
      },
      "source": [
        "## MLP\n",
        "\n",
        "<img src='https://github.com/ahn283/finance/blob/main/deep_learning/img/DL_MLP_Custom.PNG?raw=true'\n",
        " width=500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# module regression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "import statsmodels.api as sm \n",
        "from scipy import stats \n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "\n",
        "\n",
        "### Boston Housing Price\n",
        "# 자료형 변환 + 결측값 처리\n",
        "def prep(df_origin):\n",
        "    df = df_origin.copy()\n",
        "    \n",
        "    # 자료형 변환\n",
        "    col = []\n",
        "    if df['ZN'].dtype == 'object':\n",
        "        for i in df['ZN']:\n",
        "            col.append(float(i[1:-1]))\n",
        "        df['ZN'] = col\n",
        "\n",
        "    col = []\n",
        "    if df['CHAS'].dtype == 'object':\n",
        "        for i in df['CHAS']:\n",
        "            col.append(float(i[1:-1]))\n",
        "        df['CHAS'] = col\n",
        "\n",
        "    # 결측값 처리\n",
        "    for i in df.columns[df.isnull().sum() != 0]:\n",
        "        if i not in ['INDUS', 'RM']:\n",
        "            df[i].fillna(df[i].mean(), inplace=True)\n",
        "        else:\n",
        "            df[i].fillna(df[i].median(), inplace=True)\n",
        "            \n",
        "    return df\n",
        "\n",
        "# 데이터 변환\n",
        "def feature_engineering(df_origin):\n",
        "    df = df_origin.copy()\n",
        "    \n",
        "    interval = [100, 200, 300, 400, 500, 600, 700, 800]\n",
        "    if df['TAX'].max() >= 100:\n",
        "        df['TAX'] = np.digitize(df['TAX'], bins=interval)\n",
        "        \n",
        "    if 'TAX' in df.columns:\n",
        "        df_dummy = pd.get_dummies(df['TAX'], prefix='TAX', drop_first=True)\n",
        "        df = pd.concat([df, df_dummy], axis=1)\n",
        "        del df['TAX']\n",
        "\n",
        "    if 'CHAS' in df.columns:\n",
        "        df['CHAS'] = df['CHAS'].astype(int)\n",
        "        df_dummy = pd.get_dummies(df['CHAS'], prefix='CHAS', drop_first=False)\n",
        "        df = pd.concat([df, df_dummy], axis=1)\n",
        "        del df['CHAS']\n",
        "    \n",
        "    return df\n",
        "\n",
        "\n",
        "# 데이터 분리\n",
        "def datasplit(df, Y_colname, test_size=0.2, random_state=123):\n",
        "    X_colname = [x for x in df.columns if x not in Y_colname]\n",
        "       \n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(df[X_colname], df[Y_colname],\n",
        "                                                        test_size=test_size, random_state=random_state)\n",
        "    print(X_train.shape, Y_train.shape)\n",
        "    print(X_test.shape, Y_test.shape)\n",
        "    \n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "# 데이터 변환 후 X_train, X_test의 변수 갯수 일치\n",
        "def col_mapping(X_train, X_test):\n",
        "    X_tr = X_train.copy()\n",
        "    X_te = X_test.copy()\n",
        "    \n",
        "    # Train & Test 변수명 체크\n",
        "    X_te_noncol = [i for i in X_tr.columns if i not in X_te.columns]\n",
        "    X_tr_noncol = [i for i in X_te.columns if i not in X_tr.columns]\n",
        "\n",
        "    # 변수 갯수 일치\n",
        "    if X_te_noncol != []:\n",
        "        for i in X_te_noncol:\n",
        "            X_te[i] = 0\n",
        "            X_te = X_te[X_tr.columns].copy()\n",
        "            \n",
        "    if X_tr_noncol != []:\n",
        "        for i in X_tr_noncol:\n",
        "            X_tr[i] = 0\n",
        "            X_tr = X_tr[X_te.columns].copy()\n",
        "            \n",
        "    return X_tr, X_te\n",
        "\n",
        "# 스케쥴 조정\n",
        "def scale(scaler, X_train, X_test):\n",
        "    scaler_fit = scaler.fit(X_train)\n",
        "    X_train_scaling = pd.DataFrame(scaler_fit.transform(X_train), \n",
        "                                   index=X_train.index, columns=X_train.columns)\n",
        "    X_test_scaling = pd.DataFrame(scaler_fit.transform(X_test), \n",
        "                                  index=X_test.index, columns=X_test.columns)\n",
        "    \n",
        "    return X_train_scaling, X_test_scaling\n",
        "\n",
        "# 실제 Y와 예측치 시각화\n",
        "def plot_prediction(Y_true_pred):\n",
        "    plt.figure(figsize=(16, 8))\n",
        "    plt.plot(Y_true_pred, linewidth=5, label=Y_true_pred.columns)\n",
        "    plt.xticks(fontsize=25, rotation=0)\n",
        "    plt.yticks(fontsize=25)\n",
        "    plt.xlabel('Index', fontname='serif', fontsize=28)\n",
        "    plt.legend(fontsize=20)\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# 검증 함수화\n",
        "def evaluation_reg(Y_real, Y_pred):\n",
        "    MAE = mean_absolute_error(Y_real, Y_pred)\n",
        "    MSE = mean_squared_error(Y_real, Y_pred)\n",
        "    MAPE = mean_absolute_percentage_error(Y_real, Y_pred)\n",
        "    Score = pd.DataFrame([MAE, MSE, MAPE], index=['MAE', 'MSE', 'MAPE'], columns=['Score']).T\n",
        "    \n",
        "    return Score\n",
        "\n",
        "# Train & Test 모두의 검증 함수화\n",
        "def evaluation_reg_trte(Y_real_tr, Y_pred_tr, Y_real_te, Y_pred_te):\n",
        "    Score_tr = evaluation_reg(Y_real_tr, Y_pred_tr)\n",
        "    Score_te = evaluation_reg(Y_real_te, Y_pred_te)\n",
        "    Score_trte = pd.concat([Score_tr, Score_te], axis=0)\n",
        "    Score_trte.index = ['Train', 'Test']\n",
        "\n",
        "    return Score_trte\n",
        "\n",
        "# 에러 분석\n",
        "def error_analysis(X_Data, Y_Pred, Residual, graph_on=False):\n",
        "    if graph_on == True:\n",
        "        ##### 시각화\n",
        "        # 잔차의 정규본포성 확인\n",
        "        # sns.displot(Residual, norm_hist='True', fit=stats.norm)\n",
        "        sns.displot(Residual, kind='hist')\n",
        "        plt.show()\n",
        "\n",
        "        # 잔차의 등분산성 확인\n",
        "        temp = pd.concat([Y_Pred, Residual.reset_index().iloc[:,[1]]], axis=1)\n",
        "        sns.scatterplot(x='Pred', y='Error', data=temp)\n",
        "        plt.show()\n",
        "        \n",
        "        # 잔차의 자기상관성 확인\n",
        "        sm.graphics.tsa.plot_acf(Residual, lags=50, use_vlines=True)\n",
        "        plt.show()\n",
        "\n",
        "    ##### 통계량\n",
        "    # 정규분포\n",
        "    # Null Hypothesis: The residuals are normally distributed\n",
        "    Normality = pd.DataFrame([stats.shapiro(Residual)], \n",
        "                             index=['Normality'], columns=['Test Statistics', 'p-value']).T\n",
        "\n",
        "    # 등분산성\n",
        "    # Null Hypothesis: Error terms are homoscedastic\n",
        "    Heteroscedasticity = pd.DataFrame([sm.stats.diagnostic.het_goldfeldquandt(Residual, X_Data.values, alternative='two-sided')],\n",
        "                                      index=['Heteroscedasticity'], \n",
        "                                      columns=['Test Statistics', 'p-value', 'Alternative']).T\n",
        "    \n",
        "    # 자기상관\n",
        "    # Null Hypothesis: Autocorrelation is absent\n",
        "    Autocorrelation = pd.concat([pd.DataFrame(sm.stats.diagnostic.acorr_ljungbox(Residual, lags=[10,50]).iloc[:,0]),\n",
        "                             pd.DataFrame(sm.stats.diagnostic.acorr_ljungbox(Residual, lags=[10,50]).iloc[:,1])], axis=1).T\n",
        "    Autocorrelation.index = ['Test Statistics', 'p-value']\n",
        "    Autocorrelation.columns = ['Autocorr(lag10)', 'Autocorr(lag50)']\n",
        "    \n",
        "    Error_Analysis = pd.concat([Normality, Heteroscedasticity, Autocorrelation], join='outer', axis=1)\n",
        "    \n",
        "    return Error_Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBDgj_uBmqiT"
      },
      "outputs": [],
      "source": [
        "# 하이퍼파라미터\n",
        "tf.random.set_seed(1)\n",
        "KERNEL_SIZE = (3,3)\n",
        "STRIDE = 1\n",
        "POOL_SIZE = (2,2)\n",
        "POOL_STRIDE = 2\n",
        "PADDING = 'same'\n",
        "HIDDEN_ACTIVATION = 'relu'\n",
        "OUTPUT_ACTIVATION = 'linear'\n",
        "REGULARIZER = regularizers.l2(0.01)\n",
        "DROPOUT_RATIO = 0.25\n",
        "LOSS = 'mse'\n",
        "LEARNING_RATE = 0.01\n",
        "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "METRICS = ['mse']\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 500\n",
        "VERBOSE = 0\n",
        "################\n",
        "EARLYSTOP_PATIENT = 10\n",
        "MONITOR = 'val_loss'\n",
        "FILENAME = os.path.join(os.getcwd(),'Model','BostonHousing_MLP.h5')\n",
        "CALLBACK = [callbacks.EarlyStopping(monitor=MONITOR, mode='min', \n",
        "                                    patience=EARLYSTOP_PATIENT, verbose=1),\n",
        "            tfa.callbacks.TQDMProgressBar(show_epoch_progress=False),\n",
        "            callbacks.ModelCheckpoint(monitor=MONITOR, mode='min', save_best_only=True, \n",
        "                                      filepath=FILENAME)]\n",
        "################\n",
        "\n",
        "\n",
        "# 데이터 전처리\n",
        "location = 'https://raw.githubusercontent.com/ahn283/finance/main/deep_learning/data/Price_Modify.csv'\n",
        "df = pd.read_csv(location)\n",
        "df = prep(df)\n",
        "\n",
        "Y_colname = ['Price']\n",
        "X_train, X_test, Y_train, Y_test = datasplit(df, Y_colname)\n",
        "\n",
        "X_train_fe = feature_engineering(X_train)\n",
        "X_test_fe = feature_engineering(X_test)\n",
        "X_train_fe, X_test_fe = col_mapping(X_train_fe, X_test_fe)   \n",
        "\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "X_train_fes, X_test_fes = scale(scaler, X_train_fe, X_test_fe)\n",
        "\n",
        "# 네트워크 구축\n",
        "inputs = Input(shape=(X_train_fes.shape[1],))   \n",
        "hiddens = Dense(256, activation=HIDDEN_ACTIVATION, kernel_regularizer=REGULARIZER)(inputs)\n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "hiddens = Dense(128, activation=HIDDEN_ACTIVATION, kernel_regularizer=REGULARIZER)(hiddens)\n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "hiddens = Dense(64, activation=HIDDEN_ACTIVATION, kernel_regularizer=REGULARIZER)(hiddens)\n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "hiddens = Dense(32, activation=HIDDEN_ACTIVATION, kernel_regularizer=REGULARIZER)(hiddens)\n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "output = Dense(1, activation=OUTPUT_ACTIVATION)(hiddens)  \n",
        "model = Model(inputs, output)  \n",
        "model.summary() \n",
        "plot_model(model) \n",
        "\n",
        "## 데이터 학습하기\n",
        "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
        "model_fit = model.fit(X_train_fes, Y_train, validation_split=0.2,\n",
        "                      batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE,\n",
        "                      callbacks=CALLBACK)\n",
        "plt.plot(pd.DataFrame(model_fit.history[METRICS[0]]))\n",
        "plt.plot(pd.DataFrame(model_fit.history['val_'+METRICS[0]]))\n",
        "plt.legend([METRICS[0], 'val_'+METRICS[0]])\n",
        "plt.show()\n",
        "\n",
        "## 예측하기\n",
        "Y_trpred = pd.DataFrame(model.predict(X_train_fes), \n",
        "                        index=Y_train.index, columns=['Pred'])\n",
        "Y_tepred = pd.DataFrame(model.predict(X_test_fes), \n",
        "                        index=Y_test.index, columns=['Pred'])\n",
        "plot_prediction(pd.concat([Y_train, Y_trpred], axis=1).reset_index().iloc[:,1:])\n",
        "plot_prediction(pd.concat([Y_test, Y_tepred], axis=1).reset_index().iloc[:,1:])\n",
        "\n",
        "# 분석 검증\n",
        "Score_nn_early = evaluation_reg_trte(Y_train, Y_trpred, Y_test, Y_tepred)\n",
        "display(Score_nn_early)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qchexG7p-DxZ"
      },
      "outputs": [],
      "source": [
        "# 베스트 모델 로딩\n",
        "model = load_model(FILENAME)\n",
        "\n",
        "## 예측하기\n",
        "Y_trpred = pd.DataFrame(model.predict(X_train_fes), \n",
        "                        index=Y_train.index, columns=['Pred'])\n",
        "Y_tepred = pd.DataFrame(model.predict(X_test_fes), \n",
        "                        index=Y_test.index, columns=['Pred'])\n",
        "plot_prediction(pd.concat([Y_train, Y_trpred], axis=1).reset_index().iloc[:,1:])\n",
        "plot_prediction(pd.concat([Y_test, Y_tepred], axis=1).reset_index().iloc[:,1:])\n",
        "\n",
        "# 분석 검증\n",
        "Score_nn_early = evaluation_reg_trte(Y_train, Y_trpred, Y_test, Y_tepred)\n",
        "display(Score_nn_early)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PI0127u08CPq"
      },
      "source": [
        "## CNN\n",
        "\n",
        "<img src='https://github.com/ahn283/finance/blob/main/machine_learning/img/CNN_Example.png?raw=true' width=300>\n",
        "\n",
        "<img src='https://github.com/ahn283/finance/blob/main/machine_learning/img/Conv1D2D3D.png?raw=true' width=800>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6hiL7R38bH0"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "tf.random.set_seed(1)\n",
        "KERNEL_SIZE = 3\n",
        "STRIDE = 1\n",
        "POOL_SIZE = 2\n",
        "POOL_STRIDE = 2\n",
        "PADDING = 'same'\n",
        "HIDDEN_ACTIVATION = 'relu'\n",
        "OUTPUT_ACTIVATION = 'linear'\n",
        "REGULARIZER = regularizers.l2(0.01)\n",
        "DROPOUT_RATIO = 0.25\n",
        "LOSS = 'mse'\n",
        "LEARNING_RATE = 0.01\n",
        "OPTIMIZER = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "METRICS = ['mse']\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 500\n",
        "VERBOSE = 0\n",
        "################\n",
        "EARLYSTOP_PATIENT = 10\n",
        "MONITOR = 'val_loss'\n",
        "FILENAME = os.path.join(os.getcwd(),'Model','BostonHousing_CNN.h5')\n",
        "CALLBACK = [callbacks.EarlyStopping(monitor=MONITOR, mode='min', \n",
        "                                    patience=EARLYSTOP_PATIENT, verbose=1),\n",
        "            tfa.callbacks.TQDMProgressBar(show_epoch_progress=False),\n",
        "            callbacks.ModelCheckpoint(monitor=MONITOR, mode='min', save_best_only=True, \n",
        "                                      filepath=FILENAME)]\n",
        "################\n",
        "\n",
        "# 데이터 로딩\n",
        "df = pd.read_csv(location)\n",
        "\n",
        "# 데이터 전처리\n",
        "df = prep(df)\n",
        "\n",
        "Y_colname = ['Price']\n",
        "X_train, X_test, Y_train, Y_test = datasplit(df, Y_colname)\n",
        "\n",
        "X_train_fe = feature_engineering(X_train)\n",
        "X_test_fe = feature_engineering(X_test)\n",
        "X_train_fe, X_test_fe = col_mapping(X_train_fe, X_test_fe)   \n",
        "\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "X_train_fes, X_test_fes = scale(scaler, X_train_fe, X_test_fe)\n",
        "\n",
        "# Reshape\n",
        "X_train = X_train_fes.values.reshape(-1, X_train_fes.shape[1], 1)\n",
        "X_test = X_test_fes.values.reshape(-1, X_test_fes.shape[1], 1)\n",
        "print('X_train:', X_train.shape, 'Y_train:', Y_train.shape)\n",
        "print('X_test:', X_test.shape, 'Y_test:', Y_test.shape)\n",
        "\n",
        "# 네트워크 구축\n",
        "inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))   \n",
        "## 2차원 데이터 (width(feature_map), channel) -> Conv1D\n",
        "hiddens = Conv1D(128, kernel_size=KERNEL_SIZE, strides=STRIDE, padding=PADDING, \n",
        "                 activation=HIDDEN_ACTIVATION)(inputs)\n",
        "hiddens = MaxPooling1D(pool_size=POOL_SIZE, strides=POOL_STRIDE)(hiddens)\n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "hiddens = Conv1D(64, kernel_size=KERNEL_SIZE, strides=STRIDE, padding=PADDING, \n",
        "                 activation=HIDDEN_ACTIVATION)(hiddens)\n",
        "hiddens = MaxPooling1D(pool_size=POOL_SIZE, strides=POOL_STRIDE)(hiddens)\n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "hiddens = Flatten()(hiddens)\n",
        "hiddens = Dense(32, activation=HIDDEN_ACTIVATION)(hiddens)  \n",
        "hiddens = Dropout(DROPOUT_RATIO)(hiddens)\n",
        "output = Dense(1, activation=OUTPUT_ACTIVATION)(hiddens)  \n",
        "model = Model(inputs, output)  \n",
        "model.summary() \n",
        "plot_model(model) \n",
        "\n",
        "## 데이터 학습하기\n",
        "model.compile(loss=LOSS, optimizer=OPTIMIZER, metrics=METRICS)\n",
        "model_fit = model.fit(X_train, Y_train, validation_split=0.2,\n",
        "                      batch_size=BATCH_SIZE, epochs=EPOCHS, verbose=VERBOSE,\n",
        "                      callbacks=CALLBACK)\n",
        "plt.plot(pd.DataFrame(model_fit.history[METRICS[0]]))\n",
        "plt.plot(pd.DataFrame(model_fit.history['val_'+METRICS[0]]))\n",
        "plt.legend([METRICS[0], 'val_'+METRICS[0]])\n",
        "plt.show()\n",
        "\n",
        "## 예측하기\n",
        "Y_trpred = pd.DataFrame(model.predict(X_train), \n",
        "                        index=Y_train.index, columns=['Pred'])\n",
        "Y_tepred = pd.DataFrame(model.predict(X_test), \n",
        "                        index=Y_test.index, columns=['Pred'])\n",
        "plot_prediction(pd.concat([Y_train, Y_trpred], axis=1).reset_index().iloc[:,1:])\n",
        "plot_prediction(pd.concat([Y_test, Y_tepred], axis=1).reset_index().iloc[:,1:])\n",
        "\n",
        "# 분석 검증\n",
        "Score_nn_early = evaluation_reg_trte(Y_train, Y_trpred, Y_test, Y_tepred)\n",
        "display(Score_nn_early)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jg5uP8X9UFT"
      },
      "outputs": [],
      "source": [
        "# 베스트 모델 로딩\n",
        "model = load_model(FILENAME)\n",
        "\n",
        "## 예측하기\n",
        "Y_trpred = pd.DataFrame(model.predict(X_train), \n",
        "                        index=Y_train.index, columns=['Pred'])\n",
        "Y_tepred = pd.DataFrame(model.predict(X_test), \n",
        "                        index=Y_test.index, columns=['Pred'])\n",
        "plot_prediction(pd.concat([Y_train, Y_trpred], axis=1).reset_index().iloc[:,1:])\n",
        "plot_prediction(pd.concat([Y_test, Y_tepred], axis=1).reset_index().iloc[:,1:])\n",
        "\n",
        "# 분석 검증\n",
        "Score_nn_early = evaluation_reg_trte(Y_train, Y_trpred, Y_test, Y_tepred)\n",
        "display(Score_nn_early)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "authorship_tag": "ABX9TyMsuVSXewVLyOh7HF9opO8F",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
