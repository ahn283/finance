{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **딥러닝 알고리즘의 발전(2010~)**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 위기극복의 계기\n",
    "\n",
    "- **NN의 두번째위기:** (1) `Vanishing Gradients`, (2) `Local Minimum`, (3) Low Learning Time, (4) Curse of Dimensionality\n",
    "\n",
    "- **\"RBM 이외에도, `Supervised Learning 문제에서의 아이디어로` 해법 개발 노력\"**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Vanishing Gradients의 해결:** `활성화 함수 ReLU`\n",
    "\n",
    "<img src='./img/DL_ActivationFunction_Type.png' width='700'>\n",
    "\n",
    "- `Sigmoid`는 신경망이 깊어지면 여전히 `비용함수 최적화가 어려운 이슈` 발견\n",
    "\n",
    "- `다양한 미분가능한 비선형 활성화 함수` 필요성이 증가했고 `ReLU`가 Vanishing Gradient의 문제를 해결(Hinton et al. 2010)\n",
    "\n",
    "- 기존 활성화 함수는 양끝의 기울기가 0이 되는 이슈가 있었으나, ReLU는 `기울기가 0으로 감소하는 현상이 없고 일반적으로 학습성능도 향상`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
