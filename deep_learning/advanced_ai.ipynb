{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **딥러닝 알고리즘의 발전(2010~)**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 위기극복의 계기\n",
    "\n",
    "- **NN의 두번째위기:** (1) `Vanishing Gradients`, (2) `Local Minimum`, (3) Low Learning Time, (4) Curse of Dimensionality\n",
    "\n",
    "- **\"RBM 이외에도, `Supervised Learning 문제에서의 아이디어로` 해법 개발 노력\"**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Vanishing Gradients의 해결:** `활성화 함수 ReLU`\n",
    "\n",
    "<img src='./img/DL_ActivationFunction_Type.png' width='700'>\n",
    "\n",
    "- `Sigmoid`는 신경망이 깊어지면 여전히 `비용함수 최적화가 어려운 이슈` 발견\n",
    "\n",
    "- `다양한 미분가능한 비선형 활성화 함수` 필요성이 증가했고 `ReLU`가 Vanishing Gradient의 문제를 해결(Hinton et al. 2010)\n",
    "\n",
    "- 기존 활성화 함수는 양끝의 기울기가 0이 되는 이슈가 있었으나, ReLU는 `기울기가 0으로 감소하는 현상이 없고 일반적으로 학습성능도 향상`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Local Minimum의 해결 : `Global Minimum` $\\approx$ `Local Minimum`\n",
    "\n",
    "- `고차원 데이터나 Non-convex 형태의 비용함수에서의 최적화`를 하더라도 Local Minimum들은 서로 유사할 것이며 Global Minimum과 큰 차이가 없을 것(Bengio et al. 2014)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가중치 추정 최적화\n",
    "\n",
    "**\"`비용함수가 가능한 낮은 가중치`를 찾아가는 과정인 `최적화가 중요해짐!`\"** \n",
    "\n",
    "- **Prerequisite:** Derivatives, Partial Derivatives, Chain Rule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 추정 프로세스\n",
    "\n",
    "<img src='./img/DNN_Process.PNG' width='700'>(https://www.inf.ufpr.br/todt/IAaplicada/CNN_Presentation.pdf)\n",
    "\n",
    "**(1) 네트워크 초기화:** `가중치의 초기값`이 필요하며 일반적으로 `무작위로 초기화` 됨   \n",
    "\n",
    "- 초기값이 모두 같으면 모든 노드들의 입력값에 동일 가중치가 반영되어 `같은 값이 출력되고` 역전파로 인해 `비용함수 변화량도 모두 동일`해지므로 다른값 필요\n",
    "\n",
    "**(2) 순전파:** 초기화된 가중치들의 출력이 퍼셉트론을 거쳐 Output Layer에 도달\n",
    "\n",
    "- $\\hat{Y}_{init} = f(\\sum_{i}^{k} w_i x_i - \\theta)$  \n",
    "\n",
    "**(3) 비용함수 평가:**\n",
    "\n",
    "- **회귀문제:** $MSE = \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$   \n",
    "\n",
    "- **분류문제:** $Cross Entropy = \\sum_{i=1}^{k} \\left[ - \\color{red}{\\hat{Y}_{init} log (Pr(\\hat{Y}_{init}))} - \\color{blue}{(1-\\hat{Y}_{init}) log (1-Pr(\\hat{Y}_{init}))} \\right]$  \n",
    "\n",
    "**(4) 역전파:** `각 가중치 별 현재 비용함수에 미치는 영향` 계산 \n",
    "\n",
    "- $\\frac{\\delta E}{\\delta w} = \\frac{\\delta}{\\delta w} \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$  \n",
    "\n",
    "**(5) 가중치 업데이트:** 비용함수를 줄이는 방향으로 각 가중치 업데이트\n",
    "\n",
    "<img src='./img/DL_GD.PNG' width='400'>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_1 &:= W_0 - \\alpha \\frac{\\partial}{\\partial w} \\left[ \\text{Cost Function} \\right] \\\\ &= W_0 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_0} \\\\\n",
    "W_2 &:= W_1 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_1} \\\\\n",
    "W_3 &:= W_2 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_2} \\\\\n",
    "& \\vdots \\\\\n",
    "W &:= W - \\alpha \\frac{\\partial C(W)}{\\partial W}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- **$\\alpha$:** `학습률(Learing Rate)`이 낮으면 추정이 느리고, 높으면 최적점을 벗어나 오차가 증가될 수 있음"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 예시 \n",
    "\n",
    "$총비용(Y) = w_{정장}X_{정장} + w_{셔츠}X_{셔츠} + w_{타이}X_{타이}$\n",
    "\n",
    "<img src='./img/DL_Optimization_Example.png' width='400'>\n",
    "\n",
    "**(1+2) 네트워크 초기화 및 순전파:**  초기가중치($w^{initial}$)가 모두 150원이라면 `총비용은 1500원 출력`\n",
    "\n",
    "**(3) 비용함수 평가** \n",
    "\n",
    "- **회귀문제:** $MSE = \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2 = 423200$ \n",
    "\n",
    "**(4) 역전파:** $\\frac{\\delta E}{\\delta w} = \\frac{\\delta}{\\delta w} \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$  \n",
    "\n",
    "- $\\frac{\\partial MSE}{\\partial w_i} = \\frac{\\partial Y}{\\partial w_i} \\frac{d MSE}{dY} = X_i (\\hat{Y} - Y)$\n",
    "\n",
    "**(5) 가중치 업데이트:** $$W_i^{new} = W_i^{old} - \\alpha \\Delta W_i^{old} = W_i^{old} - \\alpha \\frac{\\delta C}{\\delta w_i}$$  \n",
    "\n",
    "$$\\rightarrow W_i^{new} = W_i^{initial} - \\alpha \\frac{\\delta MSE}{\\delta W_i} = W_i^{initial} - \\alpha X_i(\\hat{Y} - Y)$$ \n",
    "\n",
    "- **정장:** $$W_{정장}^{new} = W_{정장}^{initial} - \\alpha X_{정장} (\\hat{Y} - Y) \\\\ = 150 - (1/500)2(920) = 146.32$$\n",
    "\n",
    "- **셔츠:** $$W_{셔츠}^{new} = W_{셔츠}^{initial} - \\alpha X_{셔츠} (\\hat{Y} - Y) \\\\ = 150 - (1/500)5(920) = 140.80$$\n",
    "\n",
    "- **타이:** $$W_{타이}^{new} = W_{타이}^{initial} - \\alpha X_{타이} (\\hat{Y} - Y) \\\\ = 150 - (1/500)3(920) = 144.48$$\n",
    "\n",
    "**(2') 순전파:** `총비용은 1430.08원`(69.92 감소)\n",
    "\n",
    "**(3') 비용함수 평가:** $MSE = \\frac{1}{2}(580 - 1430.08)^2 = 361318$ (14.6% 감소)\n",
    "\n",
    "**(4') 역전파:** $\\frac{\\delta E}{\\delta w} = \\frac{\\delta}{\\delta w} \\frac{1}{k} \\sum_{i=1}^{k} (\\hat{Y}_{init} - Y)^2$ \n",
    "\n",
    "- $\\frac{\\partial MSE}{\\partial w_i} = \\frac{\\partial Y}{\\partial w_i} \\frac{d MSE}{dY} = X_i (\\hat{Y} - Y)$\n",
    "\n",
    "**(5') 가중치 업데이트:** $$W_i^{new} = W_i^{old} - \\alpha \\Delta W_i^{old} = W_i^{old} - \\alpha \\frac{\\delta C}{\\delta w_i}$$ \n",
    "\n",
    "$$\\rightarrow W_i^{new} = W_i^{initial} - \\alpha \\frac{\\delta MSE}{\\delta W_i} = W_i^{initial} - \\alpha X_i(\\hat{Y} - Y)$$ \n",
    "\n",
    "- **정장:** $$W_{정장}^{new} = W_{정장}^{old} - \\alpha X_{정장} (\\hat{Y} - Y) \\\\ = 146.32 - (1/500)2(850.08) = 142.92$$\n",
    "\n",
    "- **셔츠:** $$W_{셔츠}^{new} = W_{셔츠}^{old} - \\alpha X_{셔츠} (\\hat{Y} - Y) \\\\ = 140.80 - (1/500)5(850.08) = 132.30$$\n",
    "\n",
    "- **타이:** $$W_{타이}^{new} = W_{타이}^{old} - \\alpha X_{타이} (\\hat{Y} - Y) \\\\ = 144.48 - (1/500)3(850.08) = 139.38$$\n",
    "\n",
    "---\n",
    "\n",
    "| **수량데이터** | **`epoch=0` <br> (초기가중치)** | **`epoch=1`** | **`epoch=2`** |\n",
    "|-----|-----|-----|------|\n",
    "| 2 | 150 |    146.32  |    142.92  |\n",
    "| 5 | 150 |    140.80  |    132.30  |\n",
    "| 3 | 150 |    144.48  |    139.38  |\n",
    "\n",
    "(...) **`비용함수가 더이상 변하지 않는` 최소값이 될때까지 반복하여 `목표값(580원)에 가까운 추정치 확보`**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 결론\n",
    "\n",
    "<img src='./img/DL_MLP_Learning.PNG' width='600'>\n",
    "\n",
    "- **이슈:** 딥러닝은 추정해야할 가중치($W$)가 너무 많아 선형/로지시틱회귀분석에서의 `모든 가중치를 수학적 및 통계적으로 하나씩 규명 또는 추정하기 어려움`\n",
    "\n",
    "- **대응:** `비용함수를 최소`로 하는 위치의 가중치(W)를 추정하는 `최적화 알고리즘` 활용\n",
    "\n",
    "    - **Gradient Descent Algorithm:** 예시처럼 비용함수의 변화에 따라 `가중치 업데이트하는 대표적 알고리즘 `\n",
    "    $$W_i^{new} = W_i^{old} - \\alpha \\Delta W_i^{old} = W_i^{old} - \\alpha \\frac{\\delta C}{\\delta w_i}$$ \n",
    "\n",
    "    - **변화량크기:** 기울기 변화 크기로 `크면 가중치가 크게` 변경되고 `작으면 가중치가 작게` 변경\n",
    "\n",
    "    - **학습율(Learning Rate):** 이동속도로 `크면 가중치가 크게` 변경되고 `작으면 가중치가 작게` 변경\n",
    "\n",
    "    <img src='./img/DL_Optimization_Flow.png' width='600'>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 최적화 알고리즘 종류 및 방향\n",
    "\n",
    "<img src='./img/DL_Optimization_Direction.png' width='700'>\n",
    "\n",
    "<!-- <center><img src='Image/Expert/DL_Optimization_Direction_KR.png' width='700'>(https://www.slideshare.net/yongho/ss-79607172)</center> -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 과적합 개선 아이디어 3가지\n",
    "\n",
    "**\"한계의 개선과 은닉층 증가로 성능은 대폭 향상되지만 `연산이 기하급수적으로 늘어나 학습시간이 오래걸리고 과적합(Overfitting) 가능성 높아짐`\"**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0) 과적합?(Overfitting?)\n",
    "\n",
    "<img src='./img/Bias_Variance2.jpeg' width='400'>(붉은색: Target, 파란색: Predicted)\n",
    "\n",
    "- **Underfitting:** Train 패턴 `적게 학습`하여, `주로 Bias 때문에 Train/Test 성능 낮음`\n",
    "\n",
    "- **Overfitting:** Train 패턴 `과하게 학습`하여, `주로 Variance 때문에 Test 성능 낮음`\n",
    "\n",
    "<img src='./img/Bias_Variance4.png' width='400'>\n",
    "\n",
    "**\"알고리즘이 복잡해지면 `Bias와 Variance는 모두 감소하는 경항`이지만, `어느 시점(Train 패턴과 다른 패턴이 Test에 나타나는 시점)`부터는 Variance가 증가하여 `Test의 비용함수/에러가 증가`\"**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Regularization\n",
    "\n",
    "- `비용함수를 개선`하여 과적합 낮춤\n",
    "\n",
    "**\"`추정 가중치가 커지면` 활성함수를 통해 `기울기가 급변`하게되어 비용함수 최소화가 어렵고 `과적합 높아짐`\"** \n",
    "\n",
    "**(0) Linear Regression:** `MSE`를 비용함수로 사용\n",
    "\n",
    "$\\hat{w} = \\underset{w}{\\arg\\min} \\Biggl[\\displaystyle \\sum_{j=1}^t \\Bigl(y_j - \\displaystyle \\sum_{i=0}^k w_i x_{ij}\\Bigr)^2\\Biggr]$\n",
    "\n",
    "**(1) L1 Panelty:** `LASSO Regression`에 사용한 비용함수 반영 \n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{w} = \\underset{w}{\\arg\\min} \\Biggl[\\displaystyle \\sum_{j=1}^t \\Bigl(y_j - \\displaystyle \\sum_{i=0}^k w_i x_{ij}\\Bigr)^2 + \\lambda \\displaystyle \\sum_{i=0}^k \\left|w_i \\right|\\Biggr] \\\\ where~\\lambda~is~hyper~parameter(given~by~human)\n",
    "\\end{align*}\n",
    "\n",
    "- `중요도가 낮은 변수`의 가중치는 0으로 출력하여 과적합 방지\n",
    "\n",
    "- `변수선택 효과`가 있어 모델 복잡도를 효과적으로 제약\n",
    "\n",
    "- 샘플 수보다 변수가 많더라도 변수선택 효과 때문에 `고차원의 데이터도 적용가능`\n",
    "\n",
    "- `패널티의 정도`는 Hyperparameter로 사전 결정되며 교차검증이나 유사 방법으로 결정\n",
    "\n",
    "- `모델에 제약`을 주며 정확도를 상승시킴   \n",
    "\n",
    "**(2) L2 Panelty:** `Ridge Regression`에 사용한 비용함수 반영  \n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{w} = \\underset{w}{\\arg\\min} \\Biggl[\\displaystyle \\sum_{j=1}^t \\Bigl(y_j - \\displaystyle \\sum_{i=0}^k w_i x_{ij}\\Bigr)^2 + \\lambda \\displaystyle \\sum_{i=0}^k w_i^2\\Biggr] \\\\ where~\\lambda~is~hyper~parameter(given~by~human)\n",
    "\\end{align*}\n",
    "\n",
    "- `모든 가중치`를 일률적으로 `작게 만드는 경향`\n",
    "\n",
    "- `중요도 낮은 변수`라도 0이 아닌 가중치를 출력하므로 `일반화 및 변수비교 효과`\n",
    "\n",
    "- 일반화 및 패널티 효과를 높이기 위해 `L1 보다 L2를 많이 사용하는 경향\n",
    "\n",
    "<img src='./img/DL_CF_L1L2.png' width='600'>\n",
    "\n",
    "<img src='./img/DL_CF_L1L2_Compare.png' width='600'>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Drop Out\n",
    "\n",
    "- 각 `batch마다` 은닉층의 `일부 뉴런을 무작위 확률로 제외(출력을 0으로 변환)`하면서 학습\n",
    "\n",
    "**\"`모든 직원`이 함께 일하는 것 < `소그룹`의 결과를 통합하는 것 $\\rightarrow$ 더욱 효율적일수도\"** \n",
    "\n",
    "**\"각 단계의 줄어든 뉴런은 약한 학습이지만 `약한 모델들이 합쳐져 강력한 예측력`\"**   \n",
    "\n",
    "- `Labeled 데이터의 부족`과 `Overfitting 문제 해결`을 위해 Drop Out 제안 (Hinton et al. 2012)\n",
    "\n",
    "- 학습할 때마다 `일부 유닛만을 사용하고 이를 반복해서 합치는 방식`으로 `Ensemble과 유사`\n",
    "\n",
    "- 빠진 뉴런들로 예측 하기에 `여러개의 국소적 독립적 내부패턴 학습가능`\n",
    "\n",
    "- 네트워크가 `뉴런의 특정 가중치에 덜 민감해짐`\n",
    "\n",
    "- `더욱 일반화에 기여가 가능`하고 훈련 데이터에만 과적합 가능성 적어짐\n",
    "\n",
    "- 너무 낮은 비율은 효과가 적고 너무 높은 비율은 과소적합 하기에 `20~50% 권장`\n",
    "\n",
    "- 일반적으로 `Learning Rate(LR, 10->100)과 Momentum(0.9 or 0.99)을 높여 사용`\n",
    "\n",
    "- LR을 높여 가중치의 크기를 줄이면 Ridge와 유사하게 `과적합이 줄어 높은 성능`\n",
    "\n",
    "<img src='./img/DL_DropOut.png' width='500'>\n",
    "\n",
    "- 랜덤한 뉴런을 제거하는 것 대신, `연결선을 랜덤하게 제거하는 DropConnect 방법(별도 외장함수)`도 존재 (Wan et al. 2013)\n",
    "\n",
    "- Dropout을 Dropconnect로 표현할 수 있지만, `Dropconnect를 Dropout으로 표현하기 어려움`\n",
    "\n",
    "- `특정 노드를 비활성화` = `특정 노드에 연결된 모든 가중치를 비활성화`\n",
    "\n",
    "<img src='./img/Dropout_Dropconnect.PNG' width='500'>(https://stats.stackexchange.com/questions/201569/what-is-the-difference-between-dropout-and-drop-connect)\n",
    "\n",
    "- 성능변화 예시\n",
    "\n",
    "<img src='./img/DL_Dropout_Dropconnect.png' width='700'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Early Stopping\n",
    "\n",
    "- Test 에러가 빠르게 증가하는 경우 `계속 학습하지 않고 일찍 종료`하는 방법 \n",
    "\n",
    "**\"데이터가 Train/Validation/Test로 구분되어 있을때, `Validation/Test의 비용함수가 낮으면 멈추므로 Train을 계속 학습하는 과적합 가능성 낮춤`\"**\n",
    "\n",
    "<img src='./img/DL_Overfitting_Epoch.png' width='600'>\n",
    "\n",
    "<img src='./img/DL_Overfitting_EarlyStopping.png' width='600'>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 최적화\n",
    "\n",
    "**\"최종적으로 `미래 데이터 예측에 적합한 최적 가이드를 하이퍼파라미터로 표현`\"**\n",
    "\n",
    "<img src='./img/Hyperparameter_Tuning.png' width='900'>(Hyperparameter tuning for big data using Bayesian optimisation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
