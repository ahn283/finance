{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **인공지능의 발전방향**\n",
    "\n",
    "<img src='./img/MachineLearning_Algorithms.png' width='1000'>\n",
    "\n",
    "**\"`빅데이터에 인공지능이 결합`되어 혁신적으로 빠르게 의사결정하는 `디지털시대`를 우리는 여전히 체감하기 어려움**\n",
    "\n",
    "**(1) `빅데이터와 인공지능은 만능이다`라는 환상을 가진 사람**   \n",
    "\n",
    "**(2) 열정적으로 공부는 하고 있는데, `기대에 떨어지는 현재의 수준과 한계점`에 혼란인 사람**\n",
    "\n",
    "$\\Rightarrow$ **\"`보통사람들을 위한 가이드`로 가능한 쉽게 설명하는데 중점\"**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 인공지능이란(Artificial Intelligence, AI)?\n",
    "\n",
    "- `인간지능의 원리`를 찾기위한 목적으로, `프로그래밍으로 인공적 구현`하는 대상\n",
    "\n",
    "- `1956년 처음 등장`한 이래 `아직도 미래기술로써 인식`되는 인공지능\n",
    "\n",
    "- 새롭게 지능의 원리가 발견하면, 더이상 지능으로 간주되지 않고 `일반적 사실로 인식되는 과학적 프레임워크` 기반으로 생성\n",
    "\n",
    "- **AI Effect:** 발견된 원리 또는 사실을 제외하고 `아직 발견하지 못한 새로운 지능의 비결을 꾸준히 찾는 것`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 아이디어\n",
    "\n",
    "- `규칙은 끊임없이 변하기 때문에` 사람이 규칙을 일일이 찾는 것은 좋은 접근이 아니고, 기존 입출력 데이터 기반으로 `기계가 스스로 관계성 규칙을 지속적으로 찾아내도록` 유도하자\n",
    "\n",
    "- 전체적으로 `지도학습/비지도학습/강화학습`이라는 3가지 종류의 알고리즘으로 해결중\n",
    "\n",
    "<img src='./img/ML_Type_CategoryKorean.png' width='800'>\n",
    "\n",
    "| **알고리즘 종류** \t| **학습 방향** \t|\n",
    "|-----|-----|\n",
    "| **지도학습 알고리즘** \t| 기계에게 `문제와 답을 학습`시킨 후 향후 답을 예측 \t|\n",
    "| **비지도학습 알고리즘** \t| 기계에게 `문제만 학습`시킨 후 스스로 패턴을 고려하여 답을 예측 \t|\n",
    "| **강화학습 알고리즘** \t| 아무것도 모른채 `일단 실전`에 뛰어들어 `시행착오로 학습`하고 스스로 개선시켜 향후 답을 예측<br>데이터와 상호작용을 반복하기에 `시간이 오래걸리나 가장 강력하고 진보적`인 방법 \t|\n",
    "\n",
    "- 각 알고리즘은 인간이 해결하고 싶어하는 `특정 세부문제 맞춤해결` 방식으로 진화중\n",
    "\n",
    "| **알고리즘 종류** \t| **해결문제 종류** \t| **해결 예시** \t|\n",
    "|-----|-----|-----|\n",
    "| **지도학습 알고리즘** \t| **예측문제** \t| 주관식 문제의 숫자형태의 정답을 해결 \t|\n",
    "|  \t| **분류문제** \t| 객관식 문제의 보기들 중 정답을 해결 \t|\n",
    "| **비지도학습 알고리즘** \t| **군집문제** \t| 시험문제에 어떤 유형들이 있는지 해결 \t|\n",
    "|  \t| **차원변환문제** \t| 시험문제의 풀이법을 다양한 관점으로 변환 \t|\n",
    "| **강화학습 알고리즘** \t| **-** \t| 공교육/사교육 없이 스스로 지도학습/비지도학습 문제를 해결하고<br>오답노트도 스스로 만들고 학습하여 성적을 계속 끌어올림 \t|\n",
    "\n",
    "<img src='./img/HumanLearningMachineLearning.png' width='800'>\n",
    "\n",
    "- 인간이 해결하고 싶어하는 `문제에 맞게` 기계에게 `데이터를 학습`시키는 방식으로 진화되어 `데이터는 필수`\n",
    "\n",
    "<img src='./img/ML_Type_Category.png' width='700'>\n",
    "\n",
    "<img src='./img/ML_Type_Application_Upgrade.png' width='700'>\n",
    "\n",
    "<img src='./img/AI_ML_DL.jpg' width='700'>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 알고리즘의발전\n",
    "\n",
    "- **인공지능:** `인간의 지능`을 기계 등에 `인공적으로 구현`한 것으로, 인간의 지능을 모방하는 기계를 표현하는 `가장 광범위한 용어`로 음성 및 얼굴인식이나 의사결정 등 `인간이 그동안 해온 작업을 예측, 자동화 및 최적화하는데 사용`\n",
    "\n",
    "    - **Weak AI:** `Artificial Narrow Intelligence(ANI)`라고도 하며 체스게임이나 사진 식별, 챗봇 등 `특정 Task에 특화된 AI`\n",
    "\n",
    "    - **Strong AI(Weak):** `Artificial General Intelligence(AGI)`라고도 하며 `인간과 비등한능력을 가진 AI` \n",
    "\n",
    "    - **Strong AI:** `Artificial Super Intelligence(ASI)`라고도 하며 `인간의 지능과 능력을 넘어서는 AI`\n",
    "\n",
    "- **머신러닝:** `인공지능의 한 분야`로 컴퓨터가 학습할 수 있는 알고리즘과 기술을 개발하는 분야를 의미하며, 사람이 특징을 알려주면 `기계가 명시적으로 프로그래밍 되지 않은 동작을 스스로 학습해 수행`하는데 사용되고 `스몰데이터에서 딥러닝보다 잘하는 편`\n",
    "\n",
    "- **딥러닝:** 비선형 조합을 통해 `높은 수준의 패턴학습을 시도하는 머신러닝` 알고리즘으로, 머신러닝에서의 `많은 특징 추출 작업을 자동화`하고 신경망을 이용하여 `인간의 신경계와 유사한 구조로 여러 분야에서 높은 성능으로 분석`\n",
    "\n",
    "    <center><img src='./img/DL_Evolution.png' width='600'></center>\n",
    "\n",
    "    - 인공신경망(딥러닝)은 머신러닝 방법론 중 하나로 `1950년대 처음 제안`\n",
    "\n",
    "    - 80~90년대에 널리 사용되다 `계산 비용이 높아 90년대 후반에 약세`\n",
    "\n",
    "    - 2000년대 이후 `빅데이터 + 컴퓨팅파워 + 알고리즘 고도화로 재부상`\n",
    "\n",
    "    - 인공신경망의 한계는 결국 `최적화가 어렵고 너무 많은 시간이 소요`된다는 점\n",
    "\n",
    "    - 인공신경망의 알고리즘을 개선하며 `비지도 학습을 통해 최적화를 수행하는 방식`으로 한계 극복 및 가능성 확장"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **딥러닝의 시초 인공신경망의 등장(1940~1950)**\n",
    "\n",
    "## 인간 뇌의 비밀\n",
    "\n",
    "**\"인간은 어떻게 `동물과 다른 창의적 사고`가 가능할까?\"**\n",
    "\n",
    "**\"아인슈타인은 `어떤 뇌`를 통해 시공간의 비밀을 알 수 있었을까?\"**\n",
    "\n",
    "**\"보통 사람은 왜 아인슈타인의 `천재적 사고가 어려울까?`\"**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 뇌의 구조를 모방한 퍼셉트론(Perceptron)\n",
    "\n",
    "$\\Rightarrow$ **\"과학자들이 뇌의 구조를 분석하여 `1000억개 가량의 뉴런(Neuron)이라는 신경세포가 다른 신경세포들과 100조개 가량의 가지로 연결되어 화학적 신호를 주고받는`다는 사실을 밝혀내고 이해하고 있음\"**\n",
    "\n",
    "<img src='./img/DL_Neuron.png' width='600'>\n",
    "\n",
    "- `뇌의 구조와 동작을 완벽하게 재현`할 수 있다면 인간 뇌와 유사한 `인공뇌(Artificial Brain)`를 만들 수 있지만 `현재 기술로는 구현이 어려운` 실정\n",
    "\n",
    "- 대신 `뉴런을 대신하는 프로그래밍 함수`와 화학적 신호로 전달하는 `물질을 대신하는 숫자 데이터`는 프로그램으로 충분히 구현 가능하고 이를 `퍼셉트론(Perceptron)`이라고 함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 퍼셉트론의 역할과 딥러닝\n",
    "\n",
    "- 인간의 뇌는 입력 데이터를 단순히 하나의 뉴런을 흉내낸 퍼셉트론만 거치는 것이 아니라 `숨겨진 층의 뉴런들을 거쳐서 최종 출력으로` 바꿈\n",
    "\n",
    "<img src='./img/DL_MLP_Custom.PNG' width='500'>\n",
    "\n",
    "- 이를 더욱 복잡하게 쌓아서 만들면 `깊은 신경구조`가 될 것이고 `인공뇌 또는 인공신경망 구현이 가능`할 수 있을 것이며 더욱 복잡한 일들을 해결할 수 있을 것이란 기대에 등장한 것이 `딥러닝(Deep Learning)`이라는 머신러닝 알고리즘\n",
    "\n",
    "<img src='./img/Human_vs_AI.jpg' width='700'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 인공신경망(Artificial Neural Network: ANN)\n",
    "\n",
    "- `인간 뇌의 신경구조`를 네트워크로 표현할 수 있다고 최초로 제안 (McCulloch and Pitts, 1943)\n",
    "\n",
    "- `인간 두뇌의 신경세포를 모방한 컴퓨터 알고리즘 네트워크` 구조를 의미 \n",
    "\n",
    "- `인간의 뇌`는 많은 신경들이 존재하지만 함께 작동하여 마치 `하나의 학습 알고리즘`으로 여러가지를 동시에 학습하고 해결\n",
    "\n",
    "- 마찬가지로 `수천개의 모델이나 프로그램` 대신에 `하나의 학습 알고리즘`을 목표로 접근\n",
    "\n",
    "- `어떠한 형태의 규칙/함수/가설`도 쉽게 근사할 수 있는 `Universal Function Approximator`를 목표"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 뉴런과 인공신경망 비교 정리\n",
    "\n",
    "- **뉴런의 입출력**\n",
    "\n",
    "<img src='./img/DL_Neuron.png' width='600'>\n",
    "\n",
    "- **인공신경망의 입출력**\n",
    "\n",
    "<img src='./img/DL_ANN.png' width='600'>\n",
    "\n",
    "**(1) 동그라미:** 하나의 `노드(뉴런)`\n",
    "\n",
    "**(2) 실선:** 노드와 노드가 이어진 `전달선(빨간색선)`\n",
    "\n",
    "- 인간의 뇌는 `뉴런들이 신호/자극(Input)`을 받고 그 자극이 `임계값(Weight)을 넘으면 신호를 전달(Output)`하는 과정"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **단순한 퍼셉트론의 구조:** 단층퍼셉트론\n",
    "\n",
    "**\"여러개의 노드(뉴런)들로 이루어진 `하나의 인공신경망(단일층)`으로 구성된 알고리즘 (Rosenblatt, 1958)\"**\n",
    "\n",
    "- **\"여러개의 노드(뉴런)들로 이루어진 `하나의 인공신경망(단일층)`으로 구성된 알고리즘 (Rosenblatt, 1958)\"**\n",
    "\n",
    "<img src='./img/DL_ANN.png' width='600'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 구조와 용어\n",
    "\n",
    "- 데이터가 각 노드에 입력되고 `연결강도(가중치)`를 곱한다음 합산하여 전달하며 최종적으로 `하나의 값(분류 or 연속)으로 출력`\n",
    "\n",
    "- 현재의 딥러닝도 이러한 구조를 `여러개의 레이어층으로 확장`했을 뿐 같은 구조이며 `인공뇌 개발을 기대`하게 됨\n",
    "\n",
    "**(1) 입력층(Input Layer):** 데이터가 `입력되는 노드`\n",
    "\n",
    "**(2) 가중치(Weight):** 입력 데이터 각각이 `출력에 영향을 주는 정도`인 가중치(Parameter) 연결선\n",
    "\n",
    "**(3) 출력층(Output Layer):** 해결하고자 하는 문제에 맞춰 `최종 출력값을 반환`하는 역할\n",
    "\n",
    "**(3') 활성함수(Activation Function):** 출력값을 모두 반영하지 않고 `특정 수준의 데이터만 필터/제한/변환`하는 역할\n",
    "\n",
    "- 정해진 `임계값 기준` 은닉층의 출력을 `최종적으로 원하는 출력값으로 변환`\n",
    "\n",
    "- `비선형 Scaling`과 유사하며 일반적으로 `일정 수준이 넘는 데이터를 전달`\n",
    "\n",
    "- `비용함수의 책임 또는 영향을 각 가중치 별 추정(미분)`가능하도록 반영하는 도구\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_1 &:= W_0 - \\alpha \\frac{\\partial}{\\partial w} \\left[ \\text{Cost Function} \\right] \\\\ &= W_0 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_0} \\\\\n",
    "W_2 &:= W_1 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_1} \\\\\n",
    "W_3 &:= W_2 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_2} \\\\\n",
    "& \\vdots \\\\\n",
    "W &:= W - \\alpha \\frac{\\partial C(W)}{\\partial W}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 지도학습 회귀 및 분류문제의 단층퍼셉트론 표현\n",
    "\n",
    "- **회귀문제의 신경망**\n",
    "\n",
    "<img src='./img/DL_Comparing1.PNG' width='200'>\n",
    "\n",
    "- **분류문제의 신경망**\n",
    "\n",
    "<img src='./img/DL_Classification.PNG' width='350'>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **지도학습의 간단한 인공신경망 생성**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 회귀문제\n",
    "# 입력 X\n",
    "import numpy as np\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# 가중치 W\n",
    "W = np.array([0.5, 0.6, 0.1, 0.3, 0.2])\n",
    "\n",
    "# 출력 Y \n",
    "Y = np.sum(W * X)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9852259683067269"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 분류문제\n",
    "# 입력 X\n",
    "import numpy as np\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# 가중치 W\n",
    "W = np.array([0.5, 0.6, 0.1, 0.3, 0.2])\n",
    "\n",
    "# 출력 Y\n",
    "Y = np.sum(W * X)\n",
    "\n",
    "# 비선형 activation\n",
    "def activation(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "activation(Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `은닉층 없이 출력층만 존재하는 인공신경망`\n",
    "\n",
    "<img src='./img/DL_Comparing1.PNG' width='200'>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W &= \\left[ w_1, w_2, w_3, w_4, w_5 \\right], \\\\\n",
    "X &= \\left[ X_1, X_2, X_3, X_4, X_5 \\right], \\\\\n",
    "Y &= W * X \\\\\n",
    "&= w_1 X_1 + w_2 X_2 + w_3 X_3 + w_4 X_4 + w_5 X_5 ~~~ (\\text{Linear Regression)}\\\\\n",
    "&= w_0 + w_1 X_1 + w_2 X_2 + w_3 X_3 + w_4 X_4 + w_5 X_5 ~~~ (\\text{Linear Regression with Constant)}\\\\\n",
    "&= \\sigma (w_1 X_1 + w_2 X_2 + w_3 X_3 + w_4 X_4 + w_5 X_5) ~~~ (\\text{Logistic Regression)}\\\\\n",
    "&= \\sigma (w_0 + w_1 X_1 + w_2 X_2 + w_3 X_3 + w_4 X_4 + w_5 X_5) ~~~ (\\text{Logistic Regression with Constant)}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.400000000000002"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 만약에 노드가 3개만 은닉층이 추가된다면?\n",
    "# 입력 X\n",
    "import numpy as np\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# 가중치 W\n",
    "W = np.array([[0.5, 0.6, 0.1, 0.3, 0.2], [0.2, 0.1, 0.8, 0.2, 0.1], [0.3, 0.1, 0.1, 0.2, 0.7]])\n",
    "\n",
    "# 출력 Y\n",
    "Y = np.sum(W * X)\n",
    "Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `노드가 3개인 은닉층이 추가된 인공신경망`\n",
    "\n",
    "<img src='./img/DL_Comparing2.PNG' width='350'>\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W &= \\begin{bmatrix} \n",
    "   w_{11} & w_{12} & w_{13} & w_{14} & w_{15}  \\\\\n",
    "   w_{21} & w_{22} & w_{23} & w_{24} & w_{25}  \\\\\n",
    "   w_{31} & w_{32} & w_{33} & w_{34} & w_{35}  \\\\\n",
    "\\end{bmatrix}, \\\\\n",
    "X &= \\left[ X_1, X_2, X_3, X_4, X_5 \\right], \\\\\n",
    "Y &= W * X ~~~ (\\text{Linear Regression}) \\\\\n",
    "&= w_{10} + w_{11} X_1 + w_{12} X_2 + w_{13} X_3 + w_{14} X_4 + w_{15} X_5 ~~~ (\\text{First Neuron}) \\\\\n",
    "&+ w_{10} + w_{21} X_1 + w_{22} X_2 + w_{23} X_3 + w_{24} X_4 + w_{25} X_5 ~~~ (\\text{Second Neuron}) \\\\\n",
    "&+ w_{10} + w_{31} X_1 + w_{32} X_2 + w_{33} X_3 + w_{34} X_4 + w_{35} X_5 ~~~ (\\text{Third Neuron}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- 인공신경망은 `하나의 층에 수백개의 입력값과 가중치 존재` 가능\n",
    "\n",
    "- **행렬:** 이러한 상황을 표현하는 `강력한 수학 표현`\n",
    "\n",
    "- **선형대수:** `행렬을 다루는 수학`의 한 분야 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.900000000000002"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 가중치를 변경하면 뉴런의 값 또는 기능이 변경됨\n",
    "# 입력 X\n",
    "import numpy as np\n",
    "X = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# 가중치 W\n",
    "W = np.array([[0.5, 0.6, 0.1, 0.3, 0.5], [0.2, 0.1, 0.8, 0.2, 0.5], [0.3, 0.1, 0.1, 0.2, 0.5]])\n",
    "\n",
    "# 출력 Y\n",
    "Y = np.sum(W * X)\n",
    "Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 퍼셉트론의 한계(AI Winter)와 학습?\n",
    "\n",
    "**\"앞선 예시는 데이터의 학습과정이 없는 우리가 입력한 가중치 적용하였고, `가중치를 스스로 찾도록 학습하려면 어떻게?`\"**\n",
    "\n",
    "#### 1) 배경\n",
    "\n",
    "- 도널드 올딩 헵은 뉴런의 작동원리를 연구하다가 `서로 반복적 지속적으로 영향을 주고받는 뉴런들은 함께 활성화/비활성화되어 연결의 강도(두께)가 높아/낮아진다 (1949년)`는 사실을 발견하였고 이를 인공지능의 `가중치(Weight)를 학습하는 아이디어`의 토대가 됨\n",
    "\n",
    "- **헵의 학습 법칙(Hebbian Learning Rule):** `퍼셉트론 학습의 근거`로 목표치를 만족시키도록 `뉴런에 가해지는 입력(x)과 출력(y)을 업데이트`\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_{xy}^{k+1} &:= W_{xy}^{k} + y^k x^k \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 발전\n",
    "\n",
    "- 헵의 이론은 완벽하지 않았지만 `시간이 흐르며 계속 수정 및 확장되어 여전히 타당한 주장`으로 받아들여 지고 있음\n",
    "\n",
    "- `알고리즘의 출력`($\\hat{y}$)`이 목표값`($y$)`과 일치하는지 확인`하여 일치하면 가중치가 변경되지 않음\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_{xy}^{k+1} &:= W_{xy}^{k} + y^k x^k \\\\\n",
    "& \\Downarrow \\\\\n",
    "W_{xy}^{k+1} &:= W_{xy}^{k} + (y^k - \\hat{y}^k) x^k \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "| **목표값** | **출력값** | **$$y^k - \\hat{y}^k$$** | **가중치 업데이트 방향** |\n",
    "|-----|-----|-----|-----|\n",
    "| 1 | 1 | 0 | 업데이트 안함 |\n",
    "| -1 | -1 | 0 | 업데이트 안함 |\n",
    "| 1 | -1 | 2 | 양의 방향 |\n",
    "| -1 | 1 | -2 | 음의 방향 |\n",
    "\n",
    "- 정답과 일치하지 않으면 가중치를 변경하는데 `학습률`($\\alpha$)을 통해 너무 급격히 변경되는 것을 방지\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_{xy}^{k+1} &:= W_{xy}^{k} + (y^k - \\hat{y}^k) x^k \\\\\n",
    "& \\Downarrow \\\\\n",
    "W_{xy}^{k+1} &:= W_{xy}^{k} + \\alpha (y^k - \\hat{y}^k) x^k \\\\\n",
    "& \\Downarrow \\\\\n",
    "W_1 &:= W_0 - \\alpha \\frac{\\partial}{\\partial w} \\left[ \\text{Cost Function} \\right] \\\\ &= W_0 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_0} \\\\\n",
    "W_2 &:= W_1 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_1} \\\\\n",
    "W_3 &:= W_2 - \\alpha \\frac{\\partial C(W)}{\\partial W} |_{W=W_2} \\\\\n",
    "& \\vdots \\\\\n",
    "W &:= W - \\alpha \\frac{\\partial C(W)}{\\partial W}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Y  X1  X2  X3\n",
       "0  0  -1  -1   1\n",
       "1  0  -1   1   1\n",
       "2  1   1  -1   1\n",
       "3  1   1   1   1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial W:  [0 0 0]\n",
      "입력:  [-1 -1  1] 목표:  0 출력:  0.5 \n",
      " error -0.5 업데이트된 가중치:  [ 0.005  0.005 -0.005]\n",
      "입력:  [-1  1  1] 목표:  0 출력:  0.49875000260416025 \n",
      " error -0.49875000260416025 업데이트된 가중치:  [ 9.98750003e-03  1.24999740e-05 -9.98750003e-03]\n",
      "입력:  [ 1 -1  1] 목표:  1 출력:  0.49999687500651047 \n",
      " error 0.5000031249934895 업데이트된 가중치:  [ 0.01498753 -0.00498753 -0.00498747]\n",
      "입력:  [1 1 1] 목표:  1 출력:  0.5012531301821841 \n",
      " error 0.4987468698178159 업데이트된 가중치:  [ 1.99750000e-02 -6.25777983e-08 -7.79285482e-11]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.99750000e-02, -6.25777983e-08, -7.79285482e-11])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 퍼셉트론을 학습시키는 알고리즘을 만들어서 가중치 업데이트\n",
    "# 데이터 로딩\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([[0, -1, -1, 1], [0, -1, 1, 1], [1, 1, -1, 1], [1, 1, 1, 1]],\n",
    "                  columns=['Y', 'X1', 'X2', 'X3'])\n",
    "display(df)\n",
    "\n",
    "# 입력 X\n",
    "X = df[['X1', 'X2', 'X3']].values\n",
    "\n",
    "# 출력 Y\n",
    "Y = df['Y'].values\n",
    "\n",
    "# 가중치 W\n",
    "W = np.array([0 for i in range(X.shape[1])])\n",
    "print('initial W: ', W)\n",
    "\n",
    "# 가중치 업데이트\n",
    "def training(X, Y, learning_rate=0.01):\n",
    "    global W\n",
    "    for i in range(X.shape[0]):\n",
    "        y_pred = activation(np.sum(W * X[i]))\n",
    "        error = Y[i] - y_pred\n",
    "        if error != 0:\n",
    "            W = W + learning_rate * error * X[i]\n",
    "            print('입력: ', X[i], '목표: ', Y[i], '출력: ', y_pred, '\\n',\n",
    "                  'error', error, '업데이트된 가중치: ', W)\n",
    "    return W\n",
    "\n",
    "training(X, Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Others:** 굳이 데이터를 `1번만 학습할 필요가 있을까?`\n",
    "\n",
    "**(1) Epoch:** 훈련데이터의 `알고리즘 학습 횟수`\n",
    "\n",
    "**(2) Batch:** `1 epoch`에서 학습과 평가를 진행할 `row 샘플의 갯수`\n",
    "\n",
    "- **Stochastic Gradient Descent:** `Batch Size = 1`\n",
    "\n",
    "- **Batch Gradient Descent:** `Batch Size = Size of Training Set`\n",
    "\n",
    "- **Mini-Batch Gradient Descent:** `1 < Batch Size < Size of Training Set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
